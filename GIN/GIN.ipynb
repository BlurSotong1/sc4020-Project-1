{
 "cells": [
  {
   "cell_type": "code",
   "id": "260109f8eefbad52",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.nn import GINConv\n",
    "from typing import Optional, Dict, Any\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    _HAS_SK = True\n",
    "except Exception:\n",
    "    _HAS_SK = False\n",
    "    print(\"sklearn not found: AUC will be computed via a simple approx (PRNG tie-breaks).\")\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "class GINEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Flexible GIN encoder with configurable MLP depth.\n",
    "\n",
    "    Args:\n",
    "        num_nodes (int): Number of nodes in the graph.\n",
    "        hidden_layers (int): Number of hidden layers inside each MLP.\n",
    "        emb_dim (int): Embedding dimension.\n",
    "        num_layers (int): Number of GIN layers.\n",
    "        train_eps (bool): Whether to learn epsilon.\n",
    "        dropout (float): Dropout rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_nodes, hidden_layers=2, emb_dim=128, num_layers=3, train_eps=True, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(num_nodes, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.embed.weight)\n",
    "\n",
    "        def make_mlp(in_dim, hidden_dim, out_dim, num_hidden):\n",
    "            \"\"\"Builds an MLP with variable hidden depth.\"\"\"\n",
    "            layers = [nn.Linear(in_dim, hidden_dim), nn.ReLU()]\n",
    "            for _ in range(num_hidden - 1):  # add intermediate hidden layers\n",
    "                layers += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU()]\n",
    "            layers.append(nn.Linear(hidden_dim, out_dim))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            GINConv(make_mlp(emb_dim, emb_dim * 2, emb_dim, hidden_layers), train_eps=train_eps)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "        x = self.embed.weight  # [N, emb_dim]\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            x = self.dropout(x)\n",
    "        return x  # node embeddings"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "70768b72d769066c",
   "metadata": {},
   "source": [
    "def _fmt_ts(dt: datetime) -> str:\n",
    "    return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def _fmt(x):\n",
    "    try:\n",
    "        return f\"{float(x):.4f}\"\n",
    "    except Exception:\n",
    "        return \"nan\"\n",
    "\n",
    "def print_comparison_report(\n",
    "        title: str,\n",
    "        left_name: str, left_result: Dict[str, Any],\n",
    "        right_name: str, right_result: Dict[str, Any],\n",
    "        save_path: Optional[str | Path] = None,\n",
    "):\n",
    "    ts = _fmt_ts(datetime.now())\n",
    "\n",
    "    def block(name, res):\n",
    "        best = res[\"best\"]; hist = res[\"history\"]\n",
    "        best_auc = max((h.get(\"val_auc\", float(\"nan\")) for h in hist), default=float(\"nan\"))\n",
    "        total_epochs = res.get(\"epochs_trained\", len(hist))\n",
    "\n",
    "        lines = []\n",
    "        lines.append(f\"{name} Training History\")\n",
    "        lines.append(\"=\" * 60)\n",
    "        lines.append(\"\")\n",
    "        lines.append(f\"Best Validation AUC: {_fmt(best_auc)}\")\n",
    "        lines.append(f\"Total Epochs Trained: {total_epochs}\")\n",
    "        lines.append(f\"Early Stopping Best Score: {_fmt(best.get('Hits@10'))} (Hits@10 at epoch {best.get('epoch')})\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"-\" * 90)\n",
    "        lines.append(f\"{'Epoch':<8} {'Train Loss':<14} {'Val AUC':<12} {'Val H@1':<12} {'Val H@5':<12} {'Val H@10':<12}\")\n",
    "        lines.append(\"-\" * 90)\n",
    "        for rec in hist:\n",
    "            e = rec.get(\"epoch\")\n",
    "            lines.append(\n",
    "                f\"{e:<8} \"\n",
    "                f\"{_fmt(rec.get('train_loss')):<14} \"\n",
    "                f\"{_fmt(rec.get('val_auc')):<12} \"\n",
    "                f\"{_fmt(rec.get('val_hits1')):<12} \"\n",
    "                f\"{_fmt(rec.get('val_hits5')):<12} \"\n",
    "                f\"{_fmt(rec.get('val_hits10')):<12}\"\n",
    "            )\n",
    "        lines.append(\"\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    out = []\n",
    "    out.append(f\"{title} - {ts}\")\n",
    "    out.append(\"=\" * 80)\n",
    "    out.append(\"\")\n",
    "    out.append(block(left_name, left_result))\n",
    "    out.append(block(right_name, right_result))\n",
    "\n",
    "    # Best-at-a-glance\n",
    "    out.append(\"Best Validation Metrics Summary\")\n",
    "    out.append(\"=\" * 60)\n",
    "    for name, res in [(left_name, left_result), (right_name, right_result)]:\n",
    "        b = res[\"best\"]\n",
    "        out.append(\n",
    "            f\"{name}: \"\n",
    "            f\"AUC={_fmt(b.get('AUC'))} | \"\n",
    "            f\"H@1={_fmt(b.get('Hits@1'))} | \"\n",
    "            f\"H@5={_fmt(b.get('Hits@5'))} | \"\n",
    "            f\"H@10={_fmt(b.get('Hits@10'))} \"\n",
    "            f\"(epoch {b.get('epoch')})\"\n",
    "        )\n",
    "    out.append(\"\")\n",
    "\n",
    "    report = \"\\n\".join(out)\n",
    "    print(report)\n",
    "\n",
    "    if save_path:\n",
    "        save_path = Path(save_path)\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(report)\n",
    "        print(f\"\\nComparison report saved to: {save_path.resolve()}\")\n",
    "\n",
    "\n",
    "def print_training_report(\n",
    "        model_name: str,\n",
    "        result: Dict[str, Any],\n",
    "        header_title: str = \"Model Training Results\",\n",
    "        save_path: Optional[str | Path] = None,\n",
    "):\n",
    "    ts = _fmt_ts(result.get(\"end_time\", datetime.now()))\n",
    "    best = result[\"best\"]\n",
    "    history = result[\"history\"]\n",
    "    total_epochs = result.get(\"epochs_trained\", len(history))\n",
    "    best_auc = max((h.get(\"val_auc\", float(\"nan\")) for h in history), default=float(\"nan\"))\n",
    "\n",
    "    lines = []\n",
    "    lines.append(f\"{header_title} - {ts}\")\n",
    "    lines.append(\"=\" * 80)\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"{model_name} Training History\")\n",
    "    lines.append(\"=\" * 60)\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"Best Validation AUC: {_fmt(best_auc)}\")\n",
    "    lines.append(f\"Total Epochs Trained: {total_epochs}\")\n",
    "    lines.append(f\"Early Stopping Best Score: {_fmt(best.get('Hits@10'))} (Hits@10 at epoch {best.get('epoch')})\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"-\" * 90)\n",
    "    lines.append(f\"{'Epoch':<8} {'Train Loss':<14} {'Val AUC':<12} {'Val H@1':<12} {'Val H@5':<12} {'Val H@10':<12}\")\n",
    "    lines.append(\"-\" * 90)\n",
    "\n",
    "    for rec in history:\n",
    "        e = rec.get(\"epoch\")\n",
    "        lines.append(\n",
    "            f\"{e:<8} \"\n",
    "            f\"{_fmt(rec.get('train_loss')):<14} \"\n",
    "            f\"{_fmt(rec.get('val_auc')):<12} \"\n",
    "            f\"{_fmt(rec.get('val_hits1')):<12} \"\n",
    "            f\"{_fmt(rec.get('val_hits5')):<12} \"\n",
    "            f\"{_fmt(rec.get('val_hits10')):<12}\"\n",
    "        )\n",
    "\n",
    "    lines.append(\"\")\n",
    "    report_text = \"\\n\".join(lines)\n",
    "\n",
    "    print(report_text)\n",
    "\n",
    "    if save_path:\n",
    "        save_path = Path(save_path)\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(report_text)\n",
    "        print(f\"\\nReport saved to: {save_path.resolve()}\")\n",
    "\n",
    "def build_edge_index_from_dm(dm) -> torch.LongTensor:\n",
    "    \"\"\"\n",
    "    Build an undirected edge_index [2, E] from the *train* split only.\n",
    "    - For Typed DM: use (h, t) from train triples (relation ignored for GIN).\n",
    "    - For Collapsed DM: use train pairs.\n",
    "    Ensures symmetry (adds reverse if missing).\n",
    "    \"\"\"\n",
    "    if hasattr(dm, \"_train_triples\"):\n",
    "        ht = dm._train_triples[:, [0, 2]]  # (h, t)\n",
    "    else:\n",
    "        ht = dm._train_pairs                # (h, t)\n",
    "    if ht.numel() == 0:\n",
    "        return torch.empty(2, 0, dtype=torch.long)\n",
    "\n",
    "    # ensure both directions\n",
    "    rev = torch.stack([ht[:,1], ht[:,0]], dim=1)\n",
    "    edges = torch.cat([ht, rev], dim=0).T.contiguous()  # [2, E]\n",
    "    return edges\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_negatives(pairs_or_triples: torch.Tensor, num_entities: int, typed: bool) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    1:1 negatives per positive (half head-corrupt, half tail-corrupt).\n",
    "    Returns a tensor with same shape as input.\n",
    "    \"\"\"\n",
    "    B = pairs_or_triples.size(0)\n",
    "    device = pairs_or_triples.device\n",
    "    rand_ents = torch.randint(0, num_entities, (B,), device=device)\n",
    "\n",
    "    neg = pairs_or_triples.clone()\n",
    "    flip = torch.rand(B, device=device) < 0.5\n",
    "    if typed:\n",
    "        # triples: (h, r, t)\n",
    "        neg[flip, 0] = rand_ents[flip]   # corrupt head\n",
    "        neg[~flip, 2] = rand_ents[~flip] # corrupt tail\n",
    "    else:\n",
    "        # pairs: (h, t)\n",
    "        neg[flip, 0] = rand_ents[flip]\n",
    "        neg[~flip, 1] = rand_ents[~flip]\n",
    "    return neg\n",
    "\n",
    "\n",
    "def dot_scores(z: torch.Tensor, X: torch.Tensor, typed: bool) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Dot-product decoder.\n",
    "    - If typed: X is [B,3] (h,r,t) but r is ignored for dot-product.\n",
    "    - If untyped: X is [B,2] (h,t).\n",
    "    \"\"\"\n",
    "    if typed:\n",
    "        h, t = X[:,0], X[:,2]\n",
    "    else:\n",
    "        h, t = X[:,0], X[:,1]\n",
    "    return (z[h] * z[t]).sum(dim=1)  # logits\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_metrics(\n",
    "        encoder: torch.nn.Module,\n",
    "        edge_index: torch.Tensor,\n",
    "        loader: Optional[DataLoader],\n",
    "        num_entities: int,\n",
    "        typed: bool,\n",
    "        device: torch.device,\n",
    "        show_tqdm: bool = False,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluates AUC and Hits@1/5/10 using *unfiltered* ranking.\n",
    "    Embeddings are computed from the train graph (edge_index).\n",
    "    \"\"\"\n",
    "    if loader is None:\n",
    "        return {\"AUC\": float(\"nan\"), \"Hits@1\": float(\"nan\"), \"Hits@5\": float(\"nan\"), \"Hits@10\": float(\"nan\")}\n",
    "\n",
    "    encoder.eval()\n",
    "    z = encoder(edge_index.to(device))  # [N, d]\n",
    "\n",
    "    # --- AUC: 1 negative per positive ---\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    it_auc = loader if not show_tqdm else tqdm(loader, leave=False, desc=\"Eval AUC\")\n",
    "    for X_pos, _ in it_auc:\n",
    "        X_pos = X_pos.to(device)\n",
    "        X_neg = sample_negatives(X_pos, num_entities, typed=typed).to(device)\n",
    "\n",
    "        s_pos = dot_scores(z, X_pos, typed)\n",
    "        s_neg = dot_scores(z, X_neg, typed)\n",
    "\n",
    "        all_scores.append(torch.cat([s_pos, s_neg]).detach().cpu().numpy())\n",
    "        all_labels.append(np.concatenate([np.ones(len(s_pos)), np.zeros(len(s_neg))]))\n",
    "\n",
    "    scores = np.concatenate(all_scores) if len(all_scores) > 0 else np.array([])\n",
    "    labels = np.concatenate(all_labels) if len(all_labels) > 0 else np.array([])\n",
    "    if _HAS_SK and len(scores) > 0:\n",
    "        auc = float(roc_auc_score(labels, scores))\n",
    "    else:\n",
    "        if len(scores) == 0:\n",
    "            auc = float(\"nan\")\n",
    "        else:\n",
    "            order = np.argsort(scores)\n",
    "            ranks = np.empty_like(order); ranks[order] = np.arange(len(scores))\n",
    "            pos_ranks = ranks[labels == 1]\n",
    "            neg_ranks = ranks[labels == 0]\n",
    "            auc = float(np.mean(pos_ranks[:, None] > neg_ranks[None, :]))\n",
    "\n",
    "    # --- Hits@k ---\n",
    "    hits1 = hits5 = hits10 = 0\n",
    "    trials = 0\n",
    "    it_hits = loader if not show_tqdm else tqdm(loader, leave=False, desc=\"Eval Hits\")\n",
    "    for X_pos, _ in it_hits:\n",
    "        X_pos = X_pos.to(device)\n",
    "        B = X_pos.size(0)\n",
    "        if typed:\n",
    "            h, t_true = X_pos[:, 0], X_pos[:, 2]\n",
    "        else:\n",
    "            h, t_true = X_pos[:, 0], X_pos[:, 1]\n",
    "\n",
    "        rand_t = torch.randint(0, num_entities, (B, 99), device=device)\n",
    "        cand_t = torch.cat([t_true.view(-1, 1), rand_t], dim=1)  # [B,100]\n",
    "\n",
    "        e_h = z[h]                      # [B,d]\n",
    "        e_c = z[cand_t]                 # [B,100,d]\n",
    "        s = (e_h.unsqueeze(1) * e_c).sum(dim=2)  # [B,100]\n",
    "\n",
    "        ranks = s.argsort(dim=1, descending=True)\n",
    "        true_positions = torch.nonzero(ranks == 0, as_tuple=False)[:, 1] + 1  # 1-based rank\n",
    "        hits1  += (true_positions <= 1).sum().item()\n",
    "        hits5  += (true_positions <= 5).sum().item()\n",
    "        hits10 += (true_positions <= 10).sum().item()\n",
    "        trials += B\n",
    "\n",
    "    return {\n",
    "        \"AUC\": auc,\n",
    "        \"Hits@1\": hits1 / max(trials, 1),\n",
    "        \"Hits@5\": hits5 / max(trials, 1),\n",
    "        \"Hits@10\": hits10 / max(trials, 1),\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f2823d01ce4d4d09",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "def _safe_filename(s: str) -> str:\n",
    "    # replace characters that can be annoying in shells/IDEs\n",
    "    return re.sub(r'[^A-Za-z0-9._\\-=/]', '_', s)\n",
    "\n",
    "def train_linkpred(\n",
    "        encoder: torch.nn.Module,\n",
    "        dm,                                      # KGDataModuleCollapsed or KGDataModuleTyped\n",
    "        epochs: int = 100,\n",
    "        lr: float = 1e-3,\n",
    "        weight_decay: float = 1e-4,\n",
    "        patience: int = 10,\n",
    "        device: Optional[torch.device] = None,\n",
    "        show_tqdm: bool = True,\n",
    "        save_best_path: Optional[str | Path] = None,\n",
    "        save_on_improve: bool = True,\n",
    "        hparams: Optional[Dict[str, Any]] = None,         # <<< NEW\n",
    ") -> Dict[str, Any]:\n",
    "    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    encoder = encoder.to(device)\n",
    "    opt = torch.optim.Adam(encoder.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    edge_index = build_edge_index_from_dm(dm).to(device)\n",
    "    typed = hasattr(dm, \"_train_triples\")\n",
    "\n",
    "    train_loader = dm.train_loader()\n",
    "    val_loader   = dm.val_loader()\n",
    "    num_entities = len(dm.ent2id)\n",
    "\n",
    "    # --------- Build/augment hparams ---------\n",
    "    auto_hparams: Dict[str, Any] = {\n",
    "        \"model_name\": encoder.__class__.__name__,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"lr\": lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"epochs\": epochs,\n",
    "        \"patience\": patience,\n",
    "        \"typed_graph\": typed,\n",
    "        \"batch_size\": getattr(dm, \"batch_size\", None),\n",
    "        \"add_reverse\": getattr(dm, \"add_reverse\", None),\n",
    "        \"reverse_relation_strategy\": getattr(dm, \"reverse_relation_strategy\", None),\n",
    "        \"num_nodes\": getattr(getattr(encoder, \"embed\", None), \"num_embeddings\", None),\n",
    "        \"emb_dim\": getattr(getattr(encoder, \"embed\", None), \"embedding_dim\", None),\n",
    "        \"num_layers\": len(getattr(encoder, \"convs\", [])),\n",
    "        \"dropout\": getattr(encoder, \"dropout\", None).__dict__.get(\"p\", None) if hasattr(encoder, \"dropout\") else None,\n",
    "    }\n",
    "    # user-supplied hparams override auto\n",
    "    run_hparams = {**auto_hparams, **(hparams or {})}\n",
    "\n",
    "    history = []\n",
    "    best = {\"epoch\": 0, \"AUC\": -1.0, \"Hits@1\": 0.0, \"Hits@5\": 0.0, \"Hits@10\": 0.0}\n",
    "    patience_ctr = 0\n",
    "    best_state = None\n",
    "\n",
    "    save_best_path = Path(save_best_path) if save_best_path else None\n",
    "    if save_best_path:\n",
    "        save_best_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    epoch_iter = range(1, epochs + 1)\n",
    "    if show_tqdm:\n",
    "        epoch_iter = tqdm(epoch_iter, desc=\"Epochs\")\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    for epoch in epoch_iter:\n",
    "        encoder.train()\n",
    "        running_loss = 0.0\n",
    "        running_n    = 0\n",
    "\n",
    "        epoch_iter = tqdm(range(1, epochs + 1), desc=\"Epochs\", dynamic_ncols=True)\n",
    "\n",
    "        batch_iter = tqdm(\n",
    "            train_loader,\n",
    "            desc=f\"Train {epoch}\",\n",
    "            total=len(train_loader),\n",
    "            dynamic_ncols=True,\n",
    "            leave=False,\n",
    "            mininterval=0.2,\n",
    "        )\n",
    "        if show_tqdm:\n",
    "            batch_iter = tqdm(train_loader, leave=False, desc=f\"Train {epoch}\")\n",
    "\n",
    "        for X_pos, _ in batch_iter:\n",
    "            X_pos = X_pos.to(device)\n",
    "            X_neg = sample_negatives(X_pos, num_entities, typed=typed).to(device)\n",
    "\n",
    "            z = encoder(edge_index)\n",
    "            s_pos = dot_scores(z, X_pos, typed=typed)\n",
    "            s_neg = dot_scores(z, X_neg, typed=typed)\n",
    "\n",
    "            scores = torch.cat([s_pos, s_neg], dim=0)\n",
    "            labels = torch.cat([torch.ones_like(s_pos), torch.zeros_like(s_neg)], dim=0)\n",
    "\n",
    "            loss = F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1.0)\n",
    "            opt.step()\n",
    "\n",
    "            running_loss += loss.item() * labels.numel()\n",
    "            running_n    += labels.numel()\n",
    "            batch_iter.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        train_loss = running_loss / max(running_n, 1)\n",
    "\n",
    "        # ---- Validation\n",
    "        val_metrics = evaluate_metrics(encoder, edge_index, val_loader, num_entities, typed, device, show_tqdm=show_tqdm)\n",
    "\n",
    "        history.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": float(train_loss),\n",
    "            \"val_auc\": float(val_metrics[\"AUC\"]),\n",
    "            \"val_hits1\": float(val_metrics[\"Hits@1\"]),\n",
    "            \"val_hits5\": float(val_metrics[\"Hits@5\"]),\n",
    "            \"val_hits10\": float(val_metrics[\"Hits@10\"]),\n",
    "        })\n",
    "\n",
    "        # ---- Early stopping on Hits@10\n",
    "        if val_metrics[\"Hits@10\"] > best[\"Hits@10\"]:\n",
    "            best.update({\"epoch\": epoch, **val_metrics})\n",
    "            best_state = {k: v.detach().cpu() for k, v in encoder.state_dict().items()}\n",
    "            patience_ctr = 0\n",
    "            \n",
    "\n",
    "            # SAVE on improvement (with hparams)\n",
    "            if save_best_path and save_on_improve:\n",
    "\n",
    "                if save_best_path:\n",
    "                    save_best_path = Path(_safe_filename(str(save_best_path)))\n",
    "                    save_best_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"model_state_dict\": best_state,\n",
    "                        \"epoch\": epoch,\n",
    "                        \"best_metrics\": best,\n",
    "                        \"history\": history,\n",
    "                        \"hparams\": run_hparams,      # <<< save hparams\n",
    "                        \"timestamp\": datetime.now().isoformat(),\n",
    "                    },\n",
    "                    save_best_path,\n",
    "                )\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= patience:\n",
    "                if show_tqdm:\n",
    "                    tqdm.write(f\"Early stopping at epoch {epoch} (patience={patience}).\")\n",
    "                break\n",
    "\n",
    "    # ---- Restore best into encoder\n",
    "    end_time = datetime.now()\n",
    "    if best_state is not None:\n",
    "        encoder.load_state_dict(best_state)\n",
    "        if show_tqdm:\n",
    "            tqdm.write(f\"Restored best model from epoch {best['epoch']} | \"\n",
    "                       f\"AUC={best['AUC']:.4f} | Hits@10={best['Hits@10']:.4f}\")\n",
    "\n",
    "        # SAVE final best if user wanted single save at end\n",
    "        if save_best_path and not save_on_improve:\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model_state_dict\": best_state,\n",
    "                    \"epoch\": best[\"epoch\"],\n",
    "                    \"best_metrics\": best,\n",
    "                    \"history\": history,\n",
    "                    \"hparams\": run_hparams,      # <<< save hparams\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                },\n",
    "                save_best_path,\n",
    "            )\n",
    "            if show_tqdm:\n",
    "                tqdm.write(f\"Saved final best checkpoint to {save_best_path}\")\n",
    "\n",
    "    return {\n",
    "        \"best\": best,\n",
    "        \"history\": history,\n",
    "        \"epochs_trained\": history[-1][\"epoch\"] if history else 0,\n",
    "        \"start_time\": start_time,\n",
    "        \"end_time\": end_time,\n",
    "        \"checkpoint_path\": str(save_best_path) if save_best_path else None,\n",
    "        \"hparams\": run_hparams,                 # <<< return hparams\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3cb936d2f7a8ba56",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "from dataset_loader import KGDataModuleCollapsed\n",
    "\n",
    "dataset = 'WN18RR'\n",
    "train_p = Path(\"../WN18RR/train.txt\")\n",
    "valid_p = Path(\"../WN18RR/valid.txt\")\n",
    "test_p  = Path(\"../WN18RR/test.txt\")\n",
    "\n",
    "\n",
    "dm = KGDataModuleCollapsed(train_p, valid_p, test_p, add_reverse=True)\n",
    "hidden_layers,num_layers, emb_dim = 3, 4, 128\n",
    "encoder = GINEncoder(num_nodes=len(dm.ent2id), hidden_layers=hidden_layers, emb_dim=emb_dim, num_layers=num_layers, dropout=0.1)\n",
    "\n",
    "result = train_linkpred(\n",
    "    encoder, dm,\n",
    "    epochs=100, lr=1e-3, weight_decay=1e-4, patience=10,\n",
    "    show_tqdm=True,\n",
    "    save_best_path=f\"checkpoints/checkpoints_gin/{dataset}/gin_best_embed_dim={emb_dim}|mlp={hidden_layers}|aggre={num_layers}|dataset={dataset}.pt\",\n",
    "    save_on_improve=True                        # save every improvement\n",
    ")\n",
    "print_training_report(\"GIN\", result, save_path=f\"results/results_gin/{dataset}/gin_e={emb_dim}_m={hidden_layers}_a={num_layers}.txt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c335144f498e2b81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "e5202e61aff2f6be",
   "metadata": {},
   "source": [
    "dm = KGDataModuleCollapsed(train_p, valid_p, test_p, add_reverse=True)\n",
    "hidden_layers,num_layers, emb_dim = 2, 4, 128\n",
    "encoder = GINEncoder(num_nodes=len(dm.ent2id), hidden_layers=hidden_layers, emb_dim=emb_dim, num_layers=num_layers, dropout=0.1)\n",
    "\n",
    "result = train_linkpred(\n",
    "    encoder, dm,\n",
    "    epochs=100, lr=1e-3, weight_decay=1e-4, patience=10,\n",
    "    show_tqdm=True,\n",
    "    save_best_path=f\"checkpoints/checkpoints_gin/{dataset}/gin_best_embed_dim={emb_dim}|mlp={hidden_layers}|aggre={num_layers}.pt\",\n",
    "    save_on_improve=True                        # save every improvement\n",
    ")\n",
    "print_training_report(\"GIN\", result, save_path=f\"results/results_gin/{dataset}/gin_e={emb_dim}_m={hidden_layers}_a={num_layers}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d18aee001c051059",
   "metadata": {},
   "source": [
    "dm = KGDataModuleCollapsed(train_p, valid_p, test_p, add_reverse=True)\n",
    "hidden_layers,num_layers, emb_dim = 4, 4, 128\n",
    "encoder = GINEncoder(num_nodes=len(dm.ent2id), hidden_layers=hidden_layers, emb_dim=emb_dim, num_layers=num_layers, dropout=0.1)\n",
    "\n",
    "result = train_linkpred(\n",
    "    encoder, dm,\n",
    "    epochs=100, lr=1e-3, weight_decay=1e-4, patience=10,\n",
    "    show_tqdm=True,\n",
    "    save_best_path=f\"checkpoints/checkpoints_gin/{dataset}/gin_best_embed_dim={emb_dim}|mlp={hidden_layers}|aggre={num_layers}.pt\",\n",
    "    save_on_improve=True                        # save every improvement\n",
    ")\n",
    "print_training_report(\"GIN\", result, save_path=f\"results/results_gin/{dataset}/gin_e={emb_dim}_m={hidden_layers}_a={num_layers}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1324547093fe738c",
   "metadata": {},
   "source": [
    "dm = KGDataModuleCollapsed(train_p, valid_p, test_p, add_reverse=True)\n",
    "hidden_layers,num_layers, emb_dim = 3, 3, 128\n",
    "encoder = GINEncoder(num_nodes=len(dm.ent2id), hidden_layers=hidden_layers, emb_dim=emb_dim, num_layers=num_layers, dropout=0.1)\n",
    "\n",
    "result = train_linkpred(\n",
    "    encoder, dm,\n",
    "    epochs=100, lr=1e-3, weight_decay=1e-4, patience=10,\n",
    "    show_tqdm=True,\n",
    "    save_best_path=f\"checkpoints/checkpoints_gin/{dataset}/gin_best_embed_dim={emb_dim}|mlp={hidden_layers}|aggre={num_layers}.pt\",\n",
    "    save_on_improve=True                        # save every improvement\n",
    ")\n",
    "print_training_report(\"GIN\", result, save_path=f\"results/results_gin/{dataset}/gin_e={emb_dim}_m={hidden_layers}_a={num_layers}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4b91460028aaf06b",
   "metadata": {},
   "source": [
    "dm = KGDataModuleCollapsed(train_p, valid_p, test_p, add_reverse=True)\n",
    "hidden_layers,num_layers, emb_dim = 3, 5, 128\n",
    "encoder = GINEncoder(num_nodes=len(dm.ent2id), hidden_layers=hidden_layers, emb_dim=emb_dim, num_layers=num_layers, dropout=0.1)\n",
    "\n",
    "result = train_linkpred(\n",
    "    encoder, dm,\n",
    "    epochs=100, lr=1e-3, weight_decay=1e-4, patience=10,\n",
    "    show_tqdm=True,\n",
    "    save_best_path=f\"checkpoints/checkpoints_gin/{dataset}/gin_best_embed_dim={emb_dim}|mlp={hidden_layers}|aggre={num_layers}.pt\",\n",
    "    save_on_improve=True                        # save every improvement\n",
    ")\n",
    "print_training_report(\"GIN\", result, save_path=f\"results/results_gin/{dataset}/gin_e={emb_dim}_m={hidden_layers}_a={num_layers}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "89b3164d1423b109",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6c6b1e2c0bcf93a0",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ae4d9fb40f6140c3",
   "metadata": {},
   "source": [
    "# FB15K-137 training"
   ]
  },
  {
   "cell_type": "code",
   "id": "1e30e0a628b9b375",
   "metadata": {},
   "source": [
    "train_p = Path(\"../FB15K-237/train.txt\")\n",
    "valid_p = Path(\"../FB15K-237/valid.txt\")\n",
    "test_p  = Path(\"../FB15K-237/test.txt\")\n",
    "\n",
    "dataset = \"FB15K-237\"\n",
    "\n",
    "dm = KGDataModuleCollapsed(train_p, valid_p, test_p, add_reverse=True, batch_size=4096)\n",
    "hidden_layers,num_layers, emb_dim = 2, 3, 128\n",
    "encoder = GINEncoder(num_nodes=len(dm.ent2id), hidden_layers=hidden_layers, emb_dim=emb_dim, num_layers=num_layers, dropout=0.1)\n",
    "\n",
    "result = train_linkpred(\n",
    "    encoder, dm,\n",
    "    epochs=100, lr=1e-3, weight_decay=1e-4, patience=10,\n",
    "    show_tqdm=True,\n",
    "    save_best_path=f\"checkpoints/checkpoints_gin/{dataset}/gin_best_embed_dim={emb_dim}|mlp={hidden_layers}|aggre={num_layers}.pt\",\n",
    "    save_on_improve=True                        # save every improvement\n",
    ")\n",
    "print_training_report(\"GIN\", result, save_path=f\"results/results_gin/{dataset}/gin_e={emb_dim}_mlp={hidden_layers}_ag={num_layers}.txt\")\n",
    "\n",
    "dm = KGDataModuleCollapsed(train_p, valid_p, test_p, add_reverse=True)\n",
    "hidden_layers,num_layers, emb_dim = 3, 3, 128\n",
    "encoder = GINEncoder(num_nodes=len(dm.ent2id), hidden_layers=hidden_layers, emb_dim=emb_dim, num_layers=num_layers, dropout=0.1)\n",
    "\n",
    "result = train_linkpred(\n",
    "    encoder, dm,\n",
    "    epochs=100, lr=1e-3, weight_decay=1e-4, patience=10,\n",
    "    show_tqdm=True,\n",
    "    save_best_path=f\"checkpoints/checkpoints_gin/{dataset}/gin_best_embed_dim={emb_dim}|mlp={hidden_layers}|aggre={num_layers}.pt\",\n",
    "    save_on_improve=True                        # save every improvement\n",
    ")\n",
    "print_training_report(\"GIN\", result, save_path=f\"results/results_gin/{dataset}/gin_e={emb_dim}_mlp={hidden_layers}_ag={num_layers}.txt\")\n",
    "\n",
    "dm = KGDataModuleCollapsed(train_p, valid_p, test_p, add_reverse=True)\n",
    "hidden_layers,num_layers, emb_dim = 4, 3, 128\n",
    "encoder = GINEncoder(num_nodes=len(dm.ent2id), hidden_layers=hidden_layers, emb_dim=emb_dim, num_layers=num_layers, dropout=0.1)\n",
    "\n",
    "result = train_linkpred(\n",
    "    encoder, dm,\n",
    "    epochs=100, lr=1e-3, weight_decay=1e-4, patience=10,\n",
    "    show_tqdm=True,\n",
    "    save_best_path=f\"checkpoints/checkpoints_gin/{dataset}/gin_best_embed_dim={emb_dim}|mlp={hidden_layers}|aggre={num_layers}.pt\",\n",
    "    save_on_improve=True                        # save every improvement\n",
    ")\n",
    "print_training_report(\"GIN\", result, save_path=f\"results/results_gin/{dataset}/gin_e={emb_dim}_mlp={hidden_layers}_ag={num_layers}.txt\")\n",
    "\n",
    "dm = KGDataModuleCollapsed(train_p, valid_p, test_p, add_reverse=True)\n",
    "hidden_layers,num_layers, emb_dim = 3, 2, 128\n",
    "encoder = GINEncoder(num_nodes=len(dm.ent2id), hidden_layers=hidden_layers, emb_dim=emb_dim, num_layers=num_layers, dropout=0.1)\n",
    "\n",
    "result = train_linkpred(\n",
    "    encoder, dm,\n",
    "    epochs=100, lr=1e-3, weight_decay=1e-4, patience=10,\n",
    "    show_tqdm=True,\n",
    "    save_best_path=f\"checkpoints/checkpoints_gin/{dataset}/gin_best_embed_dim={emb_dim}|mlp={hidden_layers}|aggre={num_layers}.pt\",\n",
    "    save_on_improve=True                        # save every improvement\n",
    ")\n",
    "print_training_report(\"GIN\", result, save_path=f\"results/results_gin/{dataset}/gin_e={emb_dim}_mlp={hidden_layers}_ag={num_layers}.txt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7a74485531b9bc5c",
   "metadata": {},
   "source": [
    "# cora_to_kg.py\n",
    "# Convert Cora (PyG) into WN18RR-style TSV triples your loader can read.\n",
    "\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.utils import coalesce\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# ------------------------- config -------------------------\n",
    "root = Path(\"./data\")\n",
    "out_dir = root / \"CORA_KG\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "relation_name = \"cites\"            # single relation\n",
    "is_undirected_split = True         # safer split for citation graphs\n",
    "add_reverse_edges = True           # also write reverse edges\n",
    "reverse_relation_strategy = \"duplicate_rel\"  # or \"same_rel\"\n",
    "\n",
    "# --------------------- load + coalesce --------------------\n",
    "print(\"ðŸ“¥ Loading Cora via PyG (auto-download if needed)...\")\n",
    "dataset = Planetoid(root=str(root), name=\"Cora\")\n",
    "data = dataset[0]\n",
    "\n",
    "# coalesce deduplicates edges; keep them as-is (directed list)\n",
    "edge_index, _ = coalesce(data.edge_index, None, data.num_nodes, data.num_nodes)\n",
    "\n",
    "# Build a Data object with the coalesced edges (older PyG has no .replace)\n",
    "new_data = Data(\n",
    "    x=data.x,\n",
    "    y=data.y,\n",
    "    edge_index=edge_index,\n",
    "    num_nodes=data.num_nodes,\n",
    ")\n",
    "\n",
    "print(f\"âœ… Cora: num_nodes={data.num_nodes}, edges={edge_index.size(1)}\")\n",
    "\n",
    "# ------------------- train/val/test split -----------------\n",
    "splitter = RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    is_undirected=is_undirected_split,\n",
    "    add_negative_train_samples=False,  # you sample negatives yourself\n",
    ")\n",
    "train_g, val_g, test_g = splitter(new_data)\n",
    "\n",
    "def pos_edges(g: Data) -> torch.Tensor:\n",
    "    # RandomLinkSplit attaches edge_label and edge_label_index\n",
    "    mask = (g.edge_label == 1)\n",
    "    return g.edge_label_index[:, mask]  # [2, E_pos]\n",
    "\n",
    "train_edges = pos_edges(train_g)\n",
    "val_edges   = pos_edges(val_g)\n",
    "test_edges  = pos_edges(test_g)\n",
    "\n",
    "print(f\"ðŸ“Š Splits: train={train_edges.size(1)}, val={val_edges.size(1)}, test={test_edges.size(1)}\")\n",
    "\n",
    "# -------------------- triples + saving --------------------\n",
    "def make_triples(edge_idx: torch.Tensor,\n",
    "                 rel: str,\n",
    "                 add_rev: bool,\n",
    "                 rev_strategy: str) -> list[tuple[str, str, str]]:\n",
    "    triples = []\n",
    "    h_list = edge_idx[0].tolist()\n",
    "    t_list = edge_idx[1].tolist()\n",
    "    for h, t in zip(h_list, t_list):\n",
    "        triples.append((f\"n{h}\", rel, f\"n{t}\"))\n",
    "        if add_rev:\n",
    "            if rev_strategy == \"duplicate_rel\":\n",
    "                triples.append((f\"n{t}\", rel + \"_rev\", f\"n{h}\"))\n",
    "            else:  # same_rel\n",
    "                triples.append((f\"n{t}\", rel, f\"n{h}\"))\n",
    "    return triples\n",
    "\n",
    "def save_triples(triples: list[tuple[str, str, str]], path: Path) -> None:\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f, delimiter=\"\\t\")\n",
    "        writer.writerows(triples)\n",
    "    print(f\"ðŸ’¾ Saved {len(triples):,} triples -> {path}\")\n",
    "\n",
    "train_triples = make_triples(train_edges, relation_name, add_reverse_edges, reverse_relation_strategy)\n",
    "val_triples   = make_triples(val_edges,   relation_name, add_reverse_edges, reverse_relation_strategy)\n",
    "test_triples  = make_triples(test_edges,  relation_name, add_reverse_edges, reverse_relation_strategy)\n",
    "\n",
    "save_triples(train_triples, out_dir / \"train.txt\")\n",
    "save_triples(val_triples,   out_dir / \"valid.txt\")\n",
    "save_triples(test_triples,  out_dir / \"test.txt\")\n",
    "\n",
    "print(\"âœ… Done. Files are WN18RR-style and compatible with your KGDataModuleTyped.\")\n",
    "print(f\"Use paths:\\n  train: {out_dir/'train.txt'}\\n  valid: {out_dir/'valid.txt'}\\n  test : {out_dir/'test.txt'}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f0a72d9d75a0e6e0",
   "metadata": {},
   "source": [
    "from dataset_loader import KGDataModuleCollapsed\n",
    "\n",
    "dataset = \"Cora\"\n",
    "\n",
    "train_p =  Path(\"data/CORA_KG/train.txt\")\n",
    "valid_p = Path(\"data/CORA_KG/valid.txt\")\n",
    "test_p = Path(\"data/CORA_KG/test.txt\")\n",
    "\n",
    "dm = KGDataModuleCollapsed(train_p, valid_p, test_p, add_reverse=True, batch_size=256)\n",
    "hidden_layers,num_layers, emb_dim = 3, 5, 128\n",
    "encoder = GINEncoder(num_nodes=len(dm.ent2id), hidden_layers=hidden_layers, emb_dim=emb_dim, num_layers=num_layers, dropout=0.1)\n",
    "\n",
    "result = train_linkpred(\n",
    "    encoder, dm,\n",
    "    epochs=200, lr=1e-4, weight_decay=1e-4, patience=10,\n",
    "    show_tqdm=True,\n",
    "    save_best_path=f\"checkpoints/checkpoints_gin/{dataset}/gin_best_embed_dim={emb_dim}|mlp={hidden_layers}|aggre={num_layers}.pt\",\n",
    "    save_on_improve=True                        # save every improvement\n",
    ")\n",
    "print_training_report(\"GIN\", result, save_path=f\"results/results_gin/{dataset}/gin_e={emb_dim}_mlp={hidden_layers}_ag={num_layers}.txt\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1cbacd887a69cf28",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41d71042-39ad-4978-83bb-846fafe6fc5a",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

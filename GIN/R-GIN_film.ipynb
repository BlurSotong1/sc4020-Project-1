{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---- trainer_film_gin.py ----------------------------------------------------\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    _HAS_TQDM = True\n",
    "except Exception:  # pragma: no cover\n",
    "    _HAS_TQDM = False\n",
    "    def tqdm(x, **k): return x\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    _HAS_SK = True\n",
    "except Exception:  # pragma: no cover\n",
    "    _HAS_SK = False\n",
    "\n",
    "# ---------------------------\n",
    "# FiLM-style Relational GIN\n",
    "# ---------------------------\n",
    "class RelationalFiLMGINConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    Relation-aware GIN with FiLM modulation:\n",
    "      gamma_r, beta_r = MLP_rel(r_emb[r])\n",
    "      m_{j->i} = gamma_r ⊙ (W x_j) + beta_r\n",
    "      x_i' = MLP_node( (1 + eps) * x_i + sum_j m_{j->i} )\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            emb_dim: int,\n",
    "            num_relations: int,\n",
    "            hidden_layers: int = 1,   # depth of relation MLP (>=1)\n",
    "            train_eps: bool = True\n",
    "    ):\n",
    "        super().__init__(aggr=\"add\")  # GIN uses sum aggregation\n",
    "\n",
    "        # relation embeddings (one per edge type)\n",
    "        self.rel_emb = nn.Embedding(num_relations, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.rel_emb.weight)\n",
    "\n",
    "        # relation MLP that outputs [gamma | beta] of size 2 * emb_dim\n",
    "        rel_layers = [nn.Linear(emb_dim, emb_dim * 2), nn.ReLU()]\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            rel_layers += [nn.Linear(emb_dim * 2, emb_dim * 2), nn.ReLU()]\n",
    "        rel_layers += [nn.Linear(emb_dim * 2, emb_dim * 2)]\n",
    "        self.rel_mlp = nn.Sequential(*rel_layers)\n",
    "\n",
    "        # shared linear transform on node features\n",
    "        self.W = nn.Linear(emb_dim, emb_dim)\n",
    "\n",
    "        # node-side GIN MLP (kept simple; you can deepen if you like)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim * 2, emb_dim),\n",
    "        )\n",
    "\n",
    "        # learnable epsilon like classic GIN\n",
    "        if train_eps:\n",
    "            self.eps = nn.Parameter(torch.zeros(1))\n",
    "        else:\n",
    "            self.register_buffer(\"eps\", torch.zeros(1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_type: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [N, d]\n",
    "        edge_index: [2, E]\n",
    "        edge_type: [E]  (relation id for each edge)\n",
    "        \"\"\"\n",
    "        # propagate will call message(...) then aggregate, then update(...)\n",
    "        return self.propagate(edge_index, x=x, edge_type=edge_type)\n",
    "\n",
    "    def message(self, x_j: torch.Tensor, edge_type: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x_j: [E, d] features of source nodes (neighbors)\n",
    "        edge_type: [E] relation ids aligned with edges\n",
    "        \"\"\"\n",
    "        r = self.rel_emb(edge_type)                 # [E, d]\n",
    "        gamma_beta = self.rel_mlp(r)                # [E, 2d]\n",
    "        gamma, beta = gamma_beta.chunk(2, dim=-1)   # each [E, d]\n",
    "        return gamma * self.W(x_j) + beta           # FiLM message\n",
    "\n",
    "    def update(self, aggr_out: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        aggr_out: [N, d] summed messages\n",
    "        x:         [N, d] original node features, passed from propagate via kwargs\n",
    "        \"\"\"\n",
    "        out = (1.0 + self.eps) * x + aggr_out\n",
    "        return self.mlp(out)\n",
    "\n",
    "\n",
    "class RelationalGINEncoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_nodes: int,\n",
    "            num_relations: int,\n",
    "            emb_dim: int = 128,\n",
    "            num_layers: int = 3,\n",
    "            hidden_layers: int = 1,   # depth for the relation FiLM MLP\n",
    "            dropout: float = 0.1,\n",
    "            train_eps: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(num_nodes, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.embed.weight)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            RelationalFiLMGINConv(\n",
    "                emb_dim=emb_dim,\n",
    "                num_relations=num_relations,\n",
    "                hidden_layers=hidden_layers,\n",
    "                train_eps=train_eps\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, edge_index: torch.Tensor, edge_type: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embed.weight\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, edge_type)\n",
    "            x = self.dropout(x)\n",
    "        return x  # [N, emb_dim]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _fmt_ts(dt: datetime) -> str:\n",
    "    return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def _fmt(x):\n",
    "    try:\n",
    "        return f\"{float(x):.4f}\"\n",
    "    except Exception:\n",
    "        return \"nan\"\n",
    "\n",
    "def print_comparison_report(\n",
    "        title: str,\n",
    "        left_name: str, left_result: Dict[str, Any],\n",
    "        right_name: str, right_result: Dict[str, Any],\n",
    "        save_path: Optional[str | Path] = None,\n",
    "):\n",
    "    ts = _fmt_ts(datetime.now())\n",
    "\n",
    "    def block(name, res):\n",
    "        best = res[\"best\"]; hist = res[\"history\"]\n",
    "        best_auc = max((h.get(\"val_auc\", float(\"nan\")) for h in hist), default=float(\"nan\"))\n",
    "        total_epochs = res.get(\"epochs_trained\", len(hist))\n",
    "\n",
    "        lines = []\n",
    "        lines.append(f\"{name} Training History\")\n",
    "        lines.append(\"=\" * 60)\n",
    "        lines.append(\"\")\n",
    "        lines.append(f\"Best Validation AUC: {_fmt(best_auc)}\")\n",
    "        lines.append(f\"Total Epochs Trained: {total_epochs}\")\n",
    "        lines.append(f\"Early Stopping Best Score: {_fmt(best.get('Hits@10'))} (Hits@10 at epoch {best.get('epoch')})\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"-\" * 90)\n",
    "        lines.append(f\"{'Epoch':<8} {'Train Loss':<14} {'Val AUC':<12} {'Val H@1':<12} {'Val H@5':<12} {'Val H@10':<12}\")\n",
    "        lines.append(\"-\" * 90)\n",
    "        for rec in hist:\n",
    "            e = rec.get(\"epoch\")\n",
    "            lines.append(\n",
    "                f\"{e:<8} \"\n",
    "                f\"{_fmt(rec.get('train_loss')):<14} \"\n",
    "                f\"{_fmt(rec.get('val_auc')):<12} \"\n",
    "                f\"{_fmt(rec.get('val_hits1')):<12} \"\n",
    "                f\"{_fmt(rec.get('val_hits5')):<12} \"\n",
    "                f\"{_fmt(rec.get('val_hits10')):<12}\"\n",
    "            )\n",
    "        lines.append(\"\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    out = []\n",
    "    out.append(f\"{title} - {ts}\")\n",
    "    out.append(\"=\" * 80)\n",
    "    out.append(\"\")\n",
    "    out.append(block(left_name, left_result))\n",
    "    out.append(block(right_name, right_result))\n",
    "\n",
    "    # Best-at-a-glance\n",
    "    out.append(\"Best Validation Metrics Summary\")\n",
    "    out.append(\"=\" * 60)\n",
    "    for name, res in [(left_name, left_result), (right_name, right_result)]:\n",
    "        b = res[\"best\"]\n",
    "        out.append(\n",
    "            f\"{name}: \"\n",
    "            f\"AUC={_fmt(b.get('AUC'))} | \"\n",
    "            f\"H@1={_fmt(b.get('Hits@1'))} | \"\n",
    "            f\"H@5={_fmt(b.get('Hits@5'))} | \"\n",
    "            f\"H@10={_fmt(b.get('Hits@10'))} \"\n",
    "            f\"(epoch {b.get('epoch')})\"\n",
    "        )\n",
    "    out.append(\"\")\n",
    "\n",
    "    report = \"\\n\".join(out)\n",
    "    print(report)\n",
    "\n",
    "    if save_path:\n",
    "        save_path = Path(save_path)\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(report)\n",
    "        print(f\"\\n✅ Comparison report saved to: {save_path.resolve()}\")\n",
    "\n",
    "\n",
    "def print_training_report(\n",
    "        model_name: str,\n",
    "        result: Dict[str, Any],\n",
    "        header_title: str = \"Model Training Results\",\n",
    "        save_path: Optional[str | Path] = None,\n",
    "):\n",
    "    ts = _fmt_ts(result.get(\"end_time\", datetime.now()))\n",
    "    best = result[\"best\"]\n",
    "    history = result[\"history\"]\n",
    "    total_epochs = result.get(\"epochs_trained\", len(history))\n",
    "    best_auc = max((h.get(\"val_auc\", float(\"nan\")) for h in history), default=float(\"nan\"))\n",
    "\n",
    "    lines = []\n",
    "    lines.append(f\"{header_title} - {ts}\")\n",
    "    lines.append(\"=\" * 80)\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"{model_name} Training History\")\n",
    "    lines.append(\"=\" * 60)\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"Best Validation AUC: {_fmt(best_auc)}\")\n",
    "    lines.append(f\"Total Epochs Trained: {total_epochs}\")\n",
    "    lines.append(f\"Early Stopping Best Score: {_fmt(best.get('Hits@10'))} (Hits@10 at epoch {best.get('epoch')})\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"-\" * 90)\n",
    "    lines.append(f\"{'Epoch':<8} {'Train Loss':<14} {'Val AUC':<12} {'Val H@1':<12} {'Val H@5':<12} {'Val H@10':<12}\")\n",
    "    lines.append(\"-\" * 90)\n",
    "\n",
    "    for rec in history:\n",
    "        e = rec.get(\"epoch\")\n",
    "        lines.append(\n",
    "            f\"{e:<8} \"\n",
    "            f\"{_fmt(rec.get('train_loss')):<14} \"\n",
    "            f\"{_fmt(rec.get('val_auc')):<12} \"\n",
    "            f\"{_fmt(rec.get('val_hits1')):<12} \"\n",
    "            f\"{_fmt(rec.get('val_hits5')):<12} \"\n",
    "            f\"{_fmt(rec.get('val_hits10')):<12}\"\n",
    "        )\n",
    "\n",
    "    lines.append(\"\")\n",
    "    report_text = \"\\n\".join(lines)\n",
    "\n",
    "    print(report_text)\n",
    "\n",
    "    if save_path:\n",
    "        save_path = Path(save_path)\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(report_text)\n",
    "        print(f\"\\n✅ Report saved to: {save_path.resolve()}\")\n",
    "\n",
    "\n",
    "# ---------- Build edge_index and edge_type from the *train* split ----------\n",
    "def build_edge_index_and_type_from_typed_dm(dm) -> tuple[torch.LongTensor, torch.LongTensor]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        edge_index: [2, E] directed edges\n",
    "        edge_type:  [E]    relation id per edge (aligned with edge_index columns)\n",
    "    Uses dm._train_triples directly (already contains reverse triples if add_reverse=True).\n",
    "    \"\"\"\n",
    "    assert hasattr(dm, \"_train_triples\"), \"KGDataModuleTyped expected.\"\n",
    "    triples = dm._train_triples  # [N, 3] (h, r, t), torch.long\n",
    "    if triples.numel() == 0:\n",
    "        return torch.empty(2, 0, dtype=torch.long), torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    h = triples[:, 0]\n",
    "    r = triples[:, 1]\n",
    "    t = triples[:, 2]\n",
    "\n",
    "    edge_index = torch.stack([h, t], dim=0).contiguous()  # directed edges h->t\n",
    "    edge_type = r.contiguous()                             # relation per edge\n",
    "    return edge_index, edge_type\n",
    "\n",
    "\n",
    "# ---------- Typed negative sampling (corrupt head/tail, keep relation) ----------\n",
    "@torch.no_grad()\n",
    "def sample_negatives_typed(triples: torch.Tensor, num_entities: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    1:1 negatives per positive (half head-corrupt, half tail-corrupt).\n",
    "    Input triples: [B,3] (h, r, t)\n",
    "    Output triples: [B,3] negatives (h', r, t) or (h, r, t')\n",
    "    \"\"\"\n",
    "    B = triples.size(0)\n",
    "    device = triples.device\n",
    "    neg = triples.clone()\n",
    "    flip = torch.rand(B, device=device) < 0.5\n",
    "    rand_ents = torch.randint(0, num_entities, (B,), device=device)\n",
    "\n",
    "    # corrupt head\n",
    "    neg[flip, 0] = rand_ents[flip]\n",
    "    # corrupt tail\n",
    "    neg[~flip, 2] = rand_ents[~flip]\n",
    "    return neg\n",
    "\n",
    "\n",
    "# ---------- DistMult decoder for typed link prediction ----------\n",
    "class DistMultDecoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    score(h, r, t) = <e_h, w_r, e_t> = sum_d e_h[d] * w_r[d] * e_t[d]\n",
    "    \"\"\"\n",
    "    def __init__(self, num_relations: int, dim: int):\n",
    "        super().__init__()\n",
    "        self.rel = torch.nn.Embedding(num_relations, dim)\n",
    "        torch.nn.init.xavier_uniform_(self.rel.weight)\n",
    "\n",
    "    def forward(self, z: torch.Tensor, triples: torch.LongTensor) -> torch.Tensor:\n",
    "        # triples: [B,3] (h, r, t)\n",
    "        h, r, t = triples[:, 0], triples[:, 1], triples[:, 2]\n",
    "        e_h, e_t, w_r = z[h], z[t], self.rel(r)\n",
    "        return (e_h * w_r * e_t).sum(dim=1)  # [B]\n",
    "\n",
    "class DotProductDecoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    score(h, r, t) = <e_h, e_t> (relation is ignored)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, z: torch.Tensor, triples: torch.LongTensor) -> torch.Tensor:\n",
    "        h, t = triples[:, 0], triples[:, 2]\n",
    "        return (z[h] * z[t]).sum(dim=1)  # [B]\n",
    "\n",
    "def scores_for_candidates(\n",
    "        decoder: torch.nn.Module,\n",
    "        z: torch.Tensor,\n",
    "        h: torch.Tensor,                # [B]\n",
    "        r: torch.Tensor,                # [B]\n",
    "        cand_t: torch.Tensor,           # [B, K]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns [B, K] scores for candidates.\n",
    "    Fast path for DistMult; generic path for DotProduct.\n",
    "    \"\"\"\n",
    "    if isinstance(decoder, DistMultDecoder):\n",
    "        e_h = z[h]                       # [B, d]\n",
    "        w_r = decoder.rel(r)            # [B, d]\n",
    "        e_c = z[cand_t]                 # [B, K, d]\n",
    "        s = ((e_h * w_r).unsqueeze(1) * e_c).sum(dim=2)  # [B, K]\n",
    "        return s\n",
    "    else:\n",
    "        # generic: build triples and call decoder\n",
    "        B, K = cand_t.shape\n",
    "        h_rep = h.view(B, 1).expand(B, K)\n",
    "        r_rep = r.view(B, 1).expand(B, K)  # unused by dot, but fine for API\n",
    "        triples = torch.stack([h_rep, r_rep, cand_t], dim=2).reshape(-1, 3)  # [B*K,3]\n",
    "        s = decoder(z, triples).view(B, K)\n",
    "        return s"
   ],
   "id": "3c02eb681e5195ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# assumes THESE are already defined/imported in your codebase:\n",
    "#   - build_edge_index_and_type_from_typed_dm(dm) -> (edge_index, edge_type)\n",
    "#   - sample_negatives_typed(triples, num_entities)\n",
    "#   - DistMultDecoder / scores_for_candidates\n",
    "#   - print_training_report / print_comparison_report\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_metrics_typed(\n",
    "        encoder: torch.nn.Module,\n",
    "        decoder: torch.nn.Module,\n",
    "        edge_index: torch.Tensor,\n",
    "        edge_type: torch.Tensor,\n",
    "        loader: Optional[DataLoader],\n",
    "        num_entities: int,\n",
    "        device: torch.device,\n",
    "        show_tqdm: bool = False,\n",
    "        sampled_k: int = 100,   # number of tails per query (1 gold + k-1 random)\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"AUC via 1:1 pos/neg; Hits@k via sampled ranking against (sampled_k-1) random tails.\"\"\"\n",
    "    if loader is None:\n",
    "        return {\"AUC\": float(\"nan\"), \"Hits@1\": float(\"nan\"), \"Hits@5\": float(\"nan\"), \"Hits@10\": float(\"nan\")}\n",
    "\n",
    "    encoder.eval(); decoder.eval()\n",
    "    z = encoder(edge_index.to(device), edge_type.to(device))  # [N, d]\n",
    "\n",
    "    # --- AUC ---\n",
    "    all_scores, all_labels = [], []\n",
    "    it_auc = loader if not show_tqdm else tqdm(loader, leave=False, desc=\"Eval AUC (typed)\")\n",
    "    for pos, _ in it_auc:\n",
    "        pos = pos.to(device)\n",
    "        neg = sample_negatives_typed(pos, num_entities)\n",
    "\n",
    "        s_pos = decoder(z, pos)\n",
    "        s_neg = decoder(z, neg)\n",
    "\n",
    "        all_scores.append(torch.cat([s_pos, s_neg]).detach().cpu().numpy())\n",
    "        all_labels.append(np.concatenate([np.ones(len(s_pos)), np.zeros(len(s_neg))]))\n",
    "\n",
    "    scores = np.concatenate(all_scores) if len(all_scores) > 0 else np.array([])\n",
    "    labels = np.concatenate(all_labels) if len(all_labels) > 0 else np.array([])\n",
    "    if _HAS_SK and len(scores) > 0:\n",
    "        auc = float(roc_auc_score(labels, scores))\n",
    "    else:\n",
    "        auc = float(\"nan\") if len(scores) == 0 else float((scores[labels == 1].mean() > scores[labels == 0].mean()))\n",
    "\n",
    "    # --- Hits@K (tail ranking, sampled) ---\n",
    "    hits1 = hits5 = hits10 = 0\n",
    "    trials = 0\n",
    "    it_hits = loader if not show_tqdm else tqdm(loader, leave=False, desc=\"Eval Hits@K (typed)\")\n",
    "    K = sampled_k\n",
    "    for pos, _ in it_hits:\n",
    "        pos = pos.to(device)\n",
    "        B = pos.size(0)\n",
    "        h, r, t_true = pos[:, 0], pos[:, 1], pos[:, 2]\n",
    "\n",
    "        # candidates = {true tail} ∪ {K-1 random tails}\n",
    "        rand_t = torch.randint(0, num_entities, (B, K - 1), device=device)\n",
    "        cand_t = torch.cat([t_true.view(-1, 1), rand_t], dim=1)  # [B, K]\n",
    "\n",
    "        s = scores_for_candidates(decoder, z, h, r, cand_t)  # [B, K]\n",
    "        ranks = s.argsort(dim=1, descending=True)\n",
    "        # position of column 0 (the true tail) in the argsorted index\n",
    "        true_positions = (ranks == 0).nonzero(as_tuple=False)[:, 1] + 1  # 1-based\n",
    "\n",
    "        hits1  += (true_positions <= 1).sum().item()\n",
    "        hits5  += (true_positions <= 5).sum().item()\n",
    "        hits10 += (true_positions <= 10).sum().item()\n",
    "        trials += B\n",
    "\n",
    "    return {\n",
    "        \"AUC\": auc,\n",
    "        \"Hits@1\": hits1 / max(trials, 1),\n",
    "        \"Hits@5\": hits5 / max(trials, 1),\n",
    "        \"Hits@10\": hits10 / max(trials, 1),\n",
    "    }\n",
    "\n",
    "\n",
    "def train_linkpred_film_gin(\n",
    "        encoder: torch.nn.Module,         # RelationalGINEncoder (FiLM GIN)\n",
    "        decoder: torch.nn.Module,         # e.g., DistMultDecoder\n",
    "        dm,                               # your KGDataModuleTyped instance\n",
    "        *,\n",
    "        epochs: int = 100,\n",
    "        lr: float = 1e-3,\n",
    "        weight_decay: float = 1e-4,\n",
    "        patience: int = 10,\n",
    "        device: Optional[torch.device] = None,\n",
    "        show_tqdm: bool = True,\n",
    "        save_best_path: Optional[str | Path] = None,\n",
    "        save_on_improve: bool = True,\n",
    "        hparams: Optional[Dict[str, Any]] = None,\n",
    "        eval_sampled_k: int = 100,        # eval ranking pool size (1 gold + K-1 random)\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Full-batch FiLM-GIN training loop for typed link prediction (1:1 negatives).\"\"\"\n",
    "    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(\n",
    "        list(encoder.parameters()) + list(decoder.parameters()),\n",
    "        lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # Graph from *train* triples only (avoid leakage)\n",
    "    edge_index, edge_type = build_edge_index_and_type_from_typed_dm(dm)\n",
    "    edge_index, edge_type = edge_index.to(device), edge_type.to(device)\n",
    "\n",
    "    train_loader = dm.train_loader()\n",
    "    val_loader   = dm.val_loader()\n",
    "    num_entities = len(dm.ent2id)\n",
    "    num_relations = len(dm.rel2id)\n",
    "\n",
    "    # hparams block for report\n",
    "    auto_hparams: Dict[str, Any] = {\n",
    "        \"model_name\": f\"{encoder.__class__.__name__}+{decoder.__class__.__name__}\",\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"lr\": lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"epochs\": epochs,\n",
    "        \"patience\": patience,\n",
    "        \"typed_graph\": True,\n",
    "        \"batch_size\": getattr(dm, \"batch_size\", None),\n",
    "        \"add_reverse\": getattr(dm, \"add_reverse\", None),\n",
    "        \"reverse_relation_strategy\": getattr(dm, \"reverse_relation_strategy\", None),\n",
    "        \"num_nodes\": num_entities,\n",
    "        \"num_relations\": num_relations,\n",
    "        \"enc_emb_dim\": getattr(getattr(encoder, \"embed\", None), \"embedding_dim\", None),\n",
    "        \"enc_num_layers\": len(getattr(encoder, \"convs\", [])),\n",
    "        \"decoder\": decoder.__class__.__name__,\n",
    "    }\n",
    "    run_hparams = {**auto_hparams, **(hparams or {})}\n",
    "\n",
    "    # bookkeeping\n",
    "    history: list[Dict[str, Any]] = []\n",
    "    best = {\"epoch\": 0, \"AUC\": -1.0, \"Hits@1\": 0.0, \"Hits@5\": 0.0, \"Hits@10\": 0.0}\n",
    "    patience_ctr = 0\n",
    "    best_state = None\n",
    "\n",
    "    save_best_path = Path(save_best_path) if save_best_path else None\n",
    "    if save_best_path:\n",
    "        save_best_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    epoch_iter = range(1, epochs + 1)\n",
    "    if show_tqdm and _HAS_TQDM:\n",
    "        epoch_iter = tqdm(epoch_iter, desc=\"Epochs (FiLM-GIN)\")\n",
    "\n",
    "    for epoch in epoch_iter:\n",
    "        encoder.train(); decoder.train()\n",
    "        running_loss = 0.0; running_n = 0\n",
    "\n",
    "        batch_iter = train_loader\n",
    "        if show_tqdm and _HAS_TQDM:\n",
    "            batch_iter = tqdm(train_loader, leave=False, desc=f\"Train {epoch}\")\n",
    "\n",
    "        # Full-batch encoder; recompute embeddings each step to reflect parameter updates\n",
    "        for pos, _ in batch_iter:\n",
    "            pos = pos.to(device)\n",
    "            neg = sample_negatives_typed(pos, num_entities).to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "\n",
    "            z = encoder(edge_index, edge_type)          # [N, d] (full graph)\n",
    "            s_pos = decoder(z, pos)                     # [B]\n",
    "            s_neg = decoder(z, neg)                     # [B]\n",
    "\n",
    "            scores = torch.cat([s_pos, s_neg], dim=0)\n",
    "            labels = torch.cat(\n",
    "                [torch.ones_like(s_pos), torch.zeros_like(s_neg)], dim=0\n",
    "            )\n",
    "            loss = F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(encoder.parameters()) + list(decoder.parameters()),\n",
    "                max_norm=1.0\n",
    "            )\n",
    "            opt.step()\n",
    "\n",
    "            running_loss += float(loss.item()); running_n += 1\n",
    "            if show_tqdm and _HAS_TQDM:\n",
    "                batch_iter.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        train_loss = running_loss / max(running_n, 1)\n",
    "\n",
    "        # ---- validation ----\n",
    "        val_metrics = evaluate_metrics_typed(\n",
    "            encoder, decoder, edge_index, edge_type,\n",
    "            val_loader, num_entities, device,\n",
    "            show_tqdm=show_tqdm and _HAS_TQDM, sampled_k=eval_sampled_k\n",
    "        )\n",
    "\n",
    "        if show_tqdm:\n",
    "            msg = (f\"Epoch {epoch:03d} | loss={train_loss:.4f} | \"\n",
    "                   f\"AUC={val_metrics['AUC']:.4f} | \"\n",
    "                   f\"H@1={val_metrics['Hits@1']:.4f} | \"\n",
    "                   f\"H@5={val_metrics['Hits@5']:.4f} | \"\n",
    "                   f\"H@10={val_metrics['Hits@10']:.4f}\")\n",
    "            if _HAS_TQDM:\n",
    "                tqdm.write(msg)\n",
    "            else:\n",
    "                print(msg)\n",
    "\n",
    "        history.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": float(train_loss),\n",
    "            \"val_auc\": float(val_metrics[\"AUC\"]),\n",
    "            \"val_hits1\": float(val_metrics[\"Hits@1\"]),\n",
    "            \"val_hits5\": float(val_metrics[\"Hits@5\"]),\n",
    "            \"val_hits10\": float(val_metrics[\"Hits@10\"]),\n",
    "        })\n",
    "\n",
    "        # early stopping on Hits@10\n",
    "        if val_metrics[\"Hits@10\"] > best[\"Hits@10\"]:\n",
    "            best.update({\"epoch\": epoch, **val_metrics})\n",
    "            best_state = {\n",
    "                \"encoder\": {k: v.detach().cpu() for k, v in encoder.state_dict().items()},\n",
    "                \"decoder\": {k: v.detach().cpu() for k, v in decoder.state_dict().items()},\n",
    "            }\n",
    "            patience_ctr = 0\n",
    "\n",
    "            if save_best_path and save_on_improve:\n",
    "                torch.save({\n",
    "                    \"encoder_state_dict\": best_state[\"encoder\"],\n",
    "                    \"decoder_state_dict\": best_state[\"decoder\"],\n",
    "                    \"epoch\": epoch,\n",
    "                    \"best_metrics\": best,\n",
    "                    \"history\": history,\n",
    "                    \"hparams\": run_hparams,\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                }, save_best_path)\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= patience:\n",
    "                if show_tqdm:\n",
    "                    msg = f\"Early stopping at epoch {epoch} (patience={patience}).\"\n",
    "                    if _HAS_TQDM: tqdm.write(msg)\n",
    "                    else: print(msg)\n",
    "                break\n",
    "\n",
    "    # restore best\n",
    "    end_time = datetime.now()\n",
    "    if best_state is not None:\n",
    "        encoder.load_state_dict(best_state[\"encoder\"])\n",
    "        decoder.load_state_dict(best_state[\"decoder\"])\n",
    "        if show_tqdm:\n",
    "            msg = (f\"Restored best model from epoch {best['epoch']} | \"\n",
    "                   f\"AUC={best['AUC']:.4f} | Hits@10={best['Hits@10']:.4f}\")\n",
    "            if _HAS_TQDM: tqdm.write(msg)\n",
    "            else: print(msg)\n",
    "\n",
    "        if save_best_path and not save_on_improve:\n",
    "            torch.save({\n",
    "                \"encoder_state_dict\": best_state[\"encoder\"],\n",
    "                \"decoder_state_dict\": best_state[\"decoder\"],\n",
    "                \"epoch\": best[\"epoch\"],\n",
    "                \"best_metrics\": best,\n",
    "                \"history\": history,\n",
    "                \"hparams\": run_hparams,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "            }, save_best_path)\n",
    "\n",
    "    return {\n",
    "        \"best\": best,\n",
    "        \"history\": history,\n",
    "        \"epochs_trained\": history[-1][\"epoch\"] if history else 0,\n",
    "        \"start_time\": start_time,\n",
    "        \"end_time\": end_time,\n",
    "        \"checkpoint_path\": str(save_best_path) if save_best_path else None,\n",
    "        \"hparams\": run_hparams,\n",
    "    }\n",
    "# ------------------------------------------------------------------------------"
   ],
   "id": "447c0ca541796d9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = \"WN18RR\"\n",
    "train_p = Path(\"../WN18RR/train.txt\")\n",
    "valid_p = Path(\"../WN18RR/valid.txt\")\n",
    "test_p  = Path(\"../WN18RR/test.txt\")\n",
    "\n",
    "from dataset_loader import KGDataModuleTyped\n",
    "\n",
    "\n",
    "dm_typed = KGDataModuleTyped(\n",
    "    train_p, valid_p, test_p,\n",
    "    add_reverse=True,\n",
    "    reverse_relation_strategy=\"duplicate_rel\",\n",
    ")\n",
    "\n",
    "num_nodes = len(dm_typed.ent2id)\n",
    "num_relations = len(dm_typed.rel2id)\n",
    "emb_dim = 128\n",
    "num_layers = 3\n",
    "hidden_layers = 3\n",
    "\n",
    "# --- 1) R-GIN + DistMult ---\n",
    "enc_dm = RelationalGINEncoder( num_nodes=num_nodes, num_relations=num_relations, emb_dim=emb_dim, num_layers=num_layers, hidden_layers=hidden_layers, dropout=0.1, train_eps=True\n",
    "                               )\n",
    "dec_dm = DistMultDecoder(num_relations=num_relations, dim=emb_dim)\n",
    "\n",
    "res_distmult = train_linkpred_film_gin(\n",
    "    enc_dm, dec_dm, dm_typed,\n",
    "    epochs=100, lr=1e-3, weight_decay=1e-4, patience=10,\n",
    "    show_tqdm=True,\n",
    "    save_best_path=f\"checkpoints/r-gin-film/{dataset}/best_rgin_distmult|embed_dim={emb_dim}|mlp_depth={hidden_layers}|aggre={num_layers}|dataset={dataset}.pt\",\n",
    "    save_on_improve=True,\n",
    "    hparams={\"dataset\": dataset, \"decoder\": \"DistMult\", \"emb_dim\": emb_dim, \"num_layers\": num_layers, \"hidden_layers\": hidden_layers}\n",
    ")\n",
    "\n",
    "\n",
    "print_training_report(\n",
    "    model_name = \"R-GIN_embed_rel + Distmult\",\n",
    "    result = res_distmult,\n",
    "    header_title = \"Model Training Results\",\n",
    "    save_path=f\"results/r-gin-film/{dataset}/film_rgin_report|embed_dim={emb_dim}|mlp_depth={hidden_layers}|aggre={num_layers}|dataset={dataset}.txt\"\n",
    ")"
   ],
   "id": "f4bc5c6ca0faffad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = \"FB15K-237\"\n",
    "train_p = Path(\"../FB15K-237/train.txt\")\n",
    "valid_p = Path(\"../FB15K-237/valid.txt\")\n",
    "test_p  = Path(\"../FB15K-237/test.txt\")\n",
    "\n",
    "from dataset_loader import KGDataModuleTyped\n",
    "\n",
    "\n",
    "dm_typed = KGDataModuleTyped(\n",
    "    train_p, valid_p, test_p,\n",
    "    add_reverse=True,\n",
    "    reverse_relation_strategy=\"duplicate_rel\",\n",
    ")\n",
    "\n",
    "num_nodes = len(dm_typed.ent2id)\n",
    "num_relations = len(dm_typed.rel2id)\n",
    "emb_dim = 128\n",
    "num_layers = 3\n",
    "hidden_layers = 3\n",
    "\n",
    "# --- 1) R-GIN + DistMult ---\n",
    "enc_dm = RelationalGINEncoder( num_nodes=num_nodes, num_relations=num_relations, emb_dim=emb_dim, num_layers=num_layers, hidden_layers=hidden_layers, dropout=0.1, train_eps=True\n",
    "                               )\n",
    "dec_dm = DistMultDecoder(num_relations=num_relations, dim=emb_dim)\n",
    "\n",
    "res_distmult = train_linkpred_film_gin(\n",
    "    enc_dm, dec_dm, dm_typed,\n",
    "    epochs=100, lr=1e-3, weight_decay=1e-4, patience=10,\n",
    "    show_tqdm=True,\n",
    "    save_best_path=f\"checkpoints/r-gin-film/{dataset}/best_rgin_distmult|embed_dim={emb_dim}|mlp_depth={hidden_layers}|aggre={num_layers}|dataset={dataset}.pt\",\n",
    "    save_on_improve=True,\n",
    "    hparams={\"dataset\": dataset, \"decoder\": \"DistMult\", \"emb_dim\": emb_dim, \"num_layers\": num_layers, \"hidden_layers\": hidden_layers}\n",
    ")\n",
    "\n",
    "\n",
    "print_training_report(\n",
    "    model_name = \"R-GIN_embed_rel + Distmult\",\n",
    "    result = res_distmult,\n",
    "    header_title = \"Model Training Results\",\n",
    "    save_path=f\"results/r-gin-film/{dataset}/film_rgin_report|embed_dim={emb_dim}|mlp_depth={hidden_layers}|aggre={num_layers}|dataset={dataset}.txt\"\n",
    ")"
   ],
   "id": "ad52b0f7e7b872f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# cora_to_kg.py\n",
    "# Convert Cora (PyG) into WN18RR-style TSV triples your loader can read.\n",
    "\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.utils import coalesce\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# ------------------------- config -------------------------\n",
    "root = Path(\"./data\")\n",
    "out_dir = root / \"CORA_KG\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "relation_name = \"cites\"            # single relation\n",
    "is_undirected_split = True         # safer split for citation graphs\n",
    "add_reverse_edges = True           # also write reverse edges\n",
    "reverse_relation_strategy = \"duplicate_rel\"  # or \"same_rel\"\n",
    "\n",
    "# --------------------- load + coalesce --------------------\n",
    "print(\"📥 Loading Cora via PyG (auto-download if needed)...\")\n",
    "dataset = Planetoid(root=str(root), name=\"Cora\")\n",
    "data = dataset[0]\n",
    "\n",
    "# coalesce deduplicates edges; keep them as-is (directed list)\n",
    "edge_index, _ = coalesce(data.edge_index, None, data.num_nodes, data.num_nodes)\n",
    "\n",
    "# Build a Data object with the coalesced edges (older PyG has no .replace)\n",
    "new_data = Data(\n",
    "    x=data.x,\n",
    "    y=data.y,\n",
    "    edge_index=edge_index,\n",
    "    num_nodes=data.num_nodes,\n",
    ")\n",
    "\n",
    "print(f\"✅ Cora: num_nodes={data.num_nodes}, edges={edge_index.size(1)}\")\n",
    "\n",
    "# ------------------- train/val/test split -----------------\n",
    "splitter = RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    is_undirected=is_undirected_split,\n",
    "    add_negative_train_samples=False,  # you sample negatives yourself\n",
    ")\n",
    "train_g, val_g, test_g = splitter(new_data)\n",
    "\n",
    "def pos_edges(g: Data) -> torch.Tensor:\n",
    "    # RandomLinkSplit attaches edge_label and edge_label_index\n",
    "    mask = (g.edge_label == 1)\n",
    "    return g.edge_label_index[:, mask]  # [2, E_pos]\n",
    "\n",
    "train_edges = pos_edges(train_g)\n",
    "val_edges   = pos_edges(val_g)\n",
    "test_edges  = pos_edges(test_g)\n",
    "\n",
    "print(f\"📊 Splits: train={train_edges.size(1)}, val={val_edges.size(1)}, test={test_edges.size(1)}\")\n",
    "\n",
    "# -------------------- triples + saving --------------------\n",
    "def make_triples(edge_idx: torch.Tensor,\n",
    "                 rel: str,\n",
    "                 add_rev: bool,\n",
    "                 rev_strategy: str) -> list[tuple[str, str, str]]:\n",
    "    triples = []\n",
    "    h_list = edge_idx[0].tolist()\n",
    "    t_list = edge_idx[1].tolist()\n",
    "    for h, t in zip(h_list, t_list):\n",
    "        triples.append((f\"n{h}\", rel, f\"n{t}\"))\n",
    "        if add_rev:\n",
    "            if rev_strategy == \"duplicate_rel\":\n",
    "                triples.append((f\"n{t}\", rel + \"_rev\", f\"n{h}\"))\n",
    "            else:  # same_rel\n",
    "                triples.append((f\"n{t}\", rel, f\"n{h}\"))\n",
    "    return triples\n",
    "\n",
    "def save_triples(triples: list[tuple[str, str, str]], path: Path) -> None:\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f, delimiter=\"\\t\")\n",
    "        writer.writerows(triples)\n",
    "    print(f\"💾 Saved {len(triples):,} triples -> {path}\")\n",
    "\n",
    "train_triples = make_triples(train_edges, relation_name, add_reverse_edges, reverse_relation_strategy)\n",
    "val_triples   = make_triples(val_edges,   relation_name, add_reverse_edges, reverse_relation_strategy)\n",
    "test_triples  = make_triples(test_edges,  relation_name, add_reverse_edges, reverse_relation_strategy)\n",
    "\n",
    "save_triples(train_triples, out_dir / \"train.txt\")\n",
    "save_triples(val_triples,   out_dir / \"valid.txt\")\n",
    "save_triples(test_triples,  out_dir / \"test.txt\")\n",
    "\n",
    "print(\"✅ Done. Files are WN18RR-style and compatible with your KGDataModuleTyped.\")\n",
    "print(f\"Use paths:\\n  train: {out_dir/'train.txt'}\\n  valid: {out_dir/'valid.txt'}\\n  test : {out_dir/'test.txt'}\")"
   ],
   "id": "f6ca131e9f7ab59f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from dataset_loader import KGDataModuleTyped\n",
    "dataset = \"Cora\"\n",
    "train_p =  Path(\"data/CORA_KG/train.txt\")\n",
    "valid_p = Path(\"data/CORA_KG/valid.txt\")\n",
    "test_p = Path(\"data/CORA_KG/test.txt\")\n",
    "\n",
    "dm_typed = KGDataModuleTyped(\n",
    "    train_p, valid_p, test_p,\n",
    "    add_reverse=False,\n",
    "    reverse_relation_strategy=\"duplicate_rel\",\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "num_nodes = len(dm_typed.ent2id)\n",
    "num_relations = len(dm_typed.rel2id)\n",
    "emb_dim = 128\n",
    "num_layers = 2\n",
    "hidden_layers = 3\n",
    "\n",
    "# --- 1) R-GIN + DistMult ---\n",
    "enc_dm = RelationalGINEncoder( num_nodes=num_nodes, num_relations=num_relations, emb_dim=emb_dim, num_layers=num_layers, hidden_layers=hidden_layers, dropout=0.1, train_eps=True\n",
    "                               )\n",
    "dec_dm = DistMultDecoder(num_relations=num_relations, dim=emb_dim)\n",
    "\n",
    "\n",
    "\n",
    "res_distmult = train_linkpred_film_gin(\n",
    "    enc_dm, dec_dm, dm_typed,\n",
    "    epochs=100, lr=1e-3, weight_decay=1e-4, patience=10,\n",
    "    show_tqdm=True,\n",
    "    save_best_path=f\"checkpoints/r-gin-film/{dataset}/best_rgin_distmult|embed_dim={emb_dim}|mlp_depth={hidden_layers}|aggre={num_layers}|dataset={dataset}.pt\",\n",
    "    save_on_improve=True,\n",
    "    hparams={\"dataset\": dataset, \"decoder\": \"DistMult\", \"emb_dim\": emb_dim, \"num_layers\": num_layers, \"hidden_layers\": hidden_layers}\n",
    ")\n",
    "\n",
    "print_training_report(\n",
    "    model_name = \"R-GIN-film + Distmult\",\n",
    "    result = res_distmult,\n",
    "    header_title = \"Model Training Results\",\n",
    "    save_path=f\"results/r-gin-film/{dataset}/film_rgin_report|embed_dim={emb_dim}|mlp_depth={hidden_layers}|aggre={num_layers}|dataset={dataset}.txt\"\n",
    ")"
   ],
   "id": "8d62cf546ebe10f2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

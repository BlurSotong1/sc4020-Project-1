
================================================================================
COMBINED ABLATION: Embed-Relation + DotProduct vs Baseline
================================================================================

BEST VALIDATION METRICS (used for model selection)
--------------------------------------------------------------------------------
Attention-per-Relation + DistMult (Baseline) | AUC=0.9194 | H@1=0.5923 | H@5=0.7320 | H@10=0.7887 (epoch 58)
Embed-Relation + DotProduct (Variant) | AUC=0.8161 | H@1=0.4308 | H@5=0.5554 | H@10=0.6163 (epoch 1)

FINAL TEST SET PERFORMANCE (unbiased estimate)
--------------------------------------------------------------------------------
Attention-per-Relation + DistMult (Baseline) | AUC=0.9128 | H@1=0.5964 | H@5=0.7237 | H@10=0.7843
Embed-Relation + DotProduct (Variant) | AUC=0.8052 | H@1=0.4199 | H@5=0.5453 | H@10=0.5973


Attention-per-Relation + DistMult (Baseline) - Training History
------------------------------------------------------------------------------------------
Epoch    Train Loss     Val AUC      H@1          H@5          H@10        
------------------------------------------------------------------------------------------
1        1.6293         0.7809       0.1098       0.2947       0.4196      
2        0.7379         0.8560       0.2432       0.4832       0.6088      
3        0.3893         0.8795       0.3533       0.5620       0.6671      
4        0.2593         0.8767       0.4186       0.6042       0.7063      
5        0.1904         0.8714       0.4446       0.6246       0.7142      
6        0.1517         0.8815       0.4621       0.6513       0.7320      
7        0.1332         0.8767       0.4792       0.6599       0.7456      
8        0.1135         0.8743       0.4918       0.6869       0.7630      
9        0.1097         0.8847       0.5138       0.7017       0.7736      
10       0.0912         0.8849       0.5198       0.7080       0.7881      
11       0.0962         0.8860       0.5257       0.7096       0.7811      
12       0.0843         0.8841       0.5475       0.7116       0.7848      
13       0.0845         0.8887       0.5442       0.7185       0.7861      
14       0.0880         0.8852       0.5570       0.7228       0.7904      
15       0.0765         0.8888       0.5396       0.7162       0.7841      
16       0.0795         0.8872       0.5471       0.7225       0.7897      
17       0.0753         0.8899       0.5686       0.7367       0.8026      
18       0.0697         0.8901       0.5630       0.7367       0.7980      
19       0.0668         0.8824       0.5488       0.7225       0.7818      
20       0.0664         0.8800       0.5554       0.7225       0.7881      
21       0.0593         0.8855       0.5808       0.7347       0.7960      
22       0.0558         0.8832       0.5672       0.7208       0.7868      
23       0.0540         0.8952       0.5798       0.7396       0.8006      
24       0.0525         0.8844       0.5630       0.7291       0.7933      
25       0.0511         0.8885       0.5778       0.7416       0.8111      
26       0.0499         0.8893       0.5821       0.7432       0.8039      
27       0.0527         0.8874       0.5656       0.7258       0.7914      
28       0.0468         0.8913       0.5860       0.7429       0.8036      
29       0.0421         0.8834       0.5850       0.7436       0.8016      
30       0.0425         0.8931       0.5910       0.7363       0.7996      
31       0.0428         0.8920       0.5979       0.7577       0.8098      
32       0.0392         0.8930       0.5933       0.7492       0.8118      
33       0.0353         0.9015       0.5982       0.7459       0.8098      
34       0.0345         0.8906       0.5920       0.7416       0.7976      
35       0.0360         0.8876       0.5985       0.7564       0.8059      
36       0.0313         0.9014       0.5933       0.7406       0.8039      
37       0.0343         0.8944       0.6032       0.7475       0.8032      
38       0.0335         0.8941       0.6018       0.7482       0.8019      
39       0.0295         0.8965       0.6121       0.7544       0.8118      
40       0.0277         0.8922       0.6028       0.7456       0.8006      
41       0.0281         0.8980       0.6032       0.7462       0.8049      
42       0.0279         0.9053       0.6203       0.7610       0.8102      
43       0.0285         0.9024       0.6061       0.7551       0.8125      
44       0.0263         0.9071       0.5972       0.7357       0.7868      
45       0.0275         0.9028       0.5992       0.7469       0.8085      
46       0.0285         0.9079       0.5995       0.7403       0.7933      
47       0.0278         0.9126       0.6002       0.7465       0.8039      
48       0.0301         0.9073       0.5893       0.7317       0.7854      
49       0.0277         0.9018       0.5943       0.7320       0.7940      
50       0.0273         0.9113       0.5913       0.7340       0.7953      
51       0.0264         0.9175       0.5913       0.7399       0.7943      
52       0.0282         0.9113       0.5817       0.7241       0.7802      
53       0.0256         0.9094       0.5831       0.7205       0.7779      
54       0.0269         0.9093       0.5929       0.7320       0.7825      
55       0.0270         0.9126       0.5854       0.7251       0.7811      
56       0.0281         0.9107       0.5883       0.7294       0.7788      
57       0.0279         0.9135       0.5798       0.7337       0.7868      
58       0.0268         0.9194       0.5923       0.7320       0.7887      
59       0.0265         0.9096       0.5890       0.7241       0.7746      
60       0.0271         0.9101       0.5850       0.7241       0.7769      
61       0.0262         0.9146       0.5870       0.7274       0.7798      
62       0.0261         0.9132       0.5811       0.7294       0.7825      
63       0.0266         0.9123       0.5890       0.7205       0.7755      
64       0.0290         0.9093       0.5784       0.7179       0.7752      
65       0.0273         0.9183       0.5867       0.7228       0.7769      
66       0.0257         0.9152       0.5857       0.7370       0.7914      
67       0.0257         0.9114       0.5821       0.7189       0.7788      
68       0.0256         0.9183       0.6045       0.7307       0.7874      


Embed-Relation + DotProduct (Variant) - Training History
------------------------------------------------------------------------------------------
Epoch    Train Loss     Val AUC      H@1          H@5          H@10        
------------------------------------------------------------------------------------------
1        76.3013        0.8161       0.4308       0.5554       0.6163      
2        13.4129        0.8152       0.4186       0.5557       0.6163      
3        9.6130         0.8088       0.3972       0.5461       0.6088      
4        7.6450         0.8047       0.3912       0.5425       0.6088      
5        6.3479         0.8071       0.3823       0.5402       0.6032      
6        5.3957         0.8064       0.3823       0.5300       0.5976      
7        4.6376         0.8098       0.3728       0.5366       0.5999      
8        4.0215         0.8097       0.3705       0.5267       0.5926      
9        3.4834         0.8054       0.3612       0.5214       0.5995      
10       3.0071         0.8009       0.3533       0.5168       0.5920      
11       2.5562         0.7898       0.3441       0.5069       0.5771      

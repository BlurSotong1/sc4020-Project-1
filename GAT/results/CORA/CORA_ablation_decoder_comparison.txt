
================================================================================
DECODER ABLATION: DistMult vs DotProduct
================================================================================

BEST VALIDATION METRICS (used for model selection)
--------------------------------------------------------------------------------
Attention-per-Relation + DistMult | AUC=0.8165 | H@1=0.3159 | H@5=0.5636 | H@10=0.6433 (epoch 14)
Attention-per-Relation + DotProduct | AUC=0.7498 | H@1=0.2438 | H@5=0.4545 | H@10=0.5152 (epoch 3)

FINAL TEST SET PERFORMANCE (unbiased estimate)
--------------------------------------------------------------------------------
Attention-per-Relation + DistMult | AUC=0.8039 | H@1=0.3131 | H@5=0.5579 | H@10=0.6328
Attention-per-Relation + DotProduct | AUC=0.7583 | H@1=0.2306 | H@5=0.4383 | H@10=0.5066


Attention-per-Relation + DistMult - Training History
------------------------------------------------------------------------------------------
Epoch    Train Loss     Val AUC      H@1          H@5          H@10        
------------------------------------------------------------------------------------------
1        1.9096         0.5356       0.0228       0.0987       0.1964      
2        1.3768         0.6690       0.1148       0.2666       0.3653      
3        1.0222         0.7117       0.1727       0.3719       0.4668      
4        0.6142         0.7723       0.2552       0.4507       0.5342      
5        0.3887         0.7860       0.2742       0.5104       0.5939      
6        0.2735         0.8087       0.3074       0.5180       0.6034      
7        0.1971         0.8124       0.2903       0.5361       0.6214      
8        0.1567         0.8031       0.3074       0.5436       0.6214      
9        0.1315         0.8045       0.3017       0.5446       0.6243      
10       0.1096         0.8075       0.2913       0.5588       0.6357      
11       0.0991         0.8135       0.3226       0.5645       0.6423      
12       0.0885         0.8139       0.3083       0.5522       0.6385      
13       0.0824         0.8116       0.3102       0.5598       0.6328      
14       0.0758         0.8165       0.3159       0.5636       0.6433      
15       0.0718         0.8108       0.3254       0.5655       0.6395      
16       0.0711         0.8041       0.3264       0.5607       0.6347      
17       0.0667         0.8115       0.3226       0.5598       0.6423      
18       0.0644         0.8103       0.3197       0.5645       0.6395      
19       0.0630         0.8079       0.3292       0.5702       0.6357      
20       0.0612         0.7982       0.3027       0.5550       0.6309      
21       0.0610         0.8018       0.3197       0.5541       0.6309      
22       0.0593         0.8036       0.3188       0.5531       0.6224      
23       0.0602         0.8015       0.3235       0.5436       0.6319      
24       0.0583         0.8027       0.3169       0.5503       0.6271      


Attention-per-Relation + DotProduct - Training History
------------------------------------------------------------------------------------------
Epoch    Train Loss     Val AUC      H@1          H@5          H@10        
------------------------------------------------------------------------------------------
1        144.7032       0.7231       0.2059       0.3956       0.4668      
2        32.1649        0.7441       0.2258       0.4326       0.5019      
3        17.1297        0.7498       0.2438       0.4545       0.5152      
4        13.5822        0.7424       0.2647       0.4317       0.4972      
5        11.6319        0.7424       0.2514       0.4279       0.4877      
6        10.4274        0.7427       0.2448       0.4118       0.4782      
7        9.6882         0.7349       0.2495       0.4127       0.4810      
8        9.2392         0.7324       0.2543       0.4194       0.4820      
9        8.7521         0.7289       0.2467       0.4146       0.4725      
10       8.3264         0.7272       0.2505       0.4089       0.4687      
11       8.0633         0.7286       0.2600       0.4194       0.4725      
12       7.8203         0.7183       0.2590       0.4146       0.4772      
13       7.5673         0.7223       0.2647       0.4336       0.4791      

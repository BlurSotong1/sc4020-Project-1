
================================================================================
DECODER ABLATION: DistMult vs DotProduct
================================================================================

BEST VALIDATION METRICS (used for model selection)
--------------------------------------------------------------------------------
Attention-per-Relation + DistMult | AUC=0.9875 | H@1=0.6728 | H@5=0.9202 | H@10=0.9725 (epoch 15)
Attention-per-Relation + DotProduct | AUC=0.8935 | H@1=0.3235 | H@5=0.6337 | H@10=0.7471 (epoch 18)

FINAL TEST SET PERFORMANCE (unbiased estimate)
--------------------------------------------------------------------------------
Attention-per-Relation + DistMult | AUC=0.9861 | H@1=0.6703 | H@5=0.9181 | H@10=0.9708
Attention-per-Relation + DotProduct | AUC=0.8923 | H@1=0.3150 | H@5=0.6341 | H@10=0.7460


Attention-per-Relation + DistMult - Training History
------------------------------------------------------------------------------------------
Epoch    Train Loss     Val AUC      H@1          H@5          H@10        
------------------------------------------------------------------------------------------
1        0.9579         0.9683       0.5332       0.8061       0.9011      
2        0.3798         0.9751       0.5869       0.8582       0.9411      
3        0.2996         0.9819       0.6169       0.8786       0.9545      
4        0.2680         0.9826       0.6362       0.8902       0.9600      
5        0.2497         0.9831       0.6344       0.8934       0.9629      
6        0.2368         0.9851       0.6450       0.9021       0.9685      
7        0.2294         0.9849       0.6416       0.9060       0.9686      
8        0.2220         0.9861       0.6513       0.9064       0.9698      
9        0.2151         0.9861       0.6574       0.9118       0.9717      
10       0.2103         0.9858       0.6585       0.9165       0.9722      
11       0.2070         0.9859       0.6623       0.9139       0.9718      
12       0.2039         0.9846       0.6546       0.9123       0.9710      
13       0.1996         0.9869       0.6638       0.9160       0.9723      
14       0.1967         0.9858       0.6635       0.9175       0.9732      
15       0.1959         0.9875       0.6728       0.9202       0.9725      
16       0.1900         0.9858       0.6717       0.9203       0.9725      
17       0.1883         0.9855       0.6732       0.9199       0.9729      
18       0.1894         0.9865       0.6643       0.9193       0.9722      
19       0.1881         0.9846       0.6643       0.9186       0.9731      
20       0.1837         0.9867       0.6725       0.9212       0.9733      


Attention-per-Relation + DotProduct - Training History
------------------------------------------------------------------------------------------
Epoch    Train Loss     Val AUC      H@1          H@5          H@10        
------------------------------------------------------------------------------------------
1        23.9357        0.7085       0.0557       0.1986       0.3190      
2        7.8526         0.7293       0.0503       0.1932       0.3270      
3        4.3014         0.7522       0.0525       0.2056       0.3616      
4        1.4978         0.8534       0.2086       0.4987       0.6371      
5        0.8807         0.8767       0.2675       0.5674       0.6941      
6        0.8562         0.8798       0.2793       0.5828       0.7044      
7        0.8446         0.8852       0.2908       0.5999       0.7219      
8        0.8383         0.8881       0.3005       0.6086       0.7280      
9        0.8344         0.8862       0.3072       0.6100       0.7291      
10       0.8305         0.8883       0.3088       0.6236       0.7388      
11       0.8292         0.8837       0.3147       0.6252       0.7401      
12       0.8272         0.8902       0.3183       0.6342       0.7485      
13       0.8257         0.8872       0.3072       0.6238       0.7378      
14       0.8249         0.8883       0.3109       0.6295       0.7487      
15       0.8241         0.8893       0.3158       0.6302       0.7425      
16       0.8228         0.8851       0.3088       0.6277       0.7417      
17       0.8227         0.8884       0.3110       0.6366       0.7506      
18       0.8224         0.8935       0.3235       0.6337       0.7471      
19       0.8213         0.8906       0.3096       0.6323       0.7470      
20       0.8208         0.8929       0.3088       0.6331       0.7506      

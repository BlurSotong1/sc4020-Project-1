{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-29T07:35:54.367135Z",
     "start_time": "2025-09-29T07:35:54.291886Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "_train_path = Path(\"../WN18RR/train.txt\")\n",
    "_test_path = Path(\"../WN18RR/test.txt\")\n",
    "_valid_path = Path(\"../WN18RR/valid.txt\")\n",
    "\n",
    "def load_dataset(path:Path) -> list[tuple]:\n",
    "    \"\"\"\n",
    "    parses dataset path into list of tuples.\n",
    "    \"\"\"\n",
    "    datalist = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            head, relation,tail = line.strip().split(\"\\t\")\n",
    "            datalist.append((head,relation,tail))\n",
    "\n",
    "    return datalist\n",
    "\n",
    "train_dataset = load_dataset(_train_path)\n",
    "valid_dataset = load_dataset(_valid_path)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "1. Collpase Relations to singular to see pure GAT Perf\n",
    "2. adding self loops as part of information flow\n",
    "3. send to Cuda"
   ],
   "id": "7ffb1dc52eecffb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Build ID Maps\n",
    "# We collapse relations in the encoder, but we still keep IDs around if we want to play around\n",
    "entities = set()\n",
    "relations = set()\n",
    "for h, r, t in (train_dataset + valid_dataset):\n",
    "    entities.add(h); entities.add(t); relations.add(r)\n",
    "\n",
    "ent2id = {e:i for i,e in enumerate(sorted(entities))}\n",
    "rel2id = {r:i for i,r in enumerate(sorted(relations))}\n",
    "\n",
    "num_entities  = len(ent2id)\n",
    "num_relations = len(rel2id)\n",
    "print(f\"#entities={num_entities}, #relations={num_relations}\")\n",
    "\n",
    "def triples_to_tensor(triples_list):\n",
    "    # returns LongTensor [N, 3] with (h_id, r_id, t_id)\n",
    "    arr = np.array([(ent2id[h], rel2id[r], ent2id[t]) for h,r,t in triples_list], dtype=np.int64)\n",
    "    return torch.from_numpy(arr)\n",
    "\n",
    "train_triples = triples_to_tensor(train_dataset)  # [N_train, 3]\n",
    "valid_triples = triples_to_tensor(valid_dataset)  # [N_valid, 3]\n",
    "\n",
    "# --------- Build collapsed edge_index from TRAIN triples ----------\n",
    "# Use only (h,t), add reverse edges for better message flow.\n",
    "edges = []\n",
    "for h, r, t in train_triples.tolist():\n",
    "    edges.append((h, t))\n",
    "    edges.append((t, h))\n",
    "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()  # [2, E]\n",
    "print(\"edge_index:\", edge_index.shape)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "# when building edge_index:\n",
    "edge_index, _ = add_self_loops(edge_index, num_nodes=num_entities)\n",
    "edge_index = edge_index.to(device)\n",
    "train_triples = train_triples.to(device)\n",
    "valid_triples = valid_triples.to(device)"
   ],
   "id": "5af6b1cc3e02703b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Definition and Decoder\n",
    "\n",
    "GNN Models need decoder because output is a vector\n",
    "\n",
    "take this vector -> guess the tail/head (decoder)\n",
    "\n",
    "since we collpase into uni relation, there is no need for specialised decoders, a dot product is sufficient for closest neighbour.\n",
    "\n",
    "using attention provided: ```GATConv(emb_dim,hidden_dim)```, which is The graph attentional operator from the `\"Graph Attention Networks\"\n",
    "    <https://arxiv.org/abs/1710.10903>`_ paper."
   ],
   "id": "dce6f44059ce8b5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class GATLinkEncoder(nn.Module):\n",
    "    def __init__(self, num_entities, emb_dim=128, hidden_dim=128, out_dim=256,\n",
    "                 heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.entity_emb = nn.Embedding(num_entities, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.entity_emb.weight)\n",
    "\n",
    "        self.gat1 = GATConv(emb_dim, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.gat2 = GATConv(hidden_dim * heads, out_dim, heads=1, concat=False, dropout=dropout)\n",
    "\n",
    "        self.res_proj = nn.Linear(emb_dim, out_dim)  # for residual from input emb to output dim\n",
    "        self.ln = nn.LayerNorm(out_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "        x0 = self.entity_emb.weight                    # [N, emb_dim]\n",
    "        x = F.elu(self.gat1(x0, edge_index))\n",
    "        x = self.drop(x)\n",
    "        x = self.gat2(x, edge_index)                  # [N, out_dim]\n",
    "        x = x + self.res_proj(x0)                     # residual\n",
    "        x = self.ln(x)\n",
    "        return x\n",
    "\n",
    "class DotProductDecoder(nn.Module):\n",
    "    def forward(self, e_h, e_t):\n",
    "        return (e_h * e_t).sum(dim=1)  # [B]\n",
    "\n",
    "class LinkPredictor(nn.Module):\n",
    "    def __init__(self, num_entities, edge_index, out_dim=200, **gat_kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = GATLinkEncoder(num_entities, out_dim=out_dim, **gat_kwargs)\n",
    "        self.decoder = DotProductDecoder()\n",
    "        self.edge_index = edge_index\n",
    "\n",
    "    def forward(self, triples):\n",
    "        # triples: [B, 3] but we ignore relation (column 1) for scoring\n",
    "        h = triples[:, 0]\n",
    "        t = triples[:, 2]\n",
    "        ent = self.encoder(self.edge_index)  # [N, d]\n",
    "        e_h, e_t = ent[h], ent[t]\n",
    "        return self.decoder(e_h, e_t)  # raw scores (logits)"
   ],
   "id": "1d89367cbf91abcb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Negative Sampling\n",
    "\n",
    "The model also needs to see examples of false triples to learn what not to predict. Without negatives, the model would just assign high scores to everything.\n",
    "\n",
    "The datasets dont provide a negative example, so making one is compulsory."
   ],
   "id": "b51663ff672f42a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@torch.no_grad()\n",
    "def sample_negatives(pos_triples, num_entities, corrupt_tail=True):\n",
    "    \"\"\"Return negatives by corrupting tail (or head). shape matches pos_triples.\"\"\"\n",
    "    neg = pos_triples.clone()\n",
    "    if corrupt_tail:\n",
    "        neg[:, 2] = torch.randint(0, num_entities, (pos_triples.size(0),), device=pos_triples.device)\n",
    "    else:\n",
    "        neg[:, 0] = torch.randint(0, num_entities, (pos_triples.size(0),), device=pos_triples.device)\n",
    "    return neg"
   ],
   "id": "68c3629989df1455"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training and Batch Eval\n",
    "\n",
    "Training code not much to explain here"
   ],
   "id": "a3581075c561af7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T13:23:37.945512Z",
     "start_time": "2025-09-29T13:23:37.934385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------- Training / Evaluation ----------\n",
    "def batches(tensor, batch_size, shuffle=True):\n",
    "    N = tensor.size(0)\n",
    "    idx = torch.randperm(N, device=tensor.device) if shuffle else torch.arange(N, device=tensor.device)\n",
    "    for i in range(0, N, batch_size):\n",
    "        part = tensor[idx[i:i+batch_size]]\n",
    "        yield part\n",
    "\n",
    "\n",
    "def sample_negatives_both(pos_triples, num_entities, k_neg=20):\n",
    "    # Returns two tensors: head-corrupted and tail-corrupted, each [B, k_neg, 3]\n",
    "    B = pos_triples.size(0)\n",
    "    device = pos_triples.device\n",
    "\n",
    "    # tail corruption\n",
    "    tails = torch.randint(0, num_entities, (B, k_neg), device=device)\n",
    "    neg_tail = pos_triples.unsqueeze(1).repeat(1, k_neg, 1)\n",
    "    neg_tail[:, :, 2] = tails\n",
    "\n",
    "    # head corruption\n",
    "    heads = torch.randint(0, num_entities, (B, k_neg), device=device)\n",
    "    neg_head = pos_triples.unsqueeze(1).repeat(1, k_neg, 1)\n",
    "    neg_head[:, :, 0] = heads\n",
    "\n",
    "    return neg_head.view(-1, 3), neg_tail.view(-1, 3)  # [B*k,3] each\n",
    "\n",
    "def train_one_epoch(model, triples, optimizer, batch_size=2048, k_neg=20):\n",
    "    model.train()\n",
    "    # pos:neg = 1 : (2*k_neg)\n",
    "    pos_weight = torch.tensor([(2.0 * k_neg)], device=triples.device)\n",
    "    bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    total_loss, seen = 0.0, 0\n",
    "    for pos in batches(triples, batch_size, shuffle=True):\n",
    "        neg_h, neg_t = sample_negatives_both(pos, num_entities, k_neg=k_neg)\n",
    "\n",
    "        all_triples = torch.cat([pos, neg_h, neg_t], dim=0)  # [B + 2Bk, 3]\n",
    "        labels = torch.cat([\n",
    "            torch.ones(len(pos), device=triples.device),\n",
    "            torch.zeros(len(neg_h) + len(neg_t), device=triples.device)\n",
    "        ])\n",
    "\n",
    "        scores = model(all_triples)  # [N_all]\n",
    "        loss = bce(scores, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.numel()\n",
    "        seen += labels.numel()\n",
    "\n",
    "    return total_loss / seen"
   ],
   "id": "17f7bae2d4c5eeaa",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluation Function\n",
    "for calculating hit@10 and AUC"
   ],
   "id": "8bef5c604ab0a95c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T13:24:10.261691Z",
     "start_time": "2025-09-29T13:24:10.254343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_auc_hits(model, triples, batch_size=4096, hits_k=10):\n",
    "    \"\"\"Quick sanity metrics (UNFILTERED):\n",
    "       - ROC-AUC on positive vs randomly corrupted negatives\n",
    "       - Hits@k on head/tail corruption with 100 negatives per positive\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    model.eval()\n",
    "    scores_all, labels_all = [], []\n",
    "\n",
    "    # AUC: one negative per positive\n",
    "    for pos in batches(triples, batch_size, shuffle=False):\n",
    "        neg = sample_negatives(pos, num_entities, corrupt_tail=True)\n",
    "        s_pos = model(pos)\n",
    "        s_neg = model(neg)\n",
    "        scores_all.append(torch.cat([s_pos, s_neg], 0).cpu().numpy())\n",
    "        labels_all.append(np.concatenate([np.ones(len(pos)), np.zeros(len(neg))], 0))\n",
    "    scores_all = np.concatenate(scores_all, 0)\n",
    "    labels_all = np.concatenate(labels_all, 0)\n",
    "    auc = roc_auc_score(labels_all, scores_all)\n",
    "\n",
    "    # Hits@k (unfiltered): rank true tail among 1 positive + 99 negatives\n",
    "    k = hits_k\n",
    "    hits = 0\n",
    "    n_trials = 0\n",
    "    for pos in batches(triples, batch_size, shuffle=False):\n",
    "        # build a set of 100 candidates per positive (1 true + 99 random tails)\n",
    "        B = pos.size(0)\n",
    "        true_t = pos[:, 2]\n",
    "        rand_t = torch.randint(0, num_entities, (B, 99), device=pos.device)\n",
    "        tails = torch.cat([true_t.unsqueeze(1), rand_t], dim=1)   # [B, 100]\n",
    "\n",
    "        # score (h, ?) against all 100 tails\n",
    "        ent = model.encoder(model.edge_index)                     # [N,d]\n",
    "        e_h = ent[pos[:, 0]]                                      # [B,d]\n",
    "        e_candidates = ent[tails]                                 # [B,100,d]\n",
    "        s = (e_h.unsqueeze(1) * e_candidates).sum(dim=2)          # [B,100]\n",
    "        ranks = (s.argsort(dim=1, descending=True) == 0).nonzero()[:,1] + 1  # position of true tail (1-based)\n",
    "        hits += (ranks <= k).sum().item()\n",
    "        n_trials += B\n",
    "    hits_at_k = hits / n_trials\n",
    "\n",
    "    return {\"auc\": float(auc), f\"hits@{k}\": float(hits_at_k)}"
   ],
   "id": "56503c8dc0d75342",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Actual Training",
   "id": "96adb549acee29fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T09:13:02.302663Z",
     "start_time": "2025-09-29T08:34:27.090292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LinkPredictor(num_entities, edge_index, out_dim=200,\n",
    "                      emb_dim=128, hidden_dim=128, heads=4, dropout=0.3).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "EPOCHS = 20\n",
    "for epoch in tqdm(range(1, EPOCHS+1), desc=\"epoch:\"):\n",
    "    loss = train_one_epoch(model, train_triples, opt, batch_size=2048)\n",
    "    if epoch % 2 == 0 or epoch == 1:\n",
    "        metrics = evaluate_auc_hits(model, valid_triples, batch_size=4096, hits_k=10)\n",
    "        print(f\"Epoch {epoch:02d} | loss={loss:.4f} | AUC={metrics['auc']:.4f} | Hits@10={metrics['hits@10']:.4f}\")"
   ],
   "id": "2ecc064ea18cbef1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#entities=40757, #relations=11\n",
      "edge_index: torch.Size([2, 173670])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch::   5%|▌         | 1/20 [02:14<42:27, 134.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss=54.2459 | AUC=0.8087 | Hits@10=0.5992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch::  10%|█         | 2/20 [04:20<38:56, 129.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | loss=12.0317 | AUC=0.8116 | Hits@10=0.6094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch::  20%|██        | 4/20 [08:29<33:45, 126.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | loss=8.3247 | AUC=0.8137 | Hits@10=0.6111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch::  30%|███       | 6/20 [12:32<28:53, 123.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | loss=6.5370 | AUC=0.8070 | Hits@10=0.6117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch::  40%|████      | 8/20 [16:29<24:10, 120.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | loss=5.1960 | AUC=0.8039 | Hits@10=0.6134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch::  50%|█████     | 10/20 [20:44<20:37, 123.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | loss=3.9877 | AUC=0.8053 | Hits@10=0.6187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch::  60%|██████    | 12/20 [25:12<17:01, 127.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | loss=2.7728 | AUC=0.8033 | Hits@10=0.6177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch::  70%|███████   | 14/20 [28:50<11:48, 118.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | loss=1.3322 | AUC=0.7936 | Hits@10=0.6104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch::  80%|████████  | 16/20 [32:11<07:14, 108.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | loss=0.8742 | AUC=0.7973 | Hits@10=0.6282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch::  90%|█████████ | 18/20 [35:20<03:23, 101.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | loss=0.8306 | AUC=0.8103 | Hits@10=0.6454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:: 100%|██████████| 20/20 [38:34<00:00, 115.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | loss=0.8081 | AUC=0.8153 | Hits@10=0.6543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Relatioinal GAT (Simplified)\n",
    "\n",
    "R-GAT Code.\n",
    "1. Uses the same embedding Matrix\n",
    "2. Each relation gets its own Attention Feed it Forward."
   ],
   "id": "2556948274e55d45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n"
   ],
   "id": "706b577ded98ccbf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "reloading of dataset for sanity",
   "id": "3c56c4fd6c50ca11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Data loading\n",
    "\n",
    "train_list = load_dataset(_train_path)\n",
    "valid_list = load_dataset(_valid_path)\n",
    "\n",
    "# Build ID maps\n",
    "entities, relations = set(), set()\n",
    "for h, r, t in (train_list + valid_list):\n",
    "    entities.add(h); entities.add(t); relations.add(r)\n",
    "\n",
    "ent2id = {e: i for i, e in enumerate(sorted(entities))}\n",
    "rel2id = {r: i for i, r in enumerate(sorted(relations))}\n",
    "num_entities, num_relations = len(ent2id), len(rel2id)\n",
    "print(f\"#entities={num_entities}, #relations={num_relations}\")\n",
    "\n",
    "def triples_to_tensor(triples):\n",
    "    arr = np.array([(ent2id[h], rel2id[r], ent2id[t]) for h, r, t in triples], dtype=np.int64)\n",
    "    return torch.from_numpy(arr)\n",
    "\n",
    "train_triples = triples_to_tensor(train_list)  # [N,3]\n",
    "valid_triples = triples_to_tensor(valid_list)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_triples = train_triples.to(device)\n",
    "valid_triples = valid_triples.to(device)\n",
    "\n",
    "# Build relation-aware edge_index dict (add reverse edges + self loops)\n",
    "rel_edge_index = defaultdict(list)\n",
    "for h, r, t in train_triples.tolist():\n",
    "    rel_edge_index[r].append((h, t))\n",
    "    rel_edge_index[r].append((t, h))  # reverse\n",
    "\n",
    "for r in range(num_relations):\n",
    "    if len(rel_edge_index[r]) == 0:\n",
    "        # ensure key exists even if relation absent in train\n",
    "        rel_edge_index[r] = torch.empty((2, 0), dtype=torch.long, device=device)\n",
    "    else:\n",
    "        eidx = torch.tensor(rel_edge_index[r], dtype=torch.long).t().contiguous()\n",
    "        eidx, _ = add_self_loops(eidx, num_nodes=num_entities)\n",
    "        rel_edge_index[r] = eidx.to(device)"
   ],
   "id": "647fe4f872ca4065"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-09-29T10:51:25.516788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RelationalGATEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    One GATConv per relation. Messages are summed over relations each layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_entities, num_relations,\n",
    "                 emb_dim=128, hidden_dim=128, out_dim=256,\n",
    "                 heads=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.num_relations = num_relations\n",
    "        self.entity_emb = nn.Embedding(num_entities, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.entity_emb.weight)\n",
    "\n",
    "        self.gat1 = nn.ModuleDict({\n",
    "            str(r): GATConv(emb_dim, hidden_dim, heads=heads, dropout=dropout)\n",
    "            for r in range(num_relations)\n",
    "        })\n",
    "        self.gat2 = nn.ModuleDict({\n",
    "            str(r): GATConv(hidden_dim * heads, out_dim, heads=1, concat=False, dropout=dropout)\n",
    "            for r in range(num_relations)\n",
    "        })\n",
    "\n",
    "        self.res_proj = nn.Linear(emb_dim, out_dim)\n",
    "        self.ln = nn.LayerNorm(out_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, rel_edge_index: dict[int, torch.Tensor]):\n",
    "        x0 = self.entity_emb.weight  # [N, emb_dim]\n",
    "\n",
    "        # layer 1: per-relation attention then sum\n",
    "        outs = []\n",
    "        for r in range(self.num_relations):\n",
    "            eidx = rel_edge_index[r]\n",
    "            if eidx.numel() == 0:\n",
    "                continue\n",
    "            outs.append(F.elu(self.gat1[str(r)](x0, eidx)))\n",
    "        x = torch.stack(outs).sum(0) if outs else torch.zeros_like(self.res_proj.weight[:x0.size(1)])\n",
    "        x = self.drop(x)\n",
    "\n",
    "        # layer 2: per-relation attention then sum\n",
    "        outs = []\n",
    "        for r in range(self.num_relations):\n",
    "            eidx = rel_edge_index[r]\n",
    "            if eidx.numel() == 0:\n",
    "                continue\n",
    "            outs.append(self.gat2[str(r)](x, eidx))\n",
    "        x = torch.stack(outs).sum(0) if outs else torch.zeros_like(self.res_proj(x0))\n",
    "\n",
    "        # residual + norm\n",
    "        x = self.ln(x + self.res_proj(x0))\n",
    "        return x  # [N, out_dim]\n",
    "\n",
    "class DistMultDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    score(h,r,t) = <e_h, w_r, e_t>\n",
    "    \"\"\"\n",
    "    def __init__(self, num_relations, dim):\n",
    "        super().__init__()\n",
    "        self.rel_emb = nn.Embedding(num_relations, dim)\n",
    "        nn.init.xavier_uniform_(self.rel_emb.weight)\n",
    "\n",
    "    def forward(self, e_h, r, e_t):\n",
    "        w_r = self.rel_emb(r)  # [B, d]\n",
    "        return (e_h * w_r * e_t).sum(dim=1)  # [B]\n",
    "\n",
    "class RelationalGATLinkPredictor(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, rel_edge_index,\n",
    "                 out_dim=256, **gat_kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = RelationalGATEncoder(num_entities, num_relations,\n",
    "                                            out_dim=out_dim, **gat_kwargs)\n",
    "        self.decoder = DistMultDecoder(num_relations, out_dim)\n",
    "        self.rel_edge_index = rel_edge_index\n",
    "\n",
    "    def forward(self, triples):  # triples: [B,3] (h,r,t)\n",
    "        h = triples[:, 0]; r = triples[:, 1]; t = triples[:, 2]\n",
    "        ent = self.encoder(self.rel_edge_index)  # [N, d]\n",
    "        return self.decoder(ent[h], r, ent[t])   # logits [B]\n",
    "\n",
    "# -------------------------\n",
    "# Utilities\n",
    "# -------------------------\n",
    "def batches(tensor, batch_size, shuffle=True):\n",
    "    N = tensor.size(0)\n",
    "    idx = torch.randperm(N, device=tensor.device) if shuffle else torch.arange(N, device=tensor.device)\n",
    "    for i in range(0, N, batch_size):\n",
    "        yield tensor[idx[i:i+batch_size]]\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_negatives_both(pos_triples, num_entities, k_neg=10):\n",
    "    \"\"\"\n",
    "    Returns flattened head- and tail-corrupted negatives:\n",
    "      neg_h: [B*k,3], neg_t: [B*k,3]\n",
    "    \"\"\"\n",
    "    B = pos_triples.size(0)\n",
    "    device = pos_triples.device\n",
    "\n",
    "    # Tail corruption\n",
    "    tails = torch.randint(0, num_entities, (B, k_neg), device=device)\n",
    "    neg_t = pos_triples.unsqueeze(1).repeat(1, k_neg, 1)\n",
    "    neg_t[:, :, 2] = tails\n",
    "\n",
    "    # Head corruption\n",
    "    heads = torch.randint(0, num_entities, (B, k_neg), device=device)\n",
    "    neg_h = pos_triples.unsqueeze(1).repeat(1, k_neg, 1)\n",
    "    neg_h[:, :, 0] = heads\n",
    "\n",
    "    return neg_h.view(-1, 3), neg_t.view(-1, 3)\n",
    "\n",
    "def train_one_epoch(model, triples, optimizer, batch_size=2048, k_neg=10):\n",
    "    model.train()\n",
    "    # Balance: 1 pos : (2*k_neg) neg\n",
    "    pos_weight = torch.tensor([2.0 * k_neg], device=triples.device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    total_loss, total_items = 0.0, 0\n",
    "    for pos in batches(triples, batch_size, shuffle=True):\n",
    "        neg_h, neg_t = sample_negatives_both(pos, num_entities, k_neg=k_neg)\n",
    "        all_trip = torch.cat([pos, neg_h, neg_t], dim=0)\n",
    "        labels   = torch.cat([\n",
    "            torch.ones(len(pos), device=triples.device),\n",
    "            torch.zeros(len(neg_h) + len(neg_t), device=triples.device)\n",
    "        ], dim=0)\n",
    "\n",
    "        scores = model(all_trip)\n",
    "        loss = criterion(scores, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.numel()\n",
    "        total_items += labels.numel()\n",
    "\n",
    "    return total_loss / total_items\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_auc_hits(model, triples, batch_size=4096, hits_k=10):\n",
    "    model.eval()\n",
    "    # --- AUC: 1 negative per positive (tail corruption) ---\n",
    "    scores_all, labels_all = [], []\n",
    "    for pos in batches(triples, batch_size, shuffle=False):\n",
    "        B = pos.size(0)\n",
    "        neg = pos.clone()\n",
    "        neg[:, 2] = torch.randint(0, num_entities, (B,), device=pos.device)\n",
    "\n",
    "        s_pos = model(pos)\n",
    "        s_neg = model(neg)\n",
    "\n",
    "        scores_all.append(torch.cat([s_pos, s_neg], dim=0).cpu().numpy())\n",
    "        labels_all.append(np.concatenate([np.ones(B), np.zeros(B)], axis=0))\n",
    "\n",
    "    auc = roc_auc_score(np.concatenate(labels_all), np.concatenate(scores_all))\n",
    "\n",
    "    # --- Hits@k (unfiltered): rank true tail among 99 random tails + 1 true ---\n",
    "    hits, trials = 0, 0\n",
    "    # Precompute entity encs once for speed\n",
    "    ent = model.encoder(model.rel_edge_index)  # [N, d]\n",
    "    for pos in batches(triples, batch_size, shuffle=False):\n",
    "        B = pos.size(0)\n",
    "        h = pos[:, 0]; r = pos[:, 1]; t_true = pos[:, 2]\n",
    "\n",
    "        # 99 random negatives\n",
    "        rand_t = torch.randint(0, num_entities, (B, 99), device=pos.device)\n",
    "        cand_t = torch.cat([t_true.unsqueeze(1), rand_t], dim=1)  # [B,100]\n",
    "\n",
    "        e_h = ent[h]                          # [B,d]\n",
    "        w_r = model.decoder.rel_emb(r)        # [B,d]\n",
    "        e_c = ent[cand_t]                     # [B,100,d]\n",
    "\n",
    "        # DistMult score(h,r,?) for all candidates\n",
    "        s = ((e_h * w_r).unsqueeze(1) * e_c).sum(dim=2)  # [B,100]\n",
    "        ranks = (s.argsort(dim=1, descending=True) == 0).nonzero()[:, 1] + 1\n",
    "        hits += (ranks <= hits_k).sum().item()\n",
    "        trials += B\n",
    "\n",
    "    return {\"auc\": float(auc), f\"hits@{hits_k}\": hits / trials}"
   ],
   "id": "460dca63fa11309f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#entities=40757, #relations=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   5%|▌         | 1/20 [07:05<2:14:52, 425.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss=1.5246 | AUC=0.7878 | Hits@10=0.4466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  10%|█         | 2/20 [12:49<1:53:18, 377.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | loss=0.7209 | AUC=0.8515 | Hits@10=0.5969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  20%|██        | 4/20 [24:58<1:39:28, 373.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | loss=0.2300 | AUC=0.8788 | Hits@10=0.7248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  30%|███       | 6/20 [37:15<1:26:36, 371.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | loss=0.1423 | AUC=0.8764 | Hits@10=0.7561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  40%|████      | 8/20 [48:23<1:10:14, 351.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | loss=0.1145 | AUC=0.8755 | Hits@10=0.7663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  50%|█████     | 10/20 [59:27<57:05, 342.53s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | loss=0.0976 | AUC=0.8777 | Hits@10=0.7779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  60%|██████    | 12/20 [1:10:18<44:33, 334.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | loss=0.0823 | AUC=0.8735 | Hits@10=0.7630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  70%|███████   | 14/20 [1:21:54<34:21, 343.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | loss=0.0787 | AUC=0.8862 | Hits@10=0.7894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  80%|████████  | 16/20 [1:32:46<22:16, 334.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | loss=0.0673 | AUC=0.8777 | Hits@10=0.7815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  90%|█████████ | 18/20 [1:43:39<11:01, 330.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | loss=0.0650 | AUC=0.8856 | Hits@10=0.7844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 20/20 [1:55:02<00:00, 345.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | loss=0.0575 | AUC=0.8779 | Hits@10=0.7963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 8,
   "source": [
    "num_entities = len(ent2id)\n",
    "model = RelationalGATLinkPredictor(\n",
    "    num_entities=num_entities,\n",
    "    num_relations=num_relations,\n",
    "    rel_edge_index=rel_edge_index,\n",
    "    emb_dim=128, hidden_dim=128, out_dim=256,\n",
    "    heads=4, dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in tqdm(range(1, EPOCHS + 1), desc=\"epoch\"):\n",
    "    loss = train_one_epoch(model, train_triples, optimizer, batch_size=2048, k_neg=10)\n",
    "    if epoch % 2 == 0 or epoch == 1:\n",
    "        metrics = evaluate_auc_hits(model, valid_triples, batch_size=4096, hits_k=10)\n",
    "        print(f\"Epoch {epoch:02d} | loss={loss:.4f} | AUC={metrics['auc']:.4f} | Hits@10={metrics['hits@10']:.4f}\")"
   ],
   "id": "e155acc0df5eeb7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T12:46:29.307840Z",
     "start_time": "2025-09-29T12:46:29.173728Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model, \"model/r-gat.pth\")",
   "id": "d39aedabbdf80b60",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4a35ce70e0f18aef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311 WN18RR",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

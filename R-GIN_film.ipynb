{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------\n",
    "# FiLM-style Relational GIN\n",
    "# ---------------------------\n",
    "class RelationalFiLMGINConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    Relation-aware GIN with FiLM modulation:\n",
    "      gamma_r, beta_r = MLP_rel(r_emb[r])\n",
    "      m_{j->i} = gamma_r âŠ™ (W x_j) + beta_r\n",
    "      x_i' = MLP_node( (1 + eps) * x_i + sum_j m_{j->i} )\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            emb_dim: int,\n",
    "            num_relations: int,\n",
    "            hidden_layers: int = 1,   # depth of relation MLP (>=1)\n",
    "            train_eps: bool = True\n",
    "    ):\n",
    "        super().__init__(aggr=\"add\")  # GIN uses sum aggregation\n",
    "\n",
    "        # relation embeddings (one per edge type)\n",
    "        self.rel_emb = nn.Embedding(num_relations, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.rel_emb.weight)\n",
    "\n",
    "        # relation MLP that outputs [gamma | beta] of size 2 * emb_dim\n",
    "        rel_layers = [nn.Linear(emb_dim, emb_dim * 2), nn.ReLU()]\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            rel_layers += [nn.Linear(emb_dim * 2, emb_dim * 2), nn.ReLU()]\n",
    "        rel_layers += [nn.Linear(emb_dim * 2, emb_dim * 2)]\n",
    "        self.rel_mlp = nn.Sequential(*rel_layers)\n",
    "\n",
    "        # shared linear transform on node features\n",
    "        self.W = nn.Linear(emb_dim, emb_dim)\n",
    "\n",
    "        # node-side GIN MLP (kept simple; you can deepen if you like)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_dim * 2, emb_dim),\n",
    "        )\n",
    "\n",
    "        # learnable epsilon like classic GIN\n",
    "        if train_eps:\n",
    "            self.eps = nn.Parameter(torch.zeros(1))\n",
    "        else:\n",
    "            self.register_buffer(\"eps\", torch.zeros(1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_type: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [N, d]\n",
    "        edge_index: [2, E]\n",
    "        edge_type: [E]  (relation id for each edge)\n",
    "        \"\"\"\n",
    "        # propagate will call message(...) then aggregate, then update(...)\n",
    "        return self.propagate(edge_index, x=x, edge_type=edge_type)\n",
    "\n",
    "    def message(self, x_j: torch.Tensor, edge_type: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x_j: [E, d] features of source nodes (neighbors)\n",
    "        edge_type: [E] relation ids aligned with edges\n",
    "        \"\"\"\n",
    "        r = self.rel_emb(edge_type)                 # [E, d]\n",
    "        gamma_beta = self.rel_mlp(r)                # [E, 2d]\n",
    "        gamma, beta = gamma_beta.chunk(2, dim=-1)   # each [E, d]\n",
    "        return gamma * self.W(x_j) + beta           # FiLM message\n",
    "\n",
    "    def update(self, aggr_out: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        aggr_out: [N, d] summed messages\n",
    "        x:         [N, d] original node features, passed from propagate via kwargs\n",
    "        \"\"\"\n",
    "        out = (1.0 + self.eps) * x + aggr_out\n",
    "        return self.mlp(out)\n",
    "\n",
    "\n",
    "class RelationalGINEncoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_nodes: int,\n",
    "            num_relations: int,\n",
    "            emb_dim: int = 128,\n",
    "            num_layers: int = 3,\n",
    "            hidden_layers: int = 1,   # depth for the relation FiLM MLP\n",
    "            dropout: float = 0.1,\n",
    "            train_eps: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(num_nodes, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.embed.weight)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            RelationalFiLMGINConv(\n",
    "                emb_dim=emb_dim,\n",
    "                num_relations=num_relations,\n",
    "                hidden_layers=hidden_layers,\n",
    "                train_eps=train_eps\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, edge_index: torch.Tensor, edge_type: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embed.weight\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, edge_type)\n",
    "            x = self.dropout(x)\n",
    "        return x  # [N, emb_dim]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

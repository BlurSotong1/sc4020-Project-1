{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "p_FimQyynYgw",
      "metadata": {
        "id": "p_FimQyynYgw"
      },
      "outputs": [],
      "source": [
        "# !pip install torch_geometric torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f9dd09d",
      "metadata": {
        "id": "9f9dd09d"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7eaca170",
      "metadata": {
        "id": "7eaca170"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Standard library\n",
        "# -----------------------------\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "import random\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "\n",
        "# -----------------------------\n",
        "# Third-party libraries\n",
        "# -----------------------------\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from google.colab import drive\n",
        "\n",
        "# -----------------------------\n",
        "# PyTorch and PyG libraries\n",
        "# -----------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch_geometric.utils import degree, add_self_loops\n",
        "from torch_geometric.nn import MessagePassing\n",
        "# from torch_scatter import scatter\n",
        "\n",
        "# import sys\n",
        "# from pathlib import Path\n",
        "# current_dir = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n",
        "# parent_dir = current_dir.parent\n",
        "# sys.path.append(str(parent_dir))\n",
        "# from utility.dataset_loader import KGDataModuleCollapsed, KGDataModuleTyped\n",
        "\n",
        "# device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ukSMz-srql5c",
      "metadata": {
        "id": "ukSMz-srql5c"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# Helpers\n",
        "# -----------------------\n",
        "def _read_triples(path: Path) -> List[Tuple[str, str, str]]:\n",
        "    triples = []\n",
        "    with open(path, \"r\", newline=\"\") as f:\n",
        "        reader = csv.reader(f, delimiter=\"\\t\")\n",
        "        for row in reader:\n",
        "            if not row:\n",
        "                continue\n",
        "            h, r, t = row\n",
        "            triples.append((h, r, t))\n",
        "    return triples\n",
        "\n",
        "\n",
        "def _build_id_maps(\n",
        "        train_p: Path,\n",
        "        valid_p: Optional[Path] = None,\n",
        "        test_p: Optional[Path] = None,\n",
        ") -> Tuple[Dict[str, int], Dict[str, int]]:\n",
        "    \"\"\"Build ent2id and rel2id from all splits available.\"\"\"\n",
        "    ents, rels = set(), set()\n",
        "    for p in [train_p, valid_p, test_p]:\n",
        "        if p is None:\n",
        "            continue\n",
        "        for h, r, t in _read_triples(p):\n",
        "            ents.add(h); ents.add(t); rels.add(r)\n",
        "    ent2id = {e: i for i, e in enumerate(sorted(ents))}\n",
        "    rel2id = {r: i for i, r in enumerate(sorted(rels))}\n",
        "    return ent2id, rel2id\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Datasets\n",
        "# -----------------------\n",
        "class _PairsDataset(Dataset):\n",
        "    \"\"\"Untyped pairs (h, t), label=1 for each positive edge.\"\"\"\n",
        "    def __init__(self, pairs: torch.LongTensor):\n",
        "        # pairs: [N,2]\n",
        "        self.pairs = pairs\n",
        "        self.labels = torch.ones(pairs.size(0), dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.pairs.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.pairs[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "class _TriplesDataset(Dataset):\n",
        "    \"\"\"Typed triples (h, r, t), label=1 for each positive triple.\"\"\"\n",
        "    def __init__(self, triples: torch.LongTensor):\n",
        "        # triples: [N,3]\n",
        "        self.triples = triples\n",
        "        self.labels = torch.ones(triples.size(0), dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.triples.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.triples[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Collapsed edge types (untyped) datamodule\n",
        "# ============================================================\n",
        "class KGDataModuleCollapsed:\n",
        "    \"\"\"\n",
        "    Produces untyped (h, t) pairs from KG triples.\n",
        "\n",
        "    Public methods:\n",
        "      - train_loader()\n",
        "      - val_loader()\n",
        "      - test_loader()\n",
        "\n",
        "    Args:\n",
        "        train_path, valid_path, test_path: Path to split files (CORA format).\n",
        "        batch_size: int\n",
        "        shuffle: bool (applied to train loader only)\n",
        "        num_workers: int (DataLoader)\n",
        "        add_reverse: if True, also add (t, h) for every (h, t)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            train_path: Path,\n",
        "            valid_path: Optional[Path] = None,\n",
        "            test_path: Optional[Path] = None,\n",
        "            batch_size: int = 2048,\n",
        "            shuffle: bool = True,\n",
        "            num_workers: int = 0,\n",
        "            add_reverse: bool = True,\n",
        "    ):\n",
        "        self.train_path = Path(train_path)\n",
        "        self.valid_path = Path(valid_path) if valid_path else None\n",
        "        self.test_path  = Path(test_path)  if test_path  else None\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.num_workers = num_workers\n",
        "        self.add_reverse = add_reverse\n",
        "\n",
        "        # Build ids\n",
        "        self.ent2id, self.rel2id = _build_id_maps(self.train_path, self.valid_path, self.test_path)\n",
        "\n",
        "        # Build tensors per split\n",
        "        self._train_pairs = self._make_pairs(self.train_path)\n",
        "        self._valid_pairs = self._make_pairs(self.valid_path) if self.valid_path else None\n",
        "        self._test_pairs  = self._make_pairs(self.test_path)  if self.test_path  else None\n",
        "\n",
        "        # Datasets\n",
        "        self._train_ds = _PairsDataset(self._train_pairs)\n",
        "        self._valid_ds = _PairsDataset(self._valid_pairs) if self._valid_pairs is not None else None\n",
        "        self._test_ds  = _PairsDataset(self._test_pairs)  if self._test_pairs  is not None else None\n",
        "\n",
        "    def _make_pairs(self, path: Path) -> torch.LongTensor:\n",
        "        pairs = []\n",
        "        for h, _, t in _read_triples(path):\n",
        "            h_id, t_id = self.ent2id[h], self.ent2id[t]\n",
        "            pairs.append((h_id, t_id))\n",
        "            if self.add_reverse:\n",
        "                pairs.append((t_id, h_id))\n",
        "        if not pairs:\n",
        "            return torch.empty(0, 2, dtype=torch.long)\n",
        "        return torch.tensor(pairs, dtype=torch.long)\n",
        "\n",
        "    # -------- public loaders --------\n",
        "    def train_loader(self) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            self._train_ds, batch_size=self.batch_size, shuffle=self.shuffle,\n",
        "            num_workers=self.num_workers, pin_memory=False\n",
        "        )\n",
        "\n",
        "    def val_loader(self) -> Optional[DataLoader]:\n",
        "        if self._valid_ds is None:\n",
        "            return None\n",
        "        return DataLoader(\n",
        "            self._valid_ds, batch_size=self.batch_size, shuffle=False,\n",
        "            num_workers=self.num_workers, pin_memory=False\n",
        "        )\n",
        "\n",
        "    def test_loader(self) -> Optional[DataLoader]:\n",
        "        if self._test_ds is None:\n",
        "            return None\n",
        "        return DataLoader(\n",
        "            self._test_ds, batch_size=self.batch_size, shuffle=False,\n",
        "            num_workers=self.num_workers, pin_memory=False\n",
        "        )\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Typed (with edge types) datamodule\n",
        "# ============================================================\n",
        "class KGDataModuleTyped:\n",
        "    \"\"\"\n",
        "    Produces typed (h, r, t) triples from KG files.\n",
        "\n",
        "    Public methods:\n",
        "      - train_loader()\n",
        "      - val_loader()\n",
        "      - test_loader()\n",
        "\n",
        "    Args:\n",
        "        train_path, valid_path, test_path: Path to split files.\n",
        "        batch_size, shuffle, num_workers: DataLoader args.\n",
        "        add_reverse: If True, add reverse links.\n",
        "        reverse_relation_strategy:\n",
        "            'duplicate_rel' -> create an inverse relation id per r (e.g., r#inv)\n",
        "            'same_rel'      -> reuse the same r id for reverse triple\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            train_path: Path,\n",
        "            valid_path: Optional[Path] = None,\n",
        "            test_path: Optional[Path] = None,\n",
        "            batch_size: int = 2048,\n",
        "            shuffle: bool = True,\n",
        "            num_workers: int = 0,\n",
        "            add_reverse: bool = True,\n",
        "            reverse_relation_strategy: str = \"duplicate_rel\",\n",
        "    ):\n",
        "        assert reverse_relation_strategy in (\"duplicate_rel\", \"same_rel\")\n",
        "        self.train_path = Path(train_path)\n",
        "        self.valid_path = Path(valid_path) if valid_path else None\n",
        "        self.test_path  = Path(test_path)  if test_path  else None\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.num_workers = num_workers\n",
        "        self.add_reverse = add_reverse\n",
        "        self.reverse_relation_strategy = reverse_relation_strategy\n",
        "\n",
        "        # Base id maps from original relations\n",
        "        self.ent2id, base_rel2id = _build_id_maps(self.train_path, self.valid_path, self.test_path)\n",
        "\n",
        "        # Possibly extend rel2id with inverse relations\n",
        "        if add_reverse and reverse_relation_strategy == \"duplicate_rel\":\n",
        "            # Make space for inverse ids\n",
        "            self.rel2id = dict(base_rel2id)\n",
        "            self.inv_of = {}  # map original rel -> inverse rel id\n",
        "            next_id = max(self.rel2id.values(), default=-1) + 1\n",
        "            # Ensure deterministic order\n",
        "            for r in sorted(base_rel2id.keys()):\n",
        "                inv_name = r + \"_rev\"\n",
        "                if inv_name not in self.rel2id:\n",
        "                    self.rel2id[inv_name] = next_id\n",
        "                    self.inv_of[r] = next_id\n",
        "                    next_id += 1\n",
        "        else:\n",
        "            self.rel2id = base_rel2id\n",
        "            self.inv_of = None  # not used\n",
        "\n",
        "        # Build tensors per split\n",
        "        self._train_triples = self._make_triples(self.train_path)\n",
        "        self._valid_triples = self._make_triples(self.valid_path) if self.valid_path else None\n",
        "        self._test_triples  = self._make_triples(self.test_path)  if self.test_path  else None\n",
        "\n",
        "        # Datasets\n",
        "        self._train_ds = _TriplesDataset(self._train_triples)\n",
        "        self._valid_ds = _TriplesDataset(self._valid_triples) if self._valid_triples is not None else None\n",
        "        self._test_ds  = _TriplesDataset(self._test_triples)  if self._test_triples  is not None else None\n",
        "\n",
        "    def _make_triples(self, path: Path) -> torch.LongTensor:\n",
        "        if path is None:\n",
        "            return torch.empty(0, 3, dtype=torch.long)\n",
        "\n",
        "        rows = []\n",
        "        for h, r, t in _read_triples(path):\n",
        "            h_id, t_id = self.ent2id[h], self.ent2id[t]\n",
        "            r_id = self.rel2id[r]  # original direction\n",
        "            rows.append((h_id, r_id, t_id))\n",
        "\n",
        "            if self.add_reverse:\n",
        "                if self.reverse_relation_strategy == \"duplicate_rel\":\n",
        "                    r_inv_id = self.inv_of[r]  # created in __init__\n",
        "                    rows.append((t_id, r_inv_id, h_id))\n",
        "                else:  # same_rel\n",
        "                    rows.append((t_id, r_id, h_id))\n",
        "\n",
        "        if not rows:\n",
        "            return torch.empty(0, 3, dtype=torch.long)\n",
        "        return torch.tensor(rows, dtype=torch.long)\n",
        "\n",
        "    # -------- public loaders --------\n",
        "    def train_loader(self) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            self._train_ds, batch_size=self.batch_size, shuffle=self.shuffle,\n",
        "            num_workers=self.num_workers, pin_memory=False\n",
        "        )\n",
        "\n",
        "    def val_loader(self) -> Optional[DataLoader]:\n",
        "        if self._valid_ds is None:\n",
        "            return None\n",
        "        return DataLoader(\n",
        "            self._valid_ds, batch_size=self.batch_size, shuffle=False,\n",
        "            num_workers=self.num_workers, pin_memory=False\n",
        "        )\n",
        "\n",
        "    def test_loader(self) -> Optional[DataLoader]:\n",
        "        if self._test_ds is None:\n",
        "            return None\n",
        "        return DataLoader(\n",
        "            self._test_ds, batch_size=self.batch_size, shuffle=False,\n",
        "            num_workers=self.num_workers, pin_memory=False\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wXCUAug8oNqX",
      "metadata": {
        "id": "wXCUAug8oNqX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "1229a7677039018",
      "metadata": {
        "id": "1229a7677039018"
      },
      "source": [
        "# Dataset Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "a9927759",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9927759",
        "outputId": "aa94cf8a-c0bc-456d-c064-abe140a7c4d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Dataset: CORA\n",
            "Entities: 2,708\n",
            "Original Relations: 2\n",
            "Relations (with inverse): 2\n",
            "Training pairs: 8,448\n",
            "Training triples (typed): 8,448\n",
            "Graph edges (with self-loops): 11,156\n",
            "Graph edge_index shape: (2, 11156)\n",
            "Data loaders created:\n",
            "Train batches (collapsed): 3\n",
            "Val batches (collapsed): 1\n",
            "Test batches (collapsed): 1\n"
          ]
        }
      ],
      "source": [
        "# Dataset loading using dataset_loader.py\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your dataset\n",
        "base_path = Path(\"/content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/CORA\")\n",
        "\n",
        "# Dataset paths\n",
        "train_path = base_path / \"train.txt\"\n",
        "valid_path = base_path / \"valid.txt\"\n",
        "test_path = base_path / \"test.txt\"\n",
        "\n",
        "# Initialize data modules\n",
        "# Collapsed mode for LightGCN (untyped pairs)\n",
        "dm_collapsed = KGDataModuleCollapsed(\n",
        "    train_path=train_path,\n",
        "    valid_path=valid_path,\n",
        "    test_path=test_path,\n",
        "    batch_size=4096,\n",
        "    shuffle=True,\n",
        "    add_reverse=False\n",
        ")\n",
        "\n",
        "# Typed mode for R-LightGCN (typed triples)\n",
        "dm_typed = KGDataModuleTyped(\n",
        "    train_path=train_path,\n",
        "    valid_path=valid_path,\n",
        "    test_path=test_path,\n",
        "    batch_size=4096,\n",
        "    shuffle=True,\n",
        "    add_reverse=False,\n",
        "    reverse_relation_strategy=\"duplicate_rel\"\n",
        ")\n",
        "\n",
        "num_entities = len(dm_collapsed.ent2id)\n",
        "num_relations = len(dm_collapsed.rel2id)  # Original relations\n",
        "num_relations_with_inv = len(dm_typed.rel2id)  # With inverse relations\n",
        "\n",
        "print(f\"Dataset: CORA\")\n",
        "print(f\"Entities: {num_entities:,}\")\n",
        "print(f\"Original Relations: {num_relations:,}\")\n",
        "print(f\"Relations (with inverse): {num_relations_with_inv:,}\")\n",
        "print(f\"Training pairs: {len(dm_collapsed._train_pairs):,}\")\n",
        "print(f\"Training triples (typed): {len(dm_typed._train_triples):,}\")\n",
        "\n",
        "# Build edge_index for LightGCN (collapsed pairs)\n",
        "edge_index = dm_collapsed._train_pairs.t().contiguous().to(device)\n",
        "edge_index, _ = add_self_loops(edge_index, num_nodes=num_entities)\n",
        "\n",
        "print(f\"Graph edges (with self-loops): {edge_index.shape[1]:,}\")\n",
        "print(f\"Graph edge_index shape: {tuple(edge_index.shape)}\")\n",
        "\n",
        "train_loader_collapsed = dm_collapsed.train_loader()\n",
        "val_loader_collapsed = dm_collapsed.val_loader()\n",
        "test_loader_collapsed = dm_collapsed.test_loader()\n",
        "\n",
        "train_loader_typed = dm_typed.train_loader()\n",
        "val_loader_typed = dm_typed.val_loader()\n",
        "test_loader_typed = dm_typed.test_loader()\n",
        "\n",
        "print(f\"Data loaders created:\")\n",
        "print(f\"Train batches (collapsed): {len(train_loader_collapsed)}\")\n",
        "print(f\"Val batches (collapsed): {len(val_loader_collapsed) if val_loader_collapsed else 0}\")\n",
        "print(f\"Test batches (collapsed): {len(test_loader_collapsed) if test_loader_collapsed else 0}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0beecc39",
      "metadata": {
        "id": "0beecc39"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "a54bd072",
      "metadata": {
        "id": "a54bd072"
      },
      "outputs": [],
      "source": [
        "# Model Saving Utility Functions\n",
        "def save_model_checkpoint(model, optimizer, hyperparameters, final_test_metrics,\n",
        "                         training_history, model_name, filename):\n",
        "    \"\"\"\n",
        "    Save model checkpoint with comprehensive information.\n",
        "\n",
        "    Args:\n",
        "        model: The trained model\n",
        "        optimizer: The optimizer used\n",
        "        hyperparameters: Dict with training hyperparameters\n",
        "        final_test_metrics: Final test evaluation results\n",
        "        training_history: Training metrics history\n",
        "        model_name: Name of the model for display\n",
        "        filename: Output filename for the checkpoint\n",
        "    \"\"\"\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'hyperparameters': hyperparameters,\n",
        "        'final_test_metrics': final_test_metrics,\n",
        "        'training_history': training_history,\n",
        "        'model_name': model_name\n",
        "    }\n",
        "\n",
        "    torch.save(checkpoint, filename)\n",
        "    print(f\"{model_name} model checkpoint saved to {filename}\")\n",
        "\n",
        "def load_model_checkpoint(filename, model_class, device, **model_kwargs):\n",
        "    \"\"\"\n",
        "    Load model checkpoint and restore model state.\n",
        "\n",
        "    Args:\n",
        "        filename: Path to checkpoint file\n",
        "        model_class: Model class to instantiate\n",
        "        device: Device to load model on\n",
        "        **model_kwargs: Arguments for model instantiation\n",
        "\n",
        "    Returns:\n",
        "        model: Loaded model\n",
        "        checkpoint: Full checkpoint data\n",
        "    \"\"\"\n",
        "    checkpoint = torch.load(filename, map_location=device)\n",
        "\n",
        "    # Create model instance\n",
        "    model = model_class(**model_kwargs).to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    print(f\"Model loaded from {filename}\")\n",
        "    print(f\"Model: {checkpoint.get('model_name', 'Unknown')}\")\n",
        "    print(f\"Hyperparameters: {checkpoint.get('hyperparameters', {})}\")\n",
        "\n",
        "    if 'final_test_metrics' in checkpoint:\n",
        "        metrics = checkpoint['final_test_metrics']\n",
        "        if 'head' in metrics and 'tail' in metrics:\n",
        "            auc_avg = (metrics['head']['auc'] + metrics['tail']['auc']) / 2\n",
        "            hits10_avg = (metrics['head']['hits@10'] + metrics['tail']['hits@10']) / 2\n",
        "            print(f\"Test AUC (avg): {auc_avg:.4f}\")\n",
        "            print(f\"Test Hits@10 (avg): {hits10_avg:.4f}\")\n",
        "\n",
        "    return model, checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "8ca1b6bb",
      "metadata": {
        "id": "8ca1b6bb"
      },
      "outputs": [],
      "source": [
        "class MetricsTracker:\n",
        "    \"\"\"Simplified metrics tracker without plotting\"\"\"\n",
        "    def __init__(self):\n",
        "        self.metrics = defaultdict(list)\n",
        "\n",
        "    def add(self, epoch, **kwargs):\n",
        "        self.metrics['epoch'].append(epoch)\n",
        "        for key, value in kwargs.items():\n",
        "            self.metrics[key].append(value)\n",
        "\n",
        "    def get_best_epoch(self, metric='val_auc_head'):\n",
        "        \"\"\"Get the epoch with the best performance for a given metric\"\"\"\n",
        "        if metric not in self.metrics or not self.metrics[metric]:\n",
        "            return 0\n",
        "\n",
        "        # For AUC and Hits@K, higher is better\n",
        "        best_idx = np.argmax(self.metrics[metric])\n",
        "        return self.metrics['epoch'][best_idx]\n",
        "\n",
        "    def save_to_file(self, filepath):\n",
        "        with open(filepath, 'w') as f:\n",
        "            f.write(\"Epoch\\tLoss\\tAUC\\tHits@1\\tHits@5\\tHits@10\\n\")\n",
        "            for i in range(len(self.metrics['epoch'])):\n",
        "                epoch = self.metrics['epoch'][i]\n",
        "                loss = self.metrics['loss'][i]\n",
        "                auc = self.metrics['val_auc'][i]\n",
        "                h1 = self.metrics['val_hits1'][i]\n",
        "                h5 = self.metrics['val_hits5'][i]\n",
        "                h10 = self.metrics['val_hits10'][i]\n",
        "\n",
        "                f.write(f\"{epoch}\\t{loss:.6f}\\t{auc:.6f}\\t{h1:.6f}\\t{h5:.6f}\\t{h10:.6f}\\n\")\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, metric='val_auc_head', mode='max', min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.metric = metric\n",
        "        self.mode = mode  # 'max' for AUC, Hits@K; 'min' for loss\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.should_stop = False\n",
        "\n",
        "    def __call__(self, metrics_tracker):\n",
        "        current_score = metrics_tracker.metrics[self.metric][-1] if self.metric in metrics_tracker.metrics else 0\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = current_score\n",
        "        else:\n",
        "            if self.mode == 'max':\n",
        "                improved = current_score > self.best_score + self.min_delta\n",
        "            else:  # mode == 'min'\n",
        "                improved = current_score < self.best_score - self.min_delta\n",
        "\n",
        "            if improved:\n",
        "                self.best_score = current_score\n",
        "                self.counter = 0\n",
        "            else:\n",
        "                self.counter += 1\n",
        "\n",
        "        self.should_stop = self.counter >= self.patience\n",
        "        return self.should_stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "315071c9",
      "metadata": {
        "id": "315071c9"
      },
      "outputs": [],
      "source": [
        "# Training functions with data loaders\n",
        "def train_one_epoch_lightgcn(model, data_loader, optimizer, edge_index, device, num_entities, max_grad_norm=5.0):\n",
        "    \"\"\"\n",
        "    Stable LightGCN training loop (BPR loss).\n",
        "    Handles NaNs, exploding gradients, and large score differences gracefully.\n",
        "\n",
        "    Args:\n",
        "        model: LightGCN model\n",
        "        data_loader: DataLoader yielding positive pairs (batch_size, 2)\n",
        "        optimizer: Optimizer instance\n",
        "        edge_index: Graph edges tensor on the correct device\n",
        "        device: torch.device to run on\n",
        "        num_entities: Total number of entities (int)\n",
        "        max_grad_norm: Optional gradient clipping value\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # ------------------------------\n",
        "        # Handle batch format\n",
        "        # ------------------------------\n",
        "        if isinstance(batch, (list, tuple)):\n",
        "            pairs_tensor = batch[0]\n",
        "        else:\n",
        "            pairs_tensor = batch\n",
        "\n",
        "        pairs_tensor = pairs_tensor.to(device)\n",
        "        if pairs_tensor.dim() == 1:\n",
        "            pairs_tensor = pairs_tensor.unsqueeze(0)\n",
        "\n",
        "        pos_edges = pairs_tensor  # [batch_size, 2]\n",
        "        num_pos = pos_edges.size(0)\n",
        "\n",
        "        # ------------------------------\n",
        "        # Create random negatives\n",
        "        # ------------------------------\n",
        "        neg_tail = torch.randint(0, num_entities, (num_pos,), device=device)\n",
        "        neg_head = torch.randint(0, num_entities, (num_pos,), device=device)\n",
        "\n",
        "        # ------------------------------\n",
        "        # Encode embeddings (per-batch to avoid retain_graph issues)\n",
        "        # ------------------------------\n",
        "        embeddings = model.encode(edge_index)\n",
        "\n",
        "        # Normalize to prevent exploding scores\n",
        "        emb = embeddings / (embeddings.norm(dim=1, keepdim=True) + 1e-9)\n",
        "\n",
        "        # ------------------------------\n",
        "        # Compute scores\n",
        "        # ------------------------------\n",
        "        pos_scores = (emb[pos_edges[:, 0]] * emb[pos_edges[:, 1]]).sum(dim=1)\n",
        "        neg_scores_tail = (emb[pos_edges[:, 0]] * emb[neg_tail]).sum(dim=1)\n",
        "        neg_scores_head = (emb[neg_head] * emb[pos_edges[:, 1]]).sum(dim=1)\n",
        "\n",
        "        # ------------------------------\n",
        "        # Stable BPR loss\n",
        "        # ------------------------------\n",
        "        loss_tail = F.softplus(-(pos_scores - neg_scores_tail)).mean()\n",
        "        loss_head = F.softplus(-(pos_scores - neg_scores_head)).mean()\n",
        "        loss = loss_tail + loss_head\n",
        "\n",
        "        # ------------------------------\n",
        "        # NaN / inf guard\n",
        "        # ------------------------------\n",
        "        if not torch.isfinite(loss):\n",
        "            print(f\"[Batch {batch_idx}] NaN or inf detected — skipping this batch.\")\n",
        "            continue\n",
        "\n",
        "        # ------------------------------\n",
        "        # Backprop and update\n",
        "        # ------------------------------\n",
        "        loss.backward()\n",
        "\n",
        "        # Optional gradient clipping (prevents explosions)\n",
        "        if max_grad_norm is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track loss\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    avg_loss = total_loss / max(1, num_batches)\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8399de0a",
      "metadata": {
        "id": "8399de0a"
      },
      "source": [
        "### Negative Sampling\n",
        "- A training technique used in knowledge graph link prediction to create \"negative examples\", triples that are likely to be false.\n",
        "- Since knowledge graphs only contain positive facts (true triples), we need to artificially create negative examples for the model to learn what relationships are incorrect.\n",
        "- Both head and tail corruptions are used to train the model to understand connections flowing in both directions. Model learns \"What subjects fit this relation-object\" and \"What object fits this subject-relation\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ef898ab7",
      "metadata": {
        "id": "ef898ab7"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def pairs_from_triples(triples: torch.LongTensor) -> torch.LongTensor:\n",
        "    \"\"\"\n",
        "    Convert (h, r, t) -> pairs [2, N] = (h, t) for decoding on collapsed graph.\n",
        "    \"\"\"\n",
        "    return triples[:, [0, 2]].t().contiguous()  # [2, N]\n",
        "\n",
        "@torch.no_grad()\n",
        "def negative_sample_heads(triples: torch.LongTensor, num_nodes: int) -> torch.LongTensor:\n",
        "    \"\"\"\n",
        "    Corrupt heads: (h, r, t) -> (h', t)\n",
        "    Returns pairs [2, N].\n",
        "    \"\"\"\n",
        "    N = triples.size(0)\n",
        "    neg_h = torch.randint(0, num_nodes, (N,), device=triples.device)\n",
        "    t = triples[:, 2]\n",
        "    return torch.stack([neg_h, t], dim=0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def negative_sample_tails(triples: torch.LongTensor, num_nodes: int) -> torch.LongTensor:\n",
        "    \"\"\"\n",
        "    Corrupt tails: (h, r, t) -> (h, t')\n",
        "    Returns pairs [2, N].\n",
        "    \"\"\"\n",
        "    N = triples.size(0)\n",
        "    h = triples[:, 0]\n",
        "    neg_t = torch.randint(0, num_nodes, (N,), device=triples.device)\n",
        "    return torch.stack([h, neg_t], dim=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff4c4130",
      "metadata": {
        "id": "ff4c4130"
      },
      "source": [
        "# Model 1 LightGCN\n",
        "- We also add new inverse type of relation on top of the 11 that already exists.\n",
        "- This allows for information to be passed around which originally did not.\n",
        "- A -> B is one way, and there should be an inverse relationship (or some information) which is missed out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "8f5105ec",
      "metadata": {
        "id": "8f5105ec"
      },
      "outputs": [],
      "source": [
        "# -------- LightGCN layer --------\n",
        "class LightGCNConv(MessagePassing):\n",
        "    def __init__(self):\n",
        "        super().__init__(aggr='add')\n",
        "\n",
        "    # Compute symmetric normalization term D^-0.5*A*D^-0.5 to propagate messages through normalized adjacency\n",
        "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
        "        row, col = edge_index\n",
        "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
        "        deg_inv_sqrt = deg.clamp(min=1).pow(-0.5)\n",
        "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "        return self.propagate(edge_index, x=x, norm=norm)\n",
        "\n",
        "    # Scales the neighbor embeddings\n",
        "    def message(self, x_j: torch.Tensor, norm: torch.Tensor) -> torch.Tensor:\n",
        "        return norm.view(-1, 1) * x_j\n",
        "\n",
        "# -------- LightGCN encoder + dot-product decoder --------\n",
        "class LightGCN(nn.Module):\n",
        "    # Initialize trainable node embeddings\n",
        "    def __init__(self, num_nodes: int, emb_dim: int = 64, num_layers: int = 3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_nodes, emb_dim)\n",
        "        nn.init.xavier_uniform_(self.embedding.weight)\n",
        "        self.convs = nn.ModuleList([LightGCNConv() for _ in range(num_layers)])\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def encode(self, edge_index: torch.Tensor) -> torch.Tensor:\n",
        "        x0 = self.embedding.weight\n",
        "        out = x0\n",
        "        x = x0\n",
        "        # Each layer's output is accumulated and averaged\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index)\n",
        "            out = out + x\n",
        "        return out / (self.num_layers + 1)\n",
        "\n",
        "    # Compute dot product between node embeddings for each edge (positive or negative pair)\n",
        "    @staticmethod\n",
        "    def decode(z: torch.Tensor, pairs: torch.LongTensor) -> torch.Tensor:\n",
        "        # pairs: [2, B] with [src; dst]\n",
        "        return (z[pairs[0]] * z[pairs[1]]).sum(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "34dbf35c",
      "metadata": {
        "id": "34dbf35c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def evaluate_auc_hits(model, triples, num_entities, edge_index=None, batch_size=4096, device=None):\n",
        "    \"\"\"\n",
        "    Evaluate AUC and Hits@K (K=1,5,10) for LightGCN or R-LightGCN.\n",
        "    Unfiltered version — each positive triple is compared to 99 random negatives.\n",
        "\n",
        "    Args:\n",
        "        model: LightGCN or R-LightGCN with .encode()\n",
        "        triples: torch.Tensor [N, 3] or [N, 2] validation/test triples\n",
        "        num_entities: total number of entities\n",
        "        edge_index: graph structure for encoding\n",
        "        batch_size: number of triples per batch\n",
        "        device: torch.device to use\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    if device is None:\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "    # -------------------------\n",
        "    # Helper functions\n",
        "    # -------------------------\n",
        "    def batch_iter(tensor, size):\n",
        "        for i in range(0, len(tensor), size):\n",
        "            yield tensor[i:i + size]\n",
        "\n",
        "    def sample_negatives(pos_batch, num_entities):\n",
        "        \"\"\"Corrupt tail entities randomly.\"\"\"\n",
        "        neg_batch = pos_batch.clone()\n",
        "        neg_batch[:, -1] = torch.randint(0, num_entities, (pos_batch.size(0),), device=pos_batch.device)\n",
        "        return neg_batch\n",
        "\n",
        "    # -------------------------\n",
        "    # 1️⃣ AUC computation\n",
        "    # -------------------------\n",
        "    scores_all, labels_all = [], []\n",
        "\n",
        "    for pos in batch_iter(triples, batch_size):\n",
        "        pos = pos.to(device)\n",
        "        neg = sample_negatives(pos, num_entities)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if hasattr(model, 'encode'):\n",
        "                emb = model.encode(edge_index)\n",
        "                s_pos = (emb[pos[:, 0]] * emb[pos[:, -1]]).sum(dim=1)\n",
        "                s_neg = (emb[neg[:, 0]] * emb[neg[:, -1]]).sum(dim=1)\n",
        "            else:\n",
        "                s_pos = model(pos)\n",
        "                s_neg = model(neg)\n",
        "\n",
        "            scores_all.append(torch.cat([s_pos, s_neg], dim=0))\n",
        "            labels_all.append(torch.cat([torch.ones_like(s_pos), torch.zeros_like(s_neg)], dim=0))\n",
        "\n",
        "    # If no scores were collected, return defaults\n",
        "    if len(scores_all) == 0:\n",
        "        return {\"auc\": 0.5, \"hits@1\": 0.0, \"hits@5\": 0.0, \"hits@10\": 0.0}\n",
        "\n",
        "    scores_all = torch.cat(scores_all, dim=0)\n",
        "    labels_all = torch.cat(labels_all, dim=0)\n",
        "    # Compute ROC-AUC using torchmetrics (pure torch, avoids numpy)\n",
        "    try:\n",
        "        from torchmetrics.classification import BinaryAUROC\n",
        "        auroc_metric = BinaryAUROC().to(scores_all.device)\n",
        "        auc = auroc_metric(scores_all, labels_all.int()).item()\n",
        "    except Exception:\n",
        "        auc = 0.5\n",
        "\n",
        "    # -------------------------\n",
        "    # 2️⃣ Hits@K computation (1,5,10)\n",
        "    # -------------------------\n",
        "    hits_at = {1: 0, 5: 0, 10: 0}\n",
        "    n_trials = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        ent = model.encode(edge_index) if hasattr(model, 'encode') else model.encoder(edge_index)\n",
        "\n",
        "        for pos in batch_iter(triples, batch_size):\n",
        "            pos = pos.to(device)\n",
        "            B = pos.size(0)\n",
        "            true_t = pos[:, -1]\n",
        "            rand_t = torch.randint(0, num_entities, (B, 99), device=device)\n",
        "            tails = torch.cat([true_t.unsqueeze(1), rand_t], dim=1)  # [B,100]\n",
        "\n",
        "            e_h = ent[pos[:, 0]]                                    # [B,d]\n",
        "            e_candidates = ent[tails]                               # [B,100,d]\n",
        "            s = (e_h.unsqueeze(1) * e_candidates).sum(dim=2)        # [B,100]\n",
        "\n",
        "            # rank position of true tail\n",
        "            ranks = (s.argsort(dim=1, descending=True) == 0).nonzero()[:, 1] + 1  # 1-based\n",
        "\n",
        "            for k in hits_at.keys():\n",
        "                hits_at[k] += (ranks <= k).sum().item()\n",
        "\n",
        "            n_trials += B\n",
        "\n",
        "    # Normalize\n",
        "    hits_at = {f\"hits@{k}\": v / n_trials for k, v in hits_at.items()}\n",
        "\n",
        "    return {\"auc\": float(auc), **hits_at}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "fd155a53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd155a53",
        "outputId": "cc4eb029-5ed2-435a-bf02-f949e40a2484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting LightGCN training...\n",
            "Max epochs: 100, Early stopping patience: 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining LightGCN:   0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   1: Loss=0.9951\n",
            "\n",
            "Evaluating epoch 1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining LightGCN:   1%|          | 1/100 [00:02<04:52,  2.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss=0.9951, AUC=0.6641, Hits@1=0.0740, Hits@5=0.2381, Hits@10=0.3330\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training LightGCN:   4%|▍         | 4/100 [00:03<00:53,  1.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   2: Loss=0.9417\n",
            "Epoch   3: Loss=0.9027\n",
            "Epoch   4: Loss=0.8785\n",
            "Epoch   5: Loss=0.8564\n",
            "\n",
            "Evaluating epoch 5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training LightGCN:   8%|▊         | 8/100 [00:03<00:20,  4.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Loss=0.8564, AUC=0.7577, Hits@1=0.2381, Hits@5=0.4488, Hits@10=0.5313\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training LightGCN:  12%|█▏        | 12/100 [00:04<00:13,  6.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  10: Loss=0.7851\n",
            "\n",
            "Evaluating epoch 10...\n",
            "Epoch 10: Loss=0.7851, AUC=0.8185, Hits@1=0.3150, Hits@5=0.5361, Hits@10=0.6186\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training LightGCN:  16%|█▌        | 16/100 [00:04<00:09,  9.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  15: Loss=0.7535\n",
            "\n",
            "Evaluating epoch 15...\n",
            "Epoch 15: Loss=0.7535, AUC=0.8443, Hits@1=0.3235, Hits@5=0.5778, Hits@10=0.6584\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training LightGCN:  22%|██▏       | 22/100 [00:05<00:07, 10.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  20: Loss=0.7315\n",
            "\n",
            "Evaluating epoch 20...\n",
            "Epoch 20: Loss=0.7315, AUC=0.8469, Hits@1=0.3444, Hits@5=0.5987, Hits@10=0.6793\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training LightGCN:  26%|██▌       | 26/100 [00:05<00:08,  8.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  25: Loss=0.7196\n",
            "\n",
            "Evaluating epoch 25...\n",
            "Epoch 25: Loss=0.7196, AUC=0.8568, Hits@1=0.3510, Hits@5=0.6053, Hits@10=0.6935\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training LightGCN:  32%|███▏      | 32/100 [00:05<00:05, 12.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  30: Loss=0.7104\n",
            "\n",
            "Evaluating epoch 30...\n",
            "Epoch 30: Loss=0.7104, AUC=0.8547, Hits@1=0.3463, Hits@5=0.6063, Hits@10=0.6907\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training LightGCN:  36%|███▌      | 36/100 [00:06<00:06, 10.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  35: Loss=0.7011\n",
            "\n",
            "Evaluating epoch 35...\n",
            "Epoch 35: Loss=0.7011, AUC=0.8714, Hits@1=0.3378, Hits@5=0.6148, Hits@10=0.6935\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training LightGCN:  42%|████▏     | 42/100 [00:07<00:05,  9.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  40: Loss=0.6979\n",
            "\n",
            "Evaluating epoch 40...\n",
            "Epoch 40: Loss=0.6979, AUC=0.8681, Hits@1=0.3359, Hits@5=0.6176, Hits@10=0.7078\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training LightGCN:  46%|████▌     | 46/100 [00:07<00:04, 11.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  45: Loss=0.6917\n",
            "\n",
            "Evaluating epoch 45...\n",
            "Epoch 45: Loss=0.6917, AUC=0.8640, Hits@1=0.3444, Hits@5=0.6214, Hits@10=0.7040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training LightGCN:  52%|█████▏    | 52/100 [00:07<00:04, 11.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  50: Loss=0.6912\n",
            "\n",
            "Evaluating epoch 50...\n",
            "Epoch 50: Loss=0.6912, AUC=0.8740, Hits@1=0.3416, Hits@5=0.6233, Hits@10=0.7087\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training LightGCN:  56%|█████▌    | 56/100 [00:08<00:04,  8.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  55: Loss=0.6875\n",
            "\n",
            "Evaluating epoch 55...\n",
            "Epoch 55: Loss=0.6875, AUC=0.8662, Hits@1=0.3406, Hits@5=0.6063, Hits@10=0.7049\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training LightGCN:  60%|██████    | 60/100 [00:08<00:03, 11.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  60: Loss=0.6830\n",
            "\n",
            "Evaluating epoch 60...\n",
            "Epoch 60: Loss=0.6830, AUC=0.8783, Hits@1=0.3416, Hits@5=0.6224, Hits@10=0.7087\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training LightGCN:  66%|██████▌   | 66/100 [00:09<00:03, 10.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  65: Loss=0.6834\n",
            "\n",
            "Evaluating epoch 65...\n",
            "Epoch 65: Loss=0.6834, AUC=0.8686, Hits@1=0.3491, Hits@5=0.6233, Hits@10=0.7087\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training LightGCN:  72%|███████▏  | 72/100 [00:09<00:02, 10.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  70: Loss=0.6847\n",
            "\n",
            "Evaluating epoch 70...\n",
            "Epoch 70: Loss=0.6847, AUC=0.8810, Hits@1=0.3416, Hits@5=0.6271, Hits@10=0.7097\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training LightGCN:  76%|███████▌  | 76/100 [00:10<00:02, 11.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  75: Loss=0.6852\n",
            "\n",
            "Evaluating epoch 75...\n",
            "Epoch 75: Loss=0.6852, AUC=0.8760, Hits@1=0.3558, Hits@5=0.6214, Hits@10=0.7182\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training LightGCN:  82%|████████▏ | 82/100 [00:10<00:01, 11.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  80: Loss=0.6752\n",
            "\n",
            "Evaluating epoch 80...\n",
            "Epoch 80: Loss=0.6752, AUC=0.8727, Hits@1=0.3491, Hits@5=0.6357, Hits@10=0.7154\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training LightGCN:  86%|████████▌ | 86/100 [00:11<00:01,  9.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  85: Loss=0.6784\n",
            "\n",
            "Evaluating epoch 85...\n",
            "Epoch 85: Loss=0.6784, AUC=0.8722, Hits@1=0.3558, Hits@5=0.6243, Hits@10=0.7192\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training LightGCN:  92%|█████████▏| 92/100 [00:11<00:00, 12.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  90: Loss=0.6811\n",
            "\n",
            "Evaluating epoch 90...\n",
            "Epoch 90: Loss=0.6811, AUC=0.8715, Hits@1=0.3529, Hits@5=0.6357, Hits@10=0.7116\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training LightGCN:  96%|█████████▌| 96/100 [00:12<00:00, 10.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  95: Loss=0.6743\n",
            "\n",
            "Evaluating epoch 95...\n",
            "Epoch 95: Loss=0.6743, AUC=0.8780, Hits@1=0.3349, Hits@5=0.6433, Hits@10=0.7135\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training LightGCN: 100%|██████████| 100/100 [00:12<00:00,  7.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 100: Loss=0.6732\n",
            "\n",
            "Evaluating epoch 100...\n",
            "Epoch 100: Loss=0.6732, AUC=0.8798, Hits@1=0.3425, Hits@5=0.6243, Hits@10=0.7059\n",
            "\n",
            "LightGCN training completed!\n",
            "\n",
            "Running final test evaluation...\n",
            "\n",
            "[LightGCN CORA TEST RESULTS]\n",
            "AUC: 0.8768\n",
            "Hits@1: 0.3567\n",
            "Hits@5: 0.6214\n",
            "Hits@10: 0.7068\n",
            "Final model saved: /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/cora_lightgcn_model.pt\n",
            "Saving 21 consistent metric entries...\n",
            "Metrics saved to /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/cora_lightgcn_metrics.txt\n",
            "Full model + training history bundle saved: /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/cora_lightgcn_full_bundle.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# Hyperparameters\n",
        "# ===============================\n",
        "lr = 1e-3\n",
        "epochs = 100\n",
        "emb_dim = 64\n",
        "num_layers = 3\n",
        "eval_every = 5\n",
        "batch_size = 2048\n",
        "patience = 10\n",
        "\n",
        "# ===============================\n",
        "# Setup\n",
        "# ===============================\n",
        "lightgcn_metrics_tracker = MetricsTracker()\n",
        "lightgcn_early_stopping = EarlyStopping(patience=patience, metric='val_auc')\n",
        "\n",
        "# Create model + optimizer\n",
        "lightgcn_model = LightGCN(num_nodes=num_entities, emb_dim=emb_dim, num_layers=num_layers).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(lightgcn_model.parameters(), lr=lr)\n",
        "\n",
        "print(\"Starting LightGCN training...\")\n",
        "print(f\"Max epochs: {epochs}, Early stopping patience: {patience}\")\n",
        "\n",
        "# ===============================\n",
        "# Training Loop\n",
        "# ===============================\n",
        "for epoch in tqdm(range(1, epochs + 1), desc=\"Training LightGCN\"):\n",
        "    avg_loss = train_one_epoch_lightgcn(\n",
        "        lightgcn_model, train_loader_collapsed, optimizer,\n",
        "        edge_index, device, num_entities\n",
        "    )\n",
        "\n",
        "    if epoch <= 5 or epoch % 5 == 0:\n",
        "        print(f\"Epoch {epoch:3d}: Loss={avg_loss:.4f}\")\n",
        "\n",
        "    # ===============================\n",
        "    # Evaluation\n",
        "    # ===============================\n",
        "    if epoch % eval_every == 0 or epoch == 1:\n",
        "        print(f\"\\nEvaluating epoch {epoch}...\")\n",
        "        # Convert val_loader to flat tensor of pairs/triples\n",
        "        all_val_data = []\n",
        "        for batch in val_loader_collapsed:\n",
        "            data_tensor = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
        "            if data_tensor.dim() == 1:\n",
        "                data_tensor = data_tensor.unsqueeze(0)\n",
        "            all_val_data.append(data_tensor)\n",
        "        val_triples = torch.cat(all_val_data, dim=0)\n",
        "\n",
        "        # Run simplified evaluation (AUC + Hits@K)\n",
        "        val_metrics = evaluate_auc_hits(\n",
        "            lightgcn_model,\n",
        "            val_triples,\n",
        "            num_entities=num_entities,\n",
        "            edge_index=edge_index,\n",
        "            batch_size=2048,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Print summary\n",
        "        print(f\"Epoch {epoch}: Loss={avg_loss:.4f}, \"\n",
        "              f\"AUC={val_metrics['auc']:.4f}, \"\n",
        "              f\"Hits@1={val_metrics['hits@1']:.4f}, \"\n",
        "              f\"Hits@5={val_metrics['hits@5']:.4f}, \"\n",
        "              f\"Hits@10={val_metrics['hits@10']:.4f}\")\n",
        "\n",
        "        # Log to tracker (simple flattening)\n",
        "        lightgcn_metrics_tracker.add(\n",
        "            epoch=epoch,\n",
        "            loss=avg_loss,\n",
        "            val_auc=val_metrics['auc'],\n",
        "            val_hits1=val_metrics['hits@1'],\n",
        "            val_hits5=val_metrics['hits@5'],\n",
        "            val_hits10=val_metrics['hits@10']\n",
        "        )\n",
        "\n",
        "        # Early stopping\n",
        "        if lightgcn_early_stopping(lightgcn_metrics_tracker):\n",
        "            print(f\"Early stopping at epoch {epoch} \"\n",
        "                  f\"(Best AUC: {lightgcn_early_stopping.best_score:.4f})\")\n",
        "            break\n",
        "\n",
        "print(\"\\nLightGCN training completed!\")\n",
        "\n",
        "# ===============================\n",
        "# Final Test Evaluation\n",
        "# ===============================\n",
        "print(\"\\nRunning final test evaluation...\")\n",
        "all_test_data = []\n",
        "for batch in test_loader_collapsed:\n",
        "    data_tensor = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
        "    if data_tensor.dim() == 1:\n",
        "        data_tensor = data_tensor.unsqueeze(0)\n",
        "    all_test_data.append(data_tensor)\n",
        "test_triples = torch.cat(all_test_data, dim=0)\n",
        "\n",
        "lightgcn_final_test_metrics = evaluate_auc_hits(\n",
        "    lightgcn_model,\n",
        "    test_triples,\n",
        "    num_entities=num_entities,\n",
        "    edge_index=edge_index,\n",
        "    batch_size=2048,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"\\n[LightGCN CORA TEST RESULTS]\")\n",
        "print(f\"AUC: {lightgcn_final_test_metrics['auc']:.4f}\")\n",
        "print(f\"Hits@1: {lightgcn_final_test_metrics['hits@1']:.4f}\")\n",
        "print(f\"Hits@5: {lightgcn_final_test_metrics['hits@5']:.4f}\")\n",
        "print(f\"Hits@10: {lightgcn_final_test_metrics['hits@10']:.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# Save model + metrics\n",
        "# ===============================\n",
        "save_dir = \"/content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# --- Save final model ---\n",
        "model_file = os.path.join(save_dir, \"cora_lightgcn_model.pt\")\n",
        "torch.save(lightgcn_model.state_dict(), model_file)\n",
        "print(f\"Final model saved: {model_file}\")\n",
        "\n",
        "# --- Prepare and align metrics ---\n",
        "metrics = lightgcn_metrics_tracker.metrics\n",
        "min_len = min(len(v) for v in metrics.values())\n",
        "print(f\"Saving {min_len} consistent metric entries...\")\n",
        "\n",
        "# --- Save metrics safely ---\n",
        "metrics_file = os.path.join(save_dir, \"cora_lightgcn_metrics.txt\")\n",
        "with open(metrics_file, \"w\") as f:\n",
        "    f.write(\"Epoch\\tLoss\\tVal_AUC\\tHits@1\\tHits@5\\tHits@10\\n\")\n",
        "    for i in range(min_len):\n",
        "        f.write(\n",
        "            f\"{metrics['epoch'][i]}\\t\"\n",
        "            f\"{metrics['loss'][i]:.6f}\\t\"\n",
        "            f\"{metrics['val_auc'][i]:.6f}\\t\"\n",
        "            f\"{metrics['val_hits1'][i]:.6f}\\t\"\n",
        "            f\"{metrics['val_hits5'][i]:.6f}\\t\"\n",
        "            f\"{metrics['val_hits10'][i]:.6f}\\n\"\n",
        "        )\n",
        "print(f\"Metrics saved to {metrics_file}\")\n",
        "\n",
        "# --- Optional: save complete training bundle ---\n",
        "bundle_file = os.path.join(save_dir, \"cora_lightgcn_full_bundle.pt\")\n",
        "torch.save({\n",
        "    \"model_state_dict\": lightgcn_model.state_dict(),\n",
        "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    \"hyperparameters\": {\n",
        "        \"emb_dim\": emb_dim,\n",
        "        \"num_layers\": num_layers,\n",
        "        \"lr\": lr,\n",
        "        \"num_entities\": num_entities\n",
        "    },\n",
        "    \"final_test_metrics\": lightgcn_final_test_metrics,\n",
        "    \"training_history\": metrics\n",
        "}, bundle_file)\n",
        "print(f\"Full model + training history bundle saved: {bundle_file}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

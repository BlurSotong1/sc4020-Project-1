{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1229a7677039018",
   "metadata": {},
   "source": [
    "# Dataset loading from txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eaca170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.utils import degree, add_self_loops\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df22f52f",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T10:16:03.385441Z",
     "start_time": "2025-09-22T10:16:03.377007Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#entities=40943, #relations=11\n",
      "edge_index: (2, 214613)\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---- Paths (adjust if needed) ----\n",
    "_train_path = Path(\"../WN18RR/train.txt\")\n",
    "_valid_path = Path(\"../WN18RR/valid.txt\")\n",
    "_test_path  = Path(\"../WN18RR/test.txt\")\n",
    "\n",
    "def load_dataset(path: Path) -> List[Tuple[str, str, str]]:\n",
    "    data = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            h, r, t = line.strip().split(\"\\t\")\n",
    "            data.append((h, r, t))\n",
    "    return data\n",
    "\n",
    "train_dataset = load_dataset(_train_path)\n",
    "valid_dataset = load_dataset(_valid_path)\n",
    "test_dataset  = load_dataset(_test_path)\n",
    "\n",
    "# ---- Build ID maps ----\n",
    "entities: set[str] = set()\n",
    "relations: set[str] = set()\n",
    "for h, r, t in (train_dataset + valid_dataset + test_dataset):\n",
    "    entities.add(h); entities.add(t); relations.add(r)\n",
    "\n",
    "ent2id: Dict[str, int] = {e: i for i, e in enumerate(sorted(entities))}\n",
    "rel2id: Dict[str, int] = {r: i for i, r in enumerate(sorted(relations))}\n",
    "id2ent = {v: k for k, v in ent2id.items()}\n",
    "id2rel = {v: k for k, v in rel2id.items()}\n",
    "\n",
    "num_entities  = len(ent2id)\n",
    "num_relations = len(rel2id)\n",
    "print(f\"#entities={num_entities}, #relations={num_relations}\")\n",
    "\n",
    "def triples_to_tensor(triples: List[Tuple[str, str, str]]) -> torch.LongTensor:\n",
    "    arr = np.array([(ent2id[h], rel2id[r], ent2id[t]) for h, r, t in triples], dtype=np.int64)\n",
    "    return torch.from_numpy(arr)\n",
    "\n",
    "train_triples = triples_to_tensor(train_dataset).to(device)  # [Ntr, 3] (h, r, t)\n",
    "valid_triples = triples_to_tensor(valid_dataset).to(device)  # [Nv, 3]\n",
    "test_triples  = triples_to_tensor(test_dataset).to(device)   # [Nt, 3]\n",
    "\n",
    "# ---- Collapsed undirected graph for LightGCN encoder ----\n",
    "edges = []\n",
    "for h, r, t in train_triples.tolist():\n",
    "    edges.append((h, t))\n",
    "    edges.append((t, h))\n",
    "edge_index = torch.tensor(edges, dtype=torch.long, device=device).t().contiguous()  # [2, E]\n",
    "edge_index, _ = add_self_loops(edge_index, num_nodes=num_entities)  # optional self-loops\n",
    "print(\"edge_index:\", tuple(edge_index.size()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e07fe58094f09",
   "metadata": {},
   "source": [
    "# Model definition and decoder \n",
    "\n",
    "```LightGCN``` :expects 2 types of data when using hetero:\n",
    "\n",
    "\n",
    "```x_dict```   : which is the dictionary of embedding related to that node (trainable)\n",
    "\n",
    "\n",
    "```edge_index_dict```  : which is the ```{('entity', relation, 'entity'): tensor[[src],[dst]]}``` \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5262f48ffc73d04",
   "metadata": {},
   "source": [
    "we also add new inverse type of relation on top of the 11 that already exists.\n",
    "this allows for information to be passed around which originally didnt.\n",
    "A -> B is one way, and there should be an inverse relationship (or some information) which is missed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c20cda71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch \n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "# -------- LightGCN layer (parameter-free) --------\n",
    "class LightGCNConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr='add')\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.clamp(min=1).pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j: torch.Tensor, norm: torch.Tensor) -> torch.Tensor:\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "# -------- LightGCN encoder + dot-product decoder --------\n",
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, num_nodes: int, emb_dim: int = 64, num_layers: int = 3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_nodes, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        self.convs = nn.ModuleList([LightGCNConv() for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def encode(self, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        x0 = self.embedding.weight\n",
    "        out = x0\n",
    "        x = x0\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            out = out + x\n",
    "        return out / (self.num_layers + 1)   # layer-wise average\n",
    "\n",
    "    @staticmethod\n",
    "    def decode(z: torch.Tensor, pairs: torch.LongTensor) -> torch.Tensor:\n",
    "        # pairs: [2, B] with [src; dst]\n",
    "        return (z[pairs[0]] * z[pairs[1]]).sum(dim=1)\n",
    "\n",
    "# -------- R-LightGCN: relation-aware propagation (learnable per-relation scalars) --------\n",
    "class RLightGCNConv(MessagePassing):\n",
    "    def __init__(self, num_relations: int):\n",
    "        super().__init__(aggr='add')\n",
    "        self.alpha = nn.Parameter(torch.ones(num_relations))  # per-relation scalar\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_type: torch.LongTensor) -> torch.Tensor:\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.clamp(min=1).pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        return self.propagate(edge_index, x=x, edge_type=edge_type, norm=norm)\n",
    "\n",
    "    def message(self, x_j: torch.Tensor, edge_type: torch.LongTensor, norm: torch.Tensor) -> torch.Tensor:\n",
    "        w = self.alpha[edge_type].view(-1, 1)    # weight message by relation type\n",
    "        return w * norm.view(-1, 1) * x_j\n",
    "\n",
    "class RLightGCN(nn.Module):\n",
    "    def __init__(self, num_nodes: int, num_relations: int, emb_dim: int = 64, num_layers: int = 3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_nodes, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        self.convs = nn.ModuleList([RLightGCNConv(num_relations) for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def encode(self, edge_index: torch.Tensor, edge_type: torch.LongTensor) -> torch.Tensor:\n",
    "        x0 = self.embedding.weight\n",
    "        out = x0\n",
    "        x = x0\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index, edge_type)\n",
    "            out = out + x\n",
    "        return out / (self.num_layers + 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def decode(z: torch.Tensor, pairs: torch.LongTensor) -> torch.Tensor:\n",
    "        return (z[pairs[0]] * z[pairs[1]]).sum(dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ce631",
   "metadata": {},
   "source": [
    "# Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef898ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pairs_from_triples(triples: torch.LongTensor) -> torch.LongTensor:\n",
    "    \"\"\"\n",
    "    Convert (h, r, t) -> pairs [2, N] = (h, t) for decoding on collapsed graph.\n",
    "    \"\"\"\n",
    "    return triples[:, [0, 2]].t().contiguous()  # [2, N]\n",
    "\n",
    "@torch.no_grad()\n",
    "def negative_sample_heads(triples: torch.LongTensor, num_nodes: int) -> torch.LongTensor:\n",
    "    \"\"\"\n",
    "    Corrupt heads: (h, r, t) -> (h', t)\n",
    "    Returns pairs [2, N].\n",
    "    \"\"\"\n",
    "    N = triples.size(0)\n",
    "    neg_h = torch.randint(0, num_nodes, (N,), device=triples.device)\n",
    "    t = triples[:, 2]\n",
    "    return torch.stack([neg_h, t], dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def negative_sample_tails(triples: torch.LongTensor, num_nodes: int) -> torch.LongTensor:\n",
    "    \"\"\"\n",
    "    Corrupt tails: (h, r, t) -> (h, t')\n",
    "    Returns pairs [2, N].\n",
    "    \"\"\"\n",
    "    N = triples.size(0)\n",
    "    h = triples[:, 0]\n",
    "    neg_t = torch.randint(0, num_nodes, (N,), device=triples.device)\n",
    "    return torch.stack([h, neg_t], dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116acbcc",
   "metadata": {},
   "source": [
    "# Training and Batch eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5296bcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_loss(pos_scores: torch.Tensor, neg_scores: torch.Tensor) -> torch.Tensor:\n",
    "    # Bayesian Personalized Ranking loss\n",
    "    return -torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-12).mean()\n",
    "\n",
    "def logistic_pair_loss(pos_scores: torch.Tensor, neg_scores: torch.Tensor) -> torch.Tensor:\n",
    "    # Alternative: pairwise logistic\n",
    "    return -F.logsigmoid(pos_scores).mean() - F.logsigmoid(-neg_scores).mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def batch_scores(z: torch.Tensor, pairs: torch.LongTensor, batch_size: int = 4096) -> torch.Tensor:\n",
    "    scores = []\n",
    "    for i in range(0, pairs.size(1), batch_size):\n",
    "        batch = pairs[:, i:i+batch_size]\n",
    "        s = (z[batch[0]] * z[batch[1]]).sum(dim=1)\n",
    "        scores.append(s)\n",
    "    return torch.cat(scores, dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad8da9f",
   "metadata": {},
   "source": [
    "# Evaluation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7f63857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_auc_ap(z: torch.Tensor,\n",
    "                    pos_triples: torch.LongTensor,\n",
    "                    num_nodes: int,\n",
    "                    neg_mode: str = \"head\",\n",
    "                    batch_size: int = 4096) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Simple AUC/AP using equal number of sampled negatives.\n",
    "    neg_mode: \"head\" or \"tail\" corruption.\n",
    "    \"\"\"\n",
    "    pos_pairs = pairs_from_triples(pos_triples)\n",
    "    if neg_mode == \"head\":\n",
    "        neg_pairs = negative_sample_heads(pos_triples, num_nodes)\n",
    "    else:\n",
    "        neg_pairs = negative_sample_tails(pos_triples, num_nodes)\n",
    "\n",
    "    pos = batch_scores(z, pos_pairs, batch_size=batch_size).detach().cpu().numpy()\n",
    "    neg = batch_scores(z, neg_pairs, batch_size=batch_size).detach().cpu().numpy()\n",
    "\n",
    "    y_true = np.concatenate([np.ones_like(pos), np.zeros_like(neg)])\n",
    "    y_score = np.concatenate([pos, neg])\n",
    "\n",
    "    return {\n",
    "        \"AUC\": float(roc_auc_score(y_true, y_score)),\n",
    "        \"AP\":  float(average_precision_score(y_true, y_score)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd03f983",
   "metadata": {},
   "source": [
    "# Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e0f66fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGCN] Epoch 001 | Loss 0.6931 | Val(AUC/AP head) 0.7077/0.7713 | Val(AUC/AP tail) 0.7098/0.7640\n",
      "[LightGCN] Epoch 005 | Loss 0.6929 | Val(AUC/AP head) 0.7899/0.8407 | Val(AUC/AP tail) 0.7920/0.8474\n",
      "[LightGCN] Epoch 010 | Loss 0.6924 | Val(AUC/AP head) 0.8292/0.8685 | Val(AUC/AP tail) 0.8313/0.8803\n",
      "[LightGCN] Epoch 015 | Loss 0.6915 | Val(AUC/AP head) 0.8427/0.8766 | Val(AUC/AP tail) 0.8486/0.8915\n",
      "[LightGCN] Epoch 020 | Loss 0.6898 | Val(AUC/AP head) 0.8533/0.8839 | Val(AUC/AP tail) 0.8618/0.8990\n",
      "[LightGCN] Epoch 025 | Loss 0.6873 | Val(AUC/AP head) 0.8594/0.8849 | Val(AUC/AP tail) 0.8668/0.9017\n",
      "[LightGCN] Epoch 030 | Loss 0.6839 | Val(AUC/AP head) 0.8668/0.8906 | Val(AUC/AP tail) 0.8735/0.9098\n",
      "[LightGCN] Epoch 035 | Loss 0.6794 | Val(AUC/AP head) 0.8685/0.8946 | Val(AUC/AP tail) 0.8784/0.9078\n",
      "[LightGCN] Epoch 040 | Loss 0.6738 | Val(AUC/AP head) 0.8748/0.8979 | Val(AUC/AP tail) 0.8818/0.9134\n",
      "[LightGCN] Epoch 045 | Loss 0.6671 | Val(AUC/AP head) 0.8775/0.9008 | Val(AUC/AP tail) 0.8842/0.9127\n",
      "[LightGCN] Epoch 050 | Loss 0.6593 | Val(AUC/AP head) 0.8752/0.8940 | Val(AUC/AP tail) 0.8842/0.9124\n",
      "[LightGCN][TEST] AUC/AP head 0.8822/0.8978 | AUC/AP tail 0.8889/0.9116\n"
     ]
    }
   ],
   "source": [
    "# -------- Train LightGCN on collapsed graph (recommendation-style link prediction) --------\n",
    "lr = 1e-3\n",
    "epochs = 50\n",
    "emb_dim = 64\n",
    "num_layers = 3\n",
    "eval_every = 5\n",
    "batch_size = 4096  # scoring batch (not training mini-batch; training here uses full graph + full triples)\n",
    "\n",
    "model = LightGCN(num_nodes=num_entities, emb_dim=emb_dim, num_layers=num_layers).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    z = model.encode(edge_index)\n",
    "\n",
    "    pos_pairs = pairs_from_triples(train_triples)             # [2, N]\n",
    "    neg_pairs = negative_sample_heads(train_triples, num_entities)  # [2, N] (you can mix head/tail)\n",
    "\n",
    "    pos_scores = model.decode(z, pos_pairs)\n",
    "    neg_scores = model.decode(z, neg_pairs)\n",
    "\n",
    "    loss = F.binary_cross_entropy_with_logits(\n",
    "    torch.cat([pos_scores, neg_scores]),\n",
    "    torch.cat([torch.ones_like(pos_scores), torch.zeros_like(neg_scores)])\n",
    "    )\n",
    "\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if epoch % eval_every == 0 or epoch == 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            z = model.encode(edge_index)\n",
    "            val_metrics_h = evaluate_auc_ap(z, valid_triples, num_entities, neg_mode=\"head\")\n",
    "            val_metrics_t = evaluate_auc_ap(z, valid_triples, num_entities, neg_mode=\"tail\")\n",
    "        print(f\"[LightGCN] Epoch {epoch:03d} | Loss {loss.item():.4f} | \"\n",
    "              f\"Val(AUC/AP head) {val_metrics_h['AUC']:.4f}/{val_metrics_h['AP']:.4f} | \"\n",
    "              f\"Val(AUC/AP tail) {val_metrics_t['AUC']:.4f}/{val_metrics_t['AP']:.4f}\")\n",
    "\n",
    "# Final test\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = model.encode(edge_index)\n",
    "    test_h = evaluate_auc_ap(z, test_triples, num_entities, neg_mode=\"head\")\n",
    "    test_t = evaluate_auc_ap(z, test_triples, num_entities, neg_mode=\"tail\")\n",
    "print(f\"[LightGCN][TEST] AUC/AP head {test_h['AUC']:.4f}/{test_h['AP']:.4f} | \"\n",
    "      f\"AUC/AP tail {test_t['AUC']:.4f}/{test_t['AP']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b7e052",
   "metadata": {},
   "source": [
    "# R-LightGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f1b048c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[R-LightGCN] Epoch 001 | Loss 0.6931 | Val(AUC/AP head) 0.7117/0.7706 | Val(AUC/AP tail) 0.7041/0.7594\n",
      "[R-LightGCN] Epoch 005 | Loss 0.6929 | Val(AUC/AP head) 0.7971/0.8479 | Val(AUC/AP tail) 0.7976/0.8495\n",
      "[R-LightGCN] Epoch 010 | Loss 0.6924 | Val(AUC/AP head) 0.8288/0.8665 | Val(AUC/AP tail) 0.8345/0.8809\n",
      "[R-LightGCN] Epoch 015 | Loss 0.6914 | Val(AUC/AP head) 0.8494/0.8814 | Val(AUC/AP tail) 0.8483/0.8919\n",
      "[R-LightGCN] Epoch 020 | Loss 0.6896 | Val(AUC/AP head) 0.8533/0.8802 | Val(AUC/AP tail) 0.8630/0.9005\n",
      "[R-LightGCN] Epoch 025 | Loss 0.6868 | Val(AUC/AP head) 0.8604/0.8841 | Val(AUC/AP tail) 0.8653/0.9016\n",
      "[R-LightGCN] Epoch 030 | Loss 0.6829 | Val(AUC/AP head) 0.8687/0.8948 | Val(AUC/AP tail) 0.8746/0.9083\n",
      "[R-LightGCN] Epoch 035 | Loss 0.6777 | Val(AUC/AP head) 0.8705/0.8958 | Val(AUC/AP tail) 0.8763/0.9068\n",
      "[R-LightGCN] Epoch 040 | Loss 0.6709 | Val(AUC/AP head) 0.8728/0.8962 | Val(AUC/AP tail) 0.8761/0.9048\n",
      "[R-LightGCN] Epoch 045 | Loss 0.6626 | Val(AUC/AP head) 0.8764/0.9003 | Val(AUC/AP tail) 0.8827/0.9134\n",
      "[R-LightGCN] Epoch 050 | Loss 0.6527 | Val(AUC/AP head) 0.8778/0.8982 | Val(AUC/AP tail) 0.8848/0.9120\n",
      "[R-LightGCN][TEST] AUC/AP head 0.8760/0.8988 | AUC/AP tail 0.8825/0.9118\n"
     ]
    }
   ],
   "source": [
    "# -------- Build relation-aware edge_index and edge_type (with reverse edges) --------\n",
    "rel_edges = []\n",
    "rel_types = []\n",
    "for h, r, t in train_triples.tolist():\n",
    "    rel_edges.append((h, t)); rel_types.append(r)\n",
    "    rel_edges.append((t, h)); rel_types.append(r)  # add reverse edge with same relation\n",
    "\n",
    "rel_edge_index = torch.tensor(rel_edges, dtype=torch.long, device=device).t().contiguous()\n",
    "edge_type = torch.tensor(rel_types, dtype=torch.long, device=device)\n",
    "\n",
    "# ---- Add self-loops with a new relation id ----\n",
    "self_loop_edges = torch.arange(num_entities, device=device)\n",
    "self_loop_edges = torch.stack([self_loop_edges, self_loop_edges], dim=0)  # [2, N]\n",
    "rel_edge_index = torch.cat([rel_edge_index, self_loop_edges], dim=1)\n",
    "\n",
    "self_loop_types = torch.full((num_entities,), num_relations, dtype=torch.long, device=device)\n",
    "edge_type = torch.cat([edge_type, self_loop_types], dim=0)\n",
    "\n",
    "# update relation count\n",
    "num_relations_with_loops = num_relations + 1\n",
    "\n",
    "# -------- Train R-LightGCN --------\n",
    "lr = 1e-3\n",
    "epochs = 50\n",
    "emb_dim = 64\n",
    "num_layers = 3\n",
    "eval_every = 5\n",
    "\n",
    "rmodel = RLightGCN(\n",
    "    num_nodes=num_entities,\n",
    "    num_relations=num_relations_with_loops,  # ✅ updated\n",
    "    emb_dim=emb_dim,\n",
    "    num_layers=num_layers\n",
    ").to(device)\n",
    "\n",
    "ropt = torch.optim.Adam(rmodel.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    rmodel.train()\n",
    "    ropt.zero_grad()\n",
    "\n",
    "    z = rmodel.encode(rel_edge_index, edge_type)\n",
    "\n",
    "    pos_pairs = pairs_from_triples(train_triples)\n",
    "    neg_pairs = negative_sample_heads(train_triples, num_entities)\n",
    "\n",
    "    pos_scores = rmodel.decode(z, pos_pairs)\n",
    "    neg_scores = rmodel.decode(z, neg_pairs)\n",
    "\n",
    "    loss = F.binary_cross_entropy_with_logits(\n",
    "    torch.cat([pos_scores, neg_scores]),\n",
    "    torch.cat([torch.ones_like(pos_scores), torch.zeros_like(neg_scores)])\n",
    "    )\n",
    "\n",
    "    loss.backward()\n",
    "    ropt.step()\n",
    "\n",
    "    if epoch % eval_every == 0 or epoch == 1:\n",
    "        rmodel.eval()\n",
    "        with torch.no_grad():\n",
    "            z = rmodel.encode(rel_edge_index, edge_type)\n",
    "            val_metrics_h = evaluate_auc_ap(z, valid_triples, num_entities, neg_mode=\"head\")\n",
    "            val_metrics_t = evaluate_auc_ap(z, valid_triples, num_entities, neg_mode=\"tail\")\n",
    "        print(f\"[R-LightGCN] Epoch {epoch:03d} | Loss {loss.item():.4f} | \"\n",
    "              f\"Val(AUC/AP head) {val_metrics_h['AUC']:.4f}/{val_metrics_h['AP']:.4f} | \"\n",
    "              f\"Val(AUC/AP tail) {val_metrics_t['AUC']:.4f}/{val_metrics_t['AP']:.4f}\")\n",
    "\n",
    "# Final test\n",
    "rmodel.eval()\n",
    "with torch.no_grad():\n",
    "    z = rmodel.encode(rel_edge_index, edge_type)\n",
    "    test_h = evaluate_auc_ap(z, test_triples, num_entities, neg_mode=\"head\")\n",
    "    test_t = evaluate_auc_ap(z, test_triples, num_entities, neg_mode=\"tail\")\n",
    "print(f\"[R-LightGCN][TEST] AUC/AP head {test_h['AUC']:.4f}/{test_h['AP']:.4f} | \"\n",
    "      f\"AUC/AP tail {test_t['AUC']:.4f}/{test_t['AP']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd130960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training is done\n",
    "\n",
    "# Save LightGCN\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": opt.state_dict(),\n",
    "    \"num_entities\": num_entities,\n",
    "    \"emb_dim\": emb_dim,\n",
    "    \"num_layers\": num_layers\n",
    "}, \"lightgcn_wn18rr.pt\")\n",
    "\n",
    "# Save R-LightGCN\n",
    "torch.save({\n",
    "    \"model_state_dict\": rmodel.state_dict(),\n",
    "    \"optimizer_state_dict\": ropt.state_dict(),\n",
    "    \"num_entities\": num_entities,\n",
    "    \"num_relations\": num_relations,\n",
    "    \"emb_dim\": emb_dim,\n",
    "    \"num_layers\": num_layers\n",
    "}, \"rlightgcn_wn18rr.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

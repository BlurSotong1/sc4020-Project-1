{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9fbce62e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fbce62e",
        "outputId": "e7bfa36a-1521-4e77-d707-78e1d434d534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.8.0+cu126\n",
            "Uninstalling torch-2.8.0+cu126:\n",
            "  Successfully uninstalled torch-2.8.0+cu126\n",
            "Found existing installation: torchvision 0.23.0+cu126\n",
            "Uninstalling torchvision-0.23.0+cu126:\n",
            "  Successfully uninstalled torchvision-0.23.0+cu126\n",
            "Found existing installation: torchaudio 2.8.0+cu126\n",
            "Uninstalling torchaudio-2.8.0+cu126:\n",
            "  Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.2.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.2.0%2Bcu118-cp312-cp312-linux_x86_64.whl (811.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.6/811.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.17.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.17.0%2Bcu118-cp312-cp312-linux_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.2.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.2.0%2Bcu118-cp312-cp312-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m129.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m143.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.7.0.84 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl (728.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.5/728.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.19.3 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.19.3-py3-none-manylinux1_x86_64.whl (135.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.0) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.0) (2.32.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.0) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.0) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchvision==0.17.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchvision==0.17.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchvision==0.17.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchvision==0.17.0) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.0) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-8.7.0.84 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.19.3 nvidia-nvtx-cu11-11.8.86 torch-2.2.0+cu118 torchaudio-2.2.0+cu118 torchvision-0.17.0+cu118\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.2.0+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu118/torch_scatter-2.1.2%2Bpt22cu118-cp312-cp312-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt22cu118\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.2.0+cu118)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (8.7.0.84)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.8.86)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, torch_geometric\n",
            "Successfully installed lightning-utilities-0.15.2 torch_geometric-2.7.0 torchmetrics-1.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n",
        "\n",
        "# Install PyTorch 2.2.0 with CUDA 11.8\n",
        "!pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Install torch-scatter for PyTorch 2.2.0 + CUDA 11.8\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-2.2.0+cu118.html\n",
        "\n",
        "!pip install torch_geometric torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f6010c34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6010c34",
        "outputId": "7c38a8cc-a92a-4523-a401-52d14fddc675"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-1241185422.py\", line 22, in <cell line: 0>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/__init__.py\", line 1471, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# Standard library\n",
        "# -----------------------------\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "\n",
        "# -----------------------------\n",
        "# Third-party libraries\n",
        "# -----------------------------\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from google.colab import drive\n",
        "\n",
        "# -----------------------------\n",
        "# PyTorch and PyG libraries\n",
        "# -----------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch_geometric.utils import degree, add_self_loops\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_scatter import scatter\n",
        "from torchmetrics.classification import BinaryAUROC\n",
        "\n",
        "# import sys\n",
        "# current_dir = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n",
        "# parent_dir = current_dir.parent\n",
        "# sys.path.append(str(parent_dir))\n",
        "# from utility.dataset_loader import KGDataModuleCollapsed, KGDataModuleTyped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "LrHH5gxS2OL1",
      "metadata": {
        "id": "LrHH5gxS2OL1"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# Helpers\n",
        "# -----------------------\n",
        "def _read_triples(path: Path) -> List[Tuple[str, str, str]]:\n",
        "    triples = []\n",
        "    with open(path, \"r\", newline=\"\") as f:\n",
        "        reader = csv.reader(f, delimiter=\"\\t\")\n",
        "        for row in reader:\n",
        "            if not row:\n",
        "                continue\n",
        "            h, r, t = row\n",
        "            triples.append((h, r, t))\n",
        "    return triples\n",
        "\n",
        "\n",
        "def _build_id_maps(\n",
        "        train_p: Path,\n",
        "        valid_p: Optional[Path] = None,\n",
        "        test_p: Optional[Path] = None,\n",
        ") -> Tuple[Dict[str, int], Dict[str, int]]:\n",
        "    \"\"\"Build ent2id and rel2id from all splits available.\"\"\"\n",
        "    ents, rels = set(), set()\n",
        "    for p in [train_p, valid_p, test_p]:\n",
        "        if p is None:\n",
        "            continue\n",
        "        for h, r, t in _read_triples(p):\n",
        "            ents.add(h); ents.add(t); rels.add(r)\n",
        "    ent2id = {e: i for i, e in enumerate(sorted(ents))}\n",
        "    rel2id = {r: i for i, r in enumerate(sorted(rels))}\n",
        "    return ent2id, rel2id\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Datasets\n",
        "# -----------------------\n",
        "class _PairsDataset(Dataset):\n",
        "    \"\"\"Untyped pairs (h, t), label=1 for each positive edge.\"\"\"\n",
        "    def __init__(self, pairs: torch.LongTensor):\n",
        "        # pairs: [N,2]\n",
        "        self.pairs = pairs\n",
        "        self.labels = torch.ones(pairs.size(0), dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.pairs.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.pairs[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "class _TriplesDataset(Dataset):\n",
        "    \"\"\"Typed triples (h, r, t), label=1 for each positive triple.\"\"\"\n",
        "    def __init__(self, triples: torch.LongTensor):\n",
        "        # triples: [N,3]\n",
        "        self.triples = triples\n",
        "        self.labels = torch.ones(triples.size(0), dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.triples.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.triples[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Collapsed edge types (untyped) datamodule\n",
        "# ============================================================\n",
        "class KGDataModuleCollapsed:\n",
        "    \"\"\"\n",
        "    Produces untyped (h, t) pairs from KG triples.\n",
        "\n",
        "    Public methods:\n",
        "      - train_loader()\n",
        "      - val_loader()\n",
        "      - test_loader()\n",
        "\n",
        "    Args:\n",
        "        train_path, valid_path, test_path: Path to split files (CORA format).\n",
        "        batch_size: int\n",
        "        shuffle: bool (applied to train loader only)\n",
        "        num_workers: int (DataLoader)\n",
        "        add_reverse: if True, also add (t, h) for every (h, t)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            train_path: Path,\n",
        "            valid_path: Optional[Path] = None,\n",
        "            test_path: Optional[Path] = None,\n",
        "            batch_size: int = 2048,\n",
        "            shuffle: bool = True,\n",
        "            num_workers: int = 0,\n",
        "            add_reverse: bool = True,\n",
        "    ):\n",
        "        self.train_path = Path(train_path)\n",
        "        self.valid_path = Path(valid_path) if valid_path else None\n",
        "        self.test_path  = Path(test_path)  if test_path  else None\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.num_workers = num_workers\n",
        "        self.add_reverse = add_reverse\n",
        "\n",
        "        # Build ids\n",
        "        self.ent2id, self.rel2id = _build_id_maps(self.train_path, self.valid_path, self.test_path)\n",
        "\n",
        "        # Build tensors per split\n",
        "        self._train_pairs = self._make_pairs(self.train_path)\n",
        "        self._valid_pairs = self._make_pairs(self.valid_path) if self.valid_path else None\n",
        "        self._test_pairs  = self._make_pairs(self.test_path)  if self.test_path  else None\n",
        "\n",
        "        # Datasets\n",
        "        self._train_ds = _PairsDataset(self._train_pairs)\n",
        "        self._valid_ds = _PairsDataset(self._valid_pairs) if self._valid_pairs is not None else None\n",
        "        self._test_ds  = _PairsDataset(self._test_pairs)  if self._test_pairs  is not None else None\n",
        "\n",
        "    def _make_pairs(self, path: Path) -> torch.LongTensor:\n",
        "        pairs = []\n",
        "        for h, _, t in _read_triples(path):\n",
        "            h_id, t_id = self.ent2id[h], self.ent2id[t]\n",
        "            pairs.append((h_id, t_id))\n",
        "            if self.add_reverse:\n",
        "                pairs.append((t_id, h_id))\n",
        "        if not pairs:\n",
        "            return torch.empty(0, 2, dtype=torch.long)\n",
        "        return torch.tensor(pairs, dtype=torch.long)\n",
        "\n",
        "    # -------- public loaders --------\n",
        "    def train_loader(self) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            self._train_ds, batch_size=self.batch_size, shuffle=self.shuffle,\n",
        "            num_workers=self.num_workers, pin_memory=False\n",
        "        )\n",
        "\n",
        "    def val_loader(self) -> Optional[DataLoader]:\n",
        "        if self._valid_ds is None:\n",
        "            return None\n",
        "        return DataLoader(\n",
        "            self._valid_ds, batch_size=self.batch_size, shuffle=False,\n",
        "            num_workers=self.num_workers, pin_memory=False\n",
        "        )\n",
        "\n",
        "    def test_loader(self) -> Optional[DataLoader]:\n",
        "        if self._test_ds is None:\n",
        "            return None\n",
        "        return DataLoader(\n",
        "            self._test_ds, batch_size=self.batch_size, shuffle=False,\n",
        "            num_workers=self.num_workers, pin_memory=False\n",
        "        )\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Typed (with edge types) datamodule\n",
        "# ============================================================\n",
        "class KGDataModuleTyped:\n",
        "    \"\"\"\n",
        "    Produces typed (h, r, t) triples from KG files.\n",
        "\n",
        "    Public methods:\n",
        "      - train_loader()\n",
        "      - val_loader()\n",
        "      - test_loader()\n",
        "\n",
        "    Args:\n",
        "        train_path, valid_path, test_path: Path to split files.\n",
        "        batch_size, shuffle, num_workers: DataLoader args.\n",
        "        add_reverse: If True, add reverse links.\n",
        "        reverse_relation_strategy:\n",
        "            'duplicate_rel' -> create an inverse relation id per r (e.g., r#inv)\n",
        "            'same_rel'      -> reuse the same r id for reverse triple\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            train_path: Path,\n",
        "            valid_path: Optional[Path] = None,\n",
        "            test_path: Optional[Path] = None,\n",
        "            batch_size: int = 2048,\n",
        "            shuffle: bool = True,\n",
        "            num_workers: int = 0,\n",
        "            add_reverse: bool = True,\n",
        "            reverse_relation_strategy: str = \"duplicate_rel\",\n",
        "    ):\n",
        "        assert reverse_relation_strategy in (\"duplicate_rel\", \"same_rel\")\n",
        "        self.train_path = Path(train_path)\n",
        "        self.valid_path = Path(valid_path) if valid_path else None\n",
        "        self.test_path  = Path(test_path)  if test_path  else None\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.num_workers = num_workers\n",
        "        self.add_reverse = add_reverse\n",
        "        self.reverse_relation_strategy = reverse_relation_strategy\n",
        "\n",
        "        # Base id maps from original relations\n",
        "        self.ent2id, base_rel2id = _build_id_maps(self.train_path, self.valid_path, self.test_path)\n",
        "\n",
        "        # Possibly extend rel2id with inverse relations\n",
        "        if add_reverse and reverse_relation_strategy == \"duplicate_rel\":\n",
        "            # Make space for inverse ids\n",
        "            self.rel2id = dict(base_rel2id)\n",
        "            self.inv_of = {}  # map original rel -> inverse rel id\n",
        "            next_id = max(self.rel2id.values(), default=-1) + 1\n",
        "            # Ensure deterministic order\n",
        "            for r in sorted(base_rel2id.keys()):\n",
        "                inv_name = r + \"_rev\"\n",
        "                if inv_name not in self.rel2id:\n",
        "                    self.rel2id[inv_name] = next_id\n",
        "                    self.inv_of[r] = next_id\n",
        "                    next_id += 1\n",
        "        else:\n",
        "            self.rel2id = base_rel2id\n",
        "            self.inv_of = None  # not used\n",
        "\n",
        "        # Build tensors per split\n",
        "        self._train_triples = self._make_triples(self.train_path)\n",
        "        self._valid_triples = self._make_triples(self.valid_path) if self.valid_path else None\n",
        "        self._test_triples  = self._make_triples(self.test_path)  if self.test_path  else None\n",
        "\n",
        "        # Datasets\n",
        "        self._train_ds = _TriplesDataset(self._train_triples)\n",
        "        self._valid_ds = _TriplesDataset(self._valid_triples) if self._valid_triples is not None else None\n",
        "        self._test_ds  = _TriplesDataset(self._test_triples)  if self._test_triples  is not None else None\n",
        "\n",
        "    def _make_triples(self, path: Path) -> torch.LongTensor:\n",
        "        if path is None:\n",
        "            return torch.empty(0, 3, dtype=torch.long)\n",
        "\n",
        "        rows = []\n",
        "        for h, r, t in _read_triples(path):\n",
        "            h_id, t_id = self.ent2id[h], self.ent2id[t]\n",
        "            r_id = self.rel2id[r]  # original direction\n",
        "            rows.append((h_id, r_id, t_id))\n",
        "\n",
        "            if self.add_reverse:\n",
        "                if self.reverse_relation_strategy == \"duplicate_rel\":\n",
        "                    r_inv_id = self.inv_of[r]  # created in __init__\n",
        "                    rows.append((t_id, r_inv_id, h_id))\n",
        "                else:  # same_rel\n",
        "                    rows.append((t_id, r_id, h_id))\n",
        "\n",
        "        if not rows:\n",
        "            return torch.empty(0, 3, dtype=torch.long)\n",
        "        return torch.tensor(rows, dtype=torch.long)\n",
        "\n",
        "    # -------- public loaders --------\n",
        "    def train_loader(self) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            self._train_ds, batch_size=self.batch_size, shuffle=self.shuffle,\n",
        "            num_workers=self.num_workers, pin_memory=False\n",
        "        )\n",
        "\n",
        "    def val_loader(self) -> Optional[DataLoader]:\n",
        "        if self._valid_ds is None:\n",
        "            return None\n",
        "        return DataLoader(\n",
        "            self._valid_ds, batch_size=self.batch_size, shuffle=False,\n",
        "            num_workers=self.num_workers, pin_memory=False\n",
        "        )\n",
        "\n",
        "    def test_loader(self) -> Optional[DataLoader]:\n",
        "        if self._test_ds is None:\n",
        "            return None\n",
        "        return DataLoader(\n",
        "            self._test_ds, batch_size=self.batch_size, shuffle=False,\n",
        "            num_workers=self.num_workers, pin_memory=False\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8dbc9974",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dbc9974",
        "outputId": "3b984ef5-d692-4554-ebf4-9c03947b273f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset: CORA\n",
            "Entities: 2,708\n",
            "Original Relations: 2\n",
            "Relations (with inverse): 2\n",
            "Training pairs: 8,448\n",
            "Training triples (typed): 8,448\n",
            "Graph edges (with self-loops): 11,156\n",
            "Graph edge_index shape: (2, 11156)\n",
            "Data loaders created:\n",
            "Train batches (collapsed): 3\n",
            "Val batches (collapsed): 1\n",
            "Test batches (collapsed): 1\n"
          ]
        }
      ],
      "source": [
        "# Dataset loading using dataset_loader.py\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your dataset\n",
        "base_path = Path(\"/content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/CORA\")\n",
        "\n",
        "# Dataset paths\n",
        "train_path = base_path / \"train.txt\"\n",
        "valid_path = base_path / \"valid.txt\"\n",
        "test_path = base_path / \"test.txt\"\n",
        "\n",
        "# Initialize data modules\n",
        "# Collapsed mode for LightGCN (untyped pairs)\n",
        "dm_collapsed = KGDataModuleCollapsed(\n",
        "    train_path=train_path,\n",
        "    valid_path=valid_path,\n",
        "    test_path=test_path,\n",
        "    batch_size=4096,\n",
        "    shuffle=True,\n",
        "    add_reverse=False\n",
        ")\n",
        "\n",
        "# Typed mode for R-LightGCN (typed triples)\n",
        "dm_typed = KGDataModuleTyped(\n",
        "    train_path=train_path,\n",
        "    valid_path=valid_path,\n",
        "    test_path=test_path,\n",
        "    batch_size=4096,\n",
        "    shuffle=True,\n",
        "    add_reverse=False,\n",
        "    reverse_relation_strategy=\"duplicate_rel\"\n",
        ")\n",
        "\n",
        "num_entities = len(dm_collapsed.ent2id)\n",
        "num_relations = len(dm_collapsed.rel2id)  # Original relations\n",
        "num_relations_with_inv = len(dm_typed.rel2id)  # With inverse relations\n",
        "\n",
        "print(f\"Dataset: CORA\")\n",
        "print(f\"Entities: {num_entities:,}\")\n",
        "print(f\"Original Relations: {num_relations:,}\")\n",
        "print(f\"Relations (with inverse): {num_relations_with_inv:,}\")\n",
        "print(f\"Training pairs: {len(dm_collapsed._train_pairs):,}\")\n",
        "print(f\"Training triples (typed): {len(dm_typed._train_triples):,}\")\n",
        "\n",
        "# Build edge_index for LightGCN (collapsed pairs)\n",
        "edge_index = dm_collapsed._train_pairs.t().contiguous().to(device)\n",
        "edge_index, _ = add_self_loops(edge_index, num_nodes=num_entities)\n",
        "\n",
        "print(f\"Graph edges (with self-loops): {edge_index.shape[1]:,}\")\n",
        "print(f\"Graph edge_index shape: {tuple(edge_index.shape)}\")\n",
        "\n",
        "train_loader_collapsed = dm_collapsed.train_loader()\n",
        "val_loader_collapsed = dm_collapsed.val_loader()\n",
        "test_loader_collapsed = dm_collapsed.test_loader()\n",
        "\n",
        "train_loader_typed = dm_typed.train_loader()\n",
        "val_loader_typed = dm_typed.val_loader()\n",
        "test_loader_typed = dm_typed.test_loader()\n",
        "\n",
        "print(f\"Data loaders created:\")\n",
        "print(f\"Train batches (collapsed): {len(train_loader_collapsed)}\")\n",
        "print(f\"Val batches (collapsed): {len(val_loader_collapsed) if val_loader_collapsed else 0}\")\n",
        "print(f\"Test batches (collapsed): {len(test_loader_collapsed) if test_loader_collapsed else 0}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3b71c39b",
      "metadata": {
        "id": "3b71c39b"
      },
      "outputs": [],
      "source": [
        "# Model Saving Utility Functions\n",
        "def save_model_checkpoint(model, optimizer, hyperparameters, final_test_metrics,\n",
        "                         training_history, model_name, filename):\n",
        "    \"\"\"\n",
        "    Save model checkpoint with comprehensive information.\n",
        "\n",
        "    Args:\n",
        "        model: The trained model\n",
        "        optimizer: The optimizer used\n",
        "        hyperparameters: Dict with training hyperparameters\n",
        "        final_test_metrics: Final test evaluation results\n",
        "        training_history: Training metrics history\n",
        "        model_name: Name of the model for display\n",
        "        filename: Output filename for the checkpoint\n",
        "    \"\"\"\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'hyperparameters': hyperparameters,\n",
        "        'final_test_metrics': final_test_metrics,\n",
        "        'training_history': training_history,\n",
        "        'model_name': model_name\n",
        "    }\n",
        "\n",
        "    torch.save(checkpoint, filename)\n",
        "    print(f\"{model_name} model checkpoint saved to {filename}\")\n",
        "\n",
        "def load_model_checkpoint(filename, model_class, device, **model_kwargs):\n",
        "    \"\"\"\n",
        "    Load model checkpoint and restore model state.\n",
        "\n",
        "    Args:\n",
        "        filename: Path to checkpoint file\n",
        "        model_class: Model class to instantiate\n",
        "        device: Device to load model on\n",
        "        **model_kwargs: Arguments for model instantiation\n",
        "\n",
        "    Returns:\n",
        "        model: Loaded model\n",
        "        checkpoint: Full checkpoint data\n",
        "    \"\"\"\n",
        "    checkpoint = torch.load(filename, map_location=device)\n",
        "\n",
        "    # Create model instance\n",
        "    model = model_class(**model_kwargs).to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    print(f\"Model loaded from {filename}\")\n",
        "    print(f\"Model: {checkpoint.get('model_name', 'Unknown')}\")\n",
        "    print(f\"Hyperparameters: {checkpoint.get('hyperparameters', {})}\")\n",
        "\n",
        "    if 'final_test_metrics' in checkpoint:\n",
        "        metrics = checkpoint['final_test_metrics']\n",
        "        if 'head' in metrics and 'tail' in metrics:\n",
        "            auc_avg = (metrics['head']['auc'] + metrics['tail']['auc']) / 2\n",
        "            hits10_avg = (metrics['head']['hits@10'] + metrics['tail']['hits@10']) / 2\n",
        "            print(f\"Test AUC (avg): {auc_avg:.4f}\")\n",
        "            print(f\"Test Hits@10 (avg): {hits10_avg:.4f}\")\n",
        "\n",
        "    return model, checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ecde7083",
      "metadata": {
        "id": "ecde7083"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"\n",
        "    Stop training if validation metric does not improve for 'patience' epochs.\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=5, mode='max', min_delta=1e-4):\n",
        "        self.patience = patience\n",
        "        self.mode = mode\n",
        "        self.min_delta = min_delta\n",
        "        self.best_score = None\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, current_score):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = current_score\n",
        "            return False\n",
        "\n",
        "        if self.mode == 'max':\n",
        "            improvement = current_score - self.best_score\n",
        "        else:\n",
        "            improvement = self.best_score - current_score\n",
        "\n",
        "        if improvement > self.min_delta:\n",
        "            self.best_score = current_score\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            self.early_stop = True\n",
        "\n",
        "        return self.early_stop\n",
        "\n",
        "\n",
        "class MetricsTracker:\n",
        "    \"\"\"Simplified metrics tracker for training metrics without head/tail split\"\"\"\n",
        "    def __init__(self):\n",
        "        self.metrics = defaultdict(list)\n",
        "\n",
        "    def add(self, epoch, **kwargs):\n",
        "        \"\"\"Add metrics for a given epoch\"\"\"\n",
        "        self.metrics['epoch'].append(epoch)\n",
        "        for key, value in kwargs.items():\n",
        "            self.metrics[key].append(value)\n",
        "\n",
        "    def get_best_epoch(self, metric='val_auc'):\n",
        "        \"\"\"Return the epoch with the best value for the given metric (higher is better)\"\"\"\n",
        "        if metric not in self.metrics or not self.metrics[metric]:\n",
        "            return 0\n",
        "        best_idx = np.argmax(self.metrics[metric])\n",
        "        return self.metrics['epoch'][best_idx]\n",
        "\n",
        "    def save_to_file(self, filepath):\n",
        "        \"\"\"Save metrics to a text file in tabular format\"\"\"\n",
        "        with open(filepath, 'w') as f:\n",
        "            f.write(\"Epoch\\tLoss\\tVal_AUC\\tHits@1\\tHits@5\\tHits@10\\n\")\n",
        "            for i in range(len(self.metrics['epoch'])):\n",
        "                epoch = self.metrics['epoch'][i]\n",
        "                loss = self.metrics['loss'][i]\n",
        "                val_auc = self.metrics['val_auc'][i]\n",
        "                hits1 = self.metrics['hits1'][i]\n",
        "                hits5 = self.metrics['hits5'][i]\n",
        "                hits10 = self.metrics['hits10'][i]\n",
        "                f.write(f\"{epoch}\\t{loss:.6f}\\t{val_auc:.6f}\\t{hits1:.6f}\\t{hits5:.6f}\\t{hits10:.6f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bbf04eb1",
      "metadata": {
        "id": "bbf04eb1"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Decoders\n",
        "# -----------------------------\n",
        "class DotProductDecoder(nn.Module):\n",
        "    def forward(self, z: torch.Tensor, pairs: torch.LongTensor) -> torch.Tensor:\n",
        "        return (z[pairs[:, 0]] * z[pairs[:, 1]]).sum(dim=1)\n",
        "\n",
        "class DistMultDecoder(nn.Module):\n",
        "    def __init__(self, num_relations: int, emb_dim: int):\n",
        "        super().__init__()\n",
        "        self.rel_emb = nn.Embedding(num_relations, emb_dim)\n",
        "        nn.init.xavier_uniform_(self.rel_emb.weight)\n",
        "\n",
        "    def forward(self, z: torch.Tensor, triples: torch.LongTensor) -> torch.Tensor:\n",
        "        h, r, t = triples[:, 0], triples[:, 1], triples[:, 2]\n",
        "        r_vec = self.rel_emb(r)\n",
        "        return (z[h] * r_vec * z[t]).sum(dim=1)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def score_heads(self, z: torch.Tensor, r: int, t: int, entity_indices: torch.Tensor) -> torch.Tensor:\n",
        "        r_vec = self.rel_emb.weight[r]\n",
        "        chunk = z[entity_indices]\n",
        "        return (chunk * r_vec * z[t]).sum(dim=1)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def score_tails(self, z: torch.Tensor, h: int, r: int, entity_indices: torch.Tensor) -> torch.Tensor:\n",
        "        r_vec = self.rel_emb.weight[r]\n",
        "        chunk = z[entity_indices]\n",
        "        return (z[h] * r_vec * chunk).sum(dim=1)\n",
        "\n",
        "# -----------------------------\n",
        "# R-LightGCNConv with variants\n",
        "# -----------------------------\n",
        "class RLightGCNConvVar(MessagePassing):\n",
        "    def __init__(self, num_relations: int, emb_dim: int, rel_emb: nn.Embedding,\n",
        "                 relation_transform: str = 'scalar', norm_type: str = 'global'):\n",
        "        super().__init__(aggr='add')\n",
        "        assert relation_transform in {'scalar','diagonal'}\n",
        "        assert norm_type in {'global','per_relation'}\n",
        "        self.rel_emb = rel_emb\n",
        "        self.emb_dim = emb_dim\n",
        "        self.relation_transform = relation_transform\n",
        "        self.norm_type = norm_type\n",
        "\n",
        "        if relation_transform == 'scalar':\n",
        "            self.rel_scale = nn.Embedding(num_relations, 1)\n",
        "            nn.init.ones_(self.rel_scale.weight)\n",
        "        else:\n",
        "            self.rel_scale = nn.Embedding(num_relations, emb_dim)\n",
        "            nn.init.ones_(self.rel_scale.weight)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_type: torch.LongTensor) -> torch.Tensor:\n",
        "        row, col = edge_index\n",
        "        if self.norm_type == 'global':\n",
        "            deg = torch.bincount(col, minlength=x.size(0)).float()\n",
        "            deg_inv_sqrt = deg.clamp(min=1).pow(-0.5)\n",
        "            norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "        else:  # per_relation\n",
        "            num_nodes = x.size(0)\n",
        "            num_rel = self.rel_emb.num_embeddings\n",
        "            keys_col = edge_type.long() * num_nodes + col.long()\n",
        "            keys_row = edge_type.long() * num_nodes + row.long()\n",
        "            ones = torch.ones_like(col, dtype=x.dtype)\n",
        "            dim_size = int(num_rel * num_nodes)\n",
        "            deg_col_per_key = scatter(ones, keys_col, dim=0, dim_size=dim_size)\n",
        "            deg_row_per_key = scatter(ones, keys_row, dim=0, dim_size=dim_size)\n",
        "            deg_col = deg_col_per_key[keys_col]\n",
        "            deg_row = deg_row_per_key[keys_row]\n",
        "            norm = deg_row.clamp(min=1).pow(-0.5) * deg_col.clamp(min=1).pow(-0.5)\n",
        "        return self.propagate(edge_index, x=x, edge_type=edge_type, norm=norm)\n",
        "\n",
        "    def message(self, x_j: torch.Tensor, edge_type: torch.LongTensor, norm: torch.Tensor) -> torch.Tensor:\n",
        "        if self.relation_transform == 'scalar':\n",
        "            out = x_j * self.rel_scale(edge_type)\n",
        "        else:\n",
        "            out = x_j * self.rel_scale(edge_type)\n",
        "        return norm.view(-1, 1) * out\n",
        "\n",
        "# -----------------------------\n",
        "# R-LightGCNVar model\n",
        "# -----------------------------\n",
        "class RLightGCNVar(nn.Module):\n",
        "    def __init__(self, num_nodes: int, num_relations: int, emb_dim: int = 64, num_layers: int = 3,\n",
        "                 relation_transform: str = 'scalar', norm_type: str = 'global', decoder: str = 'distmult'):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_nodes, emb_dim)\n",
        "        nn.init.xavier_uniform_(self.embedding.weight)\n",
        "        self.rel_emb = nn.Embedding(num_relations, emb_dim)\n",
        "        nn.init.xavier_uniform_(self.rel_emb.weight)\n",
        "        self.convs = nn.ModuleList([\n",
        "            RLightGCNConvVar(num_relations, emb_dim, self.rel_emb, relation_transform, norm_type)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.num_layers = num_layers\n",
        "        self.decoder_type = decoder.lower()\n",
        "        if self.decoder_type == 'distmult':\n",
        "            self.decoder = DistMultDecoder(num_relations, emb_dim)\n",
        "        elif self.decoder_type == 'dot':\n",
        "            self.decoder = DotProductDecoder()\n",
        "        else:\n",
        "            raise ValueError(\"decoder must be 'dot' or 'distmult'\")\n",
        "\n",
        "    def encode(self, edge_index: torch.Tensor, edge_type: torch.LongTensor = None) -> torch.Tensor:\n",
        "        x = self.embedding.weight\n",
        "        out = x\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index, edge_type)\n",
        "            out = out + x\n",
        "        return out / (self.num_layers + 1)\n",
        "\n",
        "    def score_triples(self, z: torch.Tensor, triples: torch.LongTensor) -> torch.Tensor:\n",
        "        if self.decoder_type == 'dot':\n",
        "            pairs = triples[:, [0, 2]]\n",
        "            return self.decoder(z, pairs)\n",
        "        else:\n",
        "            return self.decoder(z, triples)\n",
        "\n",
        "# -----------------------------\n",
        "# Training function\n",
        "# -----------------------------\n",
        "def train_one_epoch_r_lightgcn_var(model, data_loader, optimizer, rel_edge_index, edge_type, device,\n",
        "                                   num_entities, max_grad_norm=5.0):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    steps = 0\n",
        "    for batch in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        triples = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
        "        triples = triples.to(device)\n",
        "        if triples.dim() == 1: triples = triples.unsqueeze(0)\n",
        "\n",
        "        z = model.encode(rel_edge_index, edge_type)\n",
        "        z = z / (z.norm(dim=1, keepdim=True) + 1e-9)\n",
        "\n",
        "        pos_scores = model.score_triples(z, triples)\n",
        "\n",
        "        B = triples.size(0)\n",
        "        neg_heads = torch.randint(0, num_entities, (B,), device=device)\n",
        "        neg_tails = torch.randint(0, num_entities, (B,), device=device)\n",
        "        neg_h_triples = triples.clone(); neg_h_triples[:, 0] = neg_heads\n",
        "        neg_t_triples = triples.clone(); neg_t_triples[:, 2] = neg_tails\n",
        "        neg_scores_h = model.score_triples(z, neg_h_triples)\n",
        "        neg_scores_t = model.score_triples(z, neg_t_triples)\n",
        "\n",
        "        loss = F.softplus(-(pos_scores - neg_scores_h)).mean() + \\\n",
        "               F.softplus(-(pos_scores - neg_scores_t)).mean()\n",
        "        if not torch.isfinite(loss): continue\n",
        "        loss.backward()\n",
        "        if max_grad_norm is not None:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        steps += 1\n",
        "    return total_loss / max(1, steps)\n",
        "\n",
        "# -----------------------------\n",
        "# Evaluation\n",
        "# -----------------------------\n",
        "def evaluate_auc_hits(model, triples, num_entities, edge_index=None, edge_type=None,\n",
        "                      batch_size=4096, device=None):\n",
        "    \"\"\"\n",
        "    AUC + Hits@K evaluation (averaged, tail-corruption)\n",
        "    Works for dot or distmult decoders\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    if device is None:\n",
        "        device = next(model.parameters()).device\n",
        "\n",
        "    if triples.numel() == 0:\n",
        "        # Handle empty validation set\n",
        "        return {\"auc\": 0.5, \"hits@1\": 0.0, \"hits@5\": 0.0, \"hits@10\": 0.0}\n",
        "\n",
        "    def batch_iter(tensor, size):\n",
        "        for i in range(0, len(tensor), size):\n",
        "            yield tensor[i:i+size]\n",
        "\n",
        "    def sample_negatives(pos_batch):\n",
        "        neg_batch = pos_batch.clone()\n",
        "        neg_batch[:, -1] = torch.randint(0, num_entities, (pos_batch.size(0),), device=pos_batch.device)\n",
        "        return neg_batch\n",
        "\n",
        "    scores_all, labels_all = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        emb = model.encode(edge_index, edge_type)\n",
        "        emb = emb / (emb.norm(dim=1, keepdim=True) + 1e-9)\n",
        "\n",
        "        for pos in batch_iter(triples, batch_size):\n",
        "            if pos.size(0) == 0:\n",
        "                continue\n",
        "\n",
        "            pos = pos.to(device)\n",
        "            neg = sample_negatives(pos)\n",
        "\n",
        "            if isinstance(model.decoder, DistMultDecoder):\n",
        "                s_pos = model.decoder(emb, pos)\n",
        "                s_neg = model.decoder(emb, neg)\n",
        "            else:\n",
        "                s_pos = (emb[pos[:,0]] * emb[pos[:,-1]]).sum(dim=1)\n",
        "                s_neg = (emb[neg[:,0]] * emb[neg[:,-1]]).sum(dim=1)\n",
        "\n",
        "            scores_all.append(torch.cat([s_pos, s_neg], dim=0))\n",
        "            labels_all.append(torch.cat([torch.ones_like(s_pos), torch.zeros_like(s_neg)], dim=0))\n",
        "\n",
        "    # If no scores were collected, return defaults\n",
        "    if len(scores_all) == 0:\n",
        "        return {\"auc\": 0.5, \"hits@1\": 0.0, \"hits@5\": 0.0, \"hits@10\": 0.0}\n",
        "\n",
        "    scores_all = torch.cat(scores_all, dim=0)\n",
        "    labels_all = torch.cat(labels_all, dim=0)\n",
        "\n",
        "    # Compute ROC-AUC using torchmetrics (pure torch, avoids numpy)\n",
        "    try:\n",
        "        auroc_metric = BinaryAUROC().to(scores_all.device)\n",
        "        auc = auroc_metric(scores_all, labels_all.int()).item()\n",
        "    except Exception:\n",
        "        auc = 0.5\n",
        "\n",
        "    # Hits@K evaluation\n",
        "    hits_at = {1:0, 5:0, 10:0}\n",
        "    n_trials = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for pos in batch_iter(triples, batch_size):\n",
        "            if pos.size(0) == 0:\n",
        "                continue\n",
        "\n",
        "            pos = pos.to(device)\n",
        "            B = pos.size(0)\n",
        "            true_t = pos[:, -1]\n",
        "            rand_t = torch.randint(0, num_entities, (B, 99), device=device)\n",
        "            tails = torch.cat([true_t.unsqueeze(1), rand_t], dim=1)\n",
        "\n",
        "            e_h = emb[pos[:,0]]\n",
        "            e_candidates = emb[tails]\n",
        "            s = (e_h.unsqueeze(1) * e_candidates).sum(dim=2)\n",
        "            ranks = (s.argsort(dim=1, descending=True) == 0).nonzero()[:,1] + 1\n",
        "\n",
        "            for k in hits_at.keys():\n",
        "                hits_at[k] += (ranks <= k).sum().item()\n",
        "            n_trials += B\n",
        "\n",
        "    hits_at = {f\"hits@{k}\": v / n_trials for k, v in hits_at.items()}\n",
        "    return {\"auc\": float(auc), **hits_at}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e2db1138",
      "metadata": {
        "id": "e2db1138"
      },
      "outputs": [],
      "source": [
        "def run_r_lightgcn_ablation(\n",
        "    num_nodes, num_relations, train_loader, val_triples, test_triples,  # Added test_triples parameter\n",
        "    edge_index, edge_type, emb_dim=64, num_layers=3,\n",
        "    lr=0.001, epochs=100, batch_size=2048, patience=10,\n",
        "    save_dir=\"/content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA\",\n",
        "    ABLATION_CONFIGS=None, device=None\n",
        "):\n",
        "    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    all_metrics = {}\n",
        "\n",
        "    # --- Sanity checks ---\n",
        "    assert val_triples.max() < num_nodes, f\"Validation node index {val_triples.max()} >= num_nodes {num_nodes}\"\n",
        "    assert test_triples.max() < num_nodes, f\"Test node index {test_triples.max()} >= num_nodes {num_nodes}\"  # Added test check\n",
        "    assert edge_index.max() < num_nodes, f\"Edge index max {edge_index.max()} >= num_nodes {num_nodes}\"\n",
        "    assert edge_type.max() < num_relations, f\"Relation index max {edge_type.max()} >= num_relations {num_relations}\"\n",
        "\n",
        "    # --- Combined tracker for all models ---\n",
        "    combined_tracker = MetricsTracker()\n",
        "\n",
        "    for cfg in ABLATION_CONFIGS:\n",
        "        print(f\"\\n=== Training {cfg['name']} ===\")\n",
        "        model = RLightGCNVar(\n",
        "            num_nodes=num_nodes,\n",
        "            num_relations=num_relations,\n",
        "            emb_dim=emb_dim,\n",
        "            num_layers=num_layers,\n",
        "            relation_transform=cfg['relation_transform'],\n",
        "            norm_type=cfg['norm_type'],\n",
        "            decoder=cfg['decoder']\n",
        "        ).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "        best_val_auc = 0.0\n",
        "        epochs_no_improve = 0\n",
        "\n",
        "        # --- Initialize per-model tracker ---\n",
        "        tracker = MetricsTracker()\n",
        "\n",
        "        for epoch in tqdm(range(1, epochs+1), desc=f\"Training {cfg['name']}\"):\n",
        "            loss = train_one_epoch_r_lightgcn_var(\n",
        "                model, train_loader, optimizer,\n",
        "                edge_index, edge_type,\n",
        "                device, num_nodes\n",
        "            )\n",
        "\n",
        "            if epoch <= 3 or epoch % 5 == 0:\n",
        "                print(f\"Epoch {epoch:3d} | Loss: {loss:.4f}\")\n",
        "\n",
        "            # --- Evaluate ---\n",
        "            val_metrics = evaluate_auc_hits(\n",
        "                model, val_triples, num_nodes,\n",
        "                edge_index=edge_index, edge_type=edge_type,\n",
        "                batch_size=batch_size, device=device\n",
        "            )\n",
        "            val_auc = val_metrics[\"auc\"]\n",
        "            if val_auc > best_val_auc:\n",
        "                best_val_auc = val_auc\n",
        "                epochs_no_improve = 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            # --- Track metrics (simplified: no head/tail split) ---\n",
        "            tracker.add(\n",
        "                epoch,\n",
        "                loss=loss,\n",
        "                val_auc=val_metrics.get('auc', 0.0),\n",
        "                hits1=val_metrics.get('hits@1', 0.0),\n",
        "                hits5=val_metrics.get('hits@5', 0.0),\n",
        "                hits10=val_metrics.get('hits@10', 0.0)\n",
        "            )\n",
        "\n",
        "            # --- Add to combined tracker ---\n",
        "            combined_tracker.add(\n",
        "                epoch,\n",
        "                model_name=cfg['name'],\n",
        "                loss=loss,\n",
        "                val_auc=val_metrics.get('auc', 0.0),\n",
        "                hits1=val_metrics.get('hits@1', 0.0),\n",
        "                hits5=val_metrics.get('hits@5', 0.0),\n",
        "                hits10=val_metrics.get('hits@10', 0.0)\n",
        "            )\n",
        "\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "        print(f\"Validation metrics for {cfg['name']}: {val_metrics}\")\n",
        "\n",
        "        # --- FINAL TEST EVALUATION (Added this!) ---\n",
        "        print(f\"Running final test evaluation for {cfg['name']}...\")\n",
        "        test_metrics = evaluate_auc_hits(\n",
        "            model, test_triples, num_nodes,\n",
        "            edge_index=edge_index, edge_type=edge_type,\n",
        "            batch_size=batch_size, device=device\n",
        "        )\n",
        "        print(f\"Test metrics for {cfg['name']}: {test_metrics}\")\n",
        "\n",
        "        # --- Save final model to drive ---\n",
        "        model_file = os.path.join(save_dir, f\"{cfg['name']}.pt\")\n",
        "        torch.save(model.state_dict(), model_file)\n",
        "        print(f\"Final model saved: {model_file}\")\n",
        "\n",
        "        # --- Save per-model metrics table (updated to include test metrics) ---\n",
        "        metrics_file = os.path.join(save_dir, f\"{cfg['name']}_metrics.txt\")\n",
        "        with open(metrics_file, \"w\") as f:\n",
        "            f.write(\"Epoch\\tLoss\\tVal_AUC\\tHits@1\\tHits@5\\tHits@10\\n\")\n",
        "            for i in range(len(tracker.metrics['epoch'])):\n",
        "                f.write(f\"{tracker.metrics['epoch'][i]}\\t{tracker.metrics['loss'][i]:.6f}\\t\"\n",
        "                        f\"{tracker.metrics['val_auc'][i]:.6f}\\t{tracker.metrics['hits1'][i]:.6f}\\t\"\n",
        "                        f\"{tracker.metrics['hits5'][i]:.6f}\\t{tracker.metrics['hits10'][i]:.6f}\\n\")\n",
        "        print(f\"Per-model metrics saved to {metrics_file}\")\n",
        "\n",
        "        # --- Save test results separately ---\n",
        "        test_results_file = os.path.join(save_dir, f\"{cfg['name']}_test_results.txt\")\n",
        "        with open(test_results_file, \"w\") as f:\n",
        "            f.write(\"Model\\tTest_AUC\\tTest_Hits@1\\tTest_Hits@5\\tTest_Hits@10\\n\")\n",
        "            f.write(f\"{cfg['name']}\\t{test_metrics['auc']:.6f}\\t{test_metrics['hits@1']:.6f}\\t\"\n",
        "                    f\"{test_metrics['hits@5']:.6f}\\t{test_metrics['hits@10']:.6f}\\n\")\n",
        "        print(f\"Test results saved to {test_results_file}\")\n",
        "\n",
        "        # Store both validation and test metrics\n",
        "        all_metrics[cfg['name']] = {\n",
        "            'validation': val_metrics,\n",
        "            'test': test_metrics\n",
        "        }\n",
        "\n",
        "    # --- Save combined table for all models ---\n",
        "    combined_file = os.path.join(save_dir, \"all_models_metrics.txt\")\n",
        "    with open(combined_file, \"w\") as f:\n",
        "        f.write(\"Model\\tEpoch\\tLoss\\tVal_AUC\\tHits@1\\tHits@5\\tHits@10\\n\")\n",
        "        for i in range(len(combined_tracker.metrics['epoch'])):\n",
        "            f.write(f\"{combined_tracker.metrics.get('model_name', [''])[i]}\\t\"\n",
        "                    f\"{combined_tracker.metrics['epoch'][i]}\\t\"\n",
        "                    f\"{combined_tracker.metrics['loss'][i]:.6f}\\t\"\n",
        "                    f\"{combined_tracker.metrics['val_auc'][i]:.6f}\\t\"\n",
        "                    f\"{combined_tracker.metrics['hits1'][i]:.6f}\\t\"\n",
        "                    f\"{combined_tracker.metrics['hits5'][i]:.6f}\\t\"\n",
        "                    f\"{combined_tracker.metrics['hits10'][i]:.6f}\\n\")\n",
        "    print(f\"Combined metrics table saved to {combined_file}\")\n",
        "\n",
        "    # --- Save combined test results summary ---\n",
        "    test_summary_file = os.path.join(save_dir, \"all_models_test_summary.txt\")\n",
        "    with open(test_summary_file, \"w\") as f:\n",
        "        f.write(\"Model\\tVal_AUC\\tVal_Hits@10\\tTest_AUC\\tTest_Hits@10\\tImprovement\\n\")\n",
        "        for model_name, metrics in all_metrics.items():\n",
        "            val_auc = metrics['validation']['auc']\n",
        "            val_hits10 = metrics['validation']['hits@10']\n",
        "            test_auc = metrics['test']['auc']\n",
        "            test_hits10 = metrics['test']['hits@10']\n",
        "            improvement = \"✓\" if test_hits10 >= val_hits10 else \"✗\"\n",
        "            f.write(f\"{model_name}\\t{val_auc:.6f}\\t{val_hits10:.6f}\\t{test_auc:.6f}\\t{test_hits10:.6f}\\t{improvement}\\n\")\n",
        "    print(f\"Test summary saved to {test_summary_file}\")\n",
        "\n",
        "    print(f\"\\n=== FINAL TEST RESULTS SUMMARY ===\")\n",
        "    for model_name, metrics in all_metrics.items():\n",
        "        print(f\"{model_name}:\")\n",
        "        print(f\"  Test AUC: {metrics['test']['auc']:.4f}\")\n",
        "        print(f\"  Test Hits@1: {metrics['test']['hits@1']:.4f}\")\n",
        "        print(f\"  Test Hits@5: {metrics['test']['hits@5']:.4f}\")\n",
        "        print(f\"  Test Hits@10: {metrics['test']['hits@10']:.4f}\")\n",
        "\n",
        "    print(f\"\\nAll ablation experiments completed. Metrics saved in {save_dir}\")\n",
        "    return all_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b-YZKmLrLFcK",
      "metadata": {
        "id": "b-YZKmLrLFcK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14728f2e-c010-42b5-8eaa-58fb7882a67f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation triples: 1,054\n",
            "Test triples: 1,054\n",
            "\n",
            "=== Training R-LightGCN-Basic-Global-Dot ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Basic-Global-Dot:   3%|▎         | 3/100 [00:00<00:14,  6.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1 | Loss: 0.9758\n",
            "Epoch   2 | Loss: 0.9088\n",
            "Epoch   3 | Loss: 0.8593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Basic-Global-Dot:   7%|▋         | 7/100 [00:00<00:08, 11.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   5 | Loss: 0.8020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Basic-Global-Dot:  11%|█         | 11/100 [00:01<00:06, 13.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  10 | Loss: 0.7400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Basic-Global-Dot:  15%|█▌        | 15/100 [00:01<00:05, 14.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  15 | Loss: 0.7158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Basic-Global-Dot:  21%|██        | 21/100 [00:01<00:06, 12.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  20 | Loss: 0.7011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Basic-Global-Dot:  27%|██▋       | 27/100 [00:02<00:05, 14.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  25 | Loss: 0.6918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Basic-Global-Dot:  31%|███       | 31/100 [00:02<00:04, 14.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  30 | Loss: 0.6866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Basic-Global-Dot:  35%|███▌      | 35/100 [00:02<00:04, 14.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  35 | Loss: 0.6834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Basic-Global-Dot:  41%|████      | 41/100 [00:03<00:04, 12.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  40 | Loss: 0.6822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Basic-Global-Dot:  47%|████▋     | 47/100 [00:03<00:04, 12.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  45 | Loss: 0.6770\n",
            "Early stopping triggered at epoch 48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation metrics for R-LightGCN-Basic-Global-Dot: {'auc': 0.8765442371368408, 'hits@1': 0.3425047438330171, 'hits@5': 0.6176470588235294, 'hits@10': 0.715370018975332}\n",
            "Running final test evaluation for R-LightGCN-Basic-Global-Dot...\n",
            "Test metrics for R-LightGCN-Basic-Global-Dot: {'auc': 0.8609588146209717, 'hits@1': 0.3415559772296015, 'hits@5': 0.6233396584440227, 'hits@10': 0.713472485768501}\n",
            "Final model saved: /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Basic-Global-Dot.pt\n",
            "Per-model metrics saved to /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Basic-Global-Dot_metrics.txt\n",
            "Test results saved to /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Basic-Global-Dot_test_results.txt\n",
            "\n",
            "=== Training R-LightGCN-Diagonal-Global-Dot ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-Global-Dot:   2%|▏         | 2/100 [00:00<00:06, 14.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1 | Loss: 0.9781\n",
            "Epoch   2 | Loss: 0.9101\n",
            "Epoch   3 | Loss: 0.8617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining R-LightGCN-Diagonal-Global-Dot:   4%|▍         | 4/100 [00:00<00:06, 14.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   5 | Loss: 0.8010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-Global-Dot:  10%|█         | 10/100 [00:00<00:05, 15.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  10 | Loss: 0.7398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-Global-Dot:  18%|█▊        | 18/100 [00:01<00:06, 12.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  15 | Loss: 0.7153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-Global-Dot:  22%|██▏       | 22/100 [00:01<00:05, 14.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  20 | Loss: 0.7030\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-Global-Dot:  28%|██▊       | 28/100 [00:02<00:04, 14.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  25 | Loss: 0.6911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining R-LightGCN-Diagonal-Global-Dot:  30%|███       | 30/100 [00:02<00:04, 14.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  30 | Loss: 0.6872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-Global-Dot:  38%|███▊      | 38/100 [00:02<00:04, 13.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  35 | Loss: 0.6831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-Global-Dot:  42%|████▏     | 42/100 [00:03<00:04, 14.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  40 | Loss: 0.6812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-Global-Dot:  48%|████▊     | 48/100 [00:03<00:03, 14.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  45 | Loss: 0.6764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-Global-Dot:  52%|█████▏    | 52/100 [00:03<00:03, 14.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  50 | Loss: 0.6748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-Global-Dot:  55%|█████▌    | 55/100 [00:04<00:03, 12.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  55 | Loss: 0.6743\n",
            "Early stopping triggered at epoch 56\n",
            "Validation metrics for R-LightGCN-Diagonal-Global-Dot: {'auc': 0.8800241947174072, 'hits@1': 0.33111954459203036, 'hits@5': 0.6195445920303605, 'hits@10': 0.7096774193548387}\n",
            "Running final test evaluation for R-LightGCN-Diagonal-Global-Dot...\n",
            "Test metrics for R-LightGCN-Diagonal-Global-Dot: {'auc': 0.8699154853820801, 'hits@1': 0.34345351043643263, 'hits@5': 0.6110056925996205, 'hits@10': 0.698292220113852}\n",
            "Final model saved: /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Diagonal-Global-Dot.pt\n",
            "Per-model metrics saved to /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Diagonal-Global-Dot_metrics.txt\n",
            "Test results saved to /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Diagonal-Global-Dot_test_results.txt\n",
            "\n",
            "=== Training R-LightGCN-Scalar-Global-DistMult ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining R-LightGCN-Scalar-Global-DistMult:   0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1 | Loss: 1.3816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining R-LightGCN-Scalar-Global-DistMult:   2%|▏         | 2/100 [00:00<00:06, 14.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   2 | Loss: 1.3585\n",
            "Epoch   3 | Loss: 1.3387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Scalar-Global-DistMult:   6%|▌         | 6/100 [00:00<00:06, 14.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   5 | Loss: 1.3051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Scalar-Global-DistMult:  12%|█▏        | 12/100 [00:00<00:06, 14.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  10 | Loss: 1.2335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Scalar-Global-DistMult:  16%|█▌        | 16/100 [00:01<00:05, 14.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  15 | Loss: 1.1749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Scalar-Global-DistMult:  22%|██▏       | 22/100 [00:01<00:07, 11.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  20 | Loss: 1.1262\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Scalar-Global-DistMult:  26%|██▌       | 26/100 [00:02<00:06, 12.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  25 | Loss: 1.0870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Scalar-Global-DistMult:  29%|██▉       | 29/100 [00:02<00:05, 12.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  30 | Loss: 1.0488\n",
            "Early stopping triggered at epoch 30\n",
            "Validation metrics for R-LightGCN-Scalar-Global-DistMult: {'auc': 0.8037686944007874, 'hits@1': 0.2333965844402277, 'hits@5': 0.46774193548387094, 'hits@10': 0.5673624288425048}\n",
            "Running final test evaluation for R-LightGCN-Scalar-Global-DistMult...\n",
            "Test metrics for R-LightGCN-Scalar-Global-DistMult: {'auc': 0.8037866353988647, 'hits@1': 0.24193548387096775, 'hits@5': 0.4658444022770398, 'hits@10': 0.5673624288425048}\n",
            "Final model saved: /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Scalar-Global-DistMult.pt\n",
            "Per-model metrics saved to /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Scalar-Global-DistMult_metrics.txt\n",
            "Test results saved to /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Scalar-Global-DistMult_test_results.txt\n",
            "\n",
            "=== Training R-LightGCN-Diagonal-Global-DistMult ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-Global-DistMult:   2%|▏         | 2/100 [00:00<00:06, 14.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1 | Loss: 1.3897\n",
            "Epoch   2 | Loss: 1.3666\n",
            "Epoch   3 | Loss: 1.3469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining R-LightGCN-Diagonal-Global-DistMult:   4%|▍         | 4/100 [00:00<00:06, 14.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   5 | Loss: 1.3094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-Global-DistMult:  10%|█         | 10/100 [00:00<00:06, 14.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  10 | Loss: 1.2196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-Global-DistMult:  16%|█▌        | 16/100 [00:01<00:07, 11.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  15 | Loss: 1.1530\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-Global-DistMult:  22%|██▏       | 22/100 [00:01<00:05, 13.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  20 | Loss: 1.1097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-Global-DistMult:  26%|██▌       | 26/100 [00:01<00:05, 14.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  25 | Loss: 1.0683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-Global-DistMult:  28%|██▊       | 28/100 [00:02<00:05, 12.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping triggered at epoch 29\n",
            "Validation metrics for R-LightGCN-Diagonal-Global-DistMult: {'auc': 0.7786259651184082, 'hits@1': 0.23055028462998103, 'hits@5': 0.4449715370018975, 'hits@10': 0.5540796963946869}\n",
            "Running final test evaluation for R-LightGCN-Diagonal-Global-DistMult...\n",
            "Test metrics for R-LightGCN-Diagonal-Global-DistMult: {'auc': 0.7848005294799805, 'hits@1': 0.20967741935483872, 'hits@5': 0.4667931688804554, 'hits@10': 0.547438330170778}\n",
            "Final model saved: /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Diagonal-Global-DistMult.pt\n",
            "Per-model metrics saved to /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Diagonal-Global-DistMult_metrics.txt\n",
            "Test results saved to /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Diagonal-Global-DistMult_test_results.txt\n",
            "\n",
            "=== Training R-LightGCN-Scalar-PerRel-Dot ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Scalar-PerRel-Dot:   2%|▏         | 2/100 [00:00<00:06, 14.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1 | Loss: 0.8578\n",
            "Epoch   2 | Loss: 0.8068\n",
            "Epoch   3 | Loss: 0.7739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Scalar-PerRel-Dot:   6%|▌         | 6/100 [00:00<00:09, 10.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   5 | Loss: 0.7364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Scalar-PerRel-Dot:  12%|█▏        | 12/100 [00:01<00:06, 13.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  10 | Loss: 0.7038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Scalar-PerRel-Dot:  16%|█▌        | 16/100 [00:01<00:06, 13.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  15 | Loss: 0.6915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Scalar-PerRel-Dot:  22%|██▏       | 22/100 [00:01<00:05, 14.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  20 | Loss: 0.6790\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining R-LightGCN-Scalar-PerRel-Dot:  24%|██▍       | 24/100 [00:01<00:05, 14.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  25 | Loss: 0.6783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Scalar-PerRel-Dot:  32%|███▏      | 32/100 [00:02<00:05, 12.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  30 | Loss: 0.6759\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Scalar-PerRel-Dot:  38%|███▊      | 38/100 [00:02<00:04, 14.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  35 | Loss: 0.6740\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Scalar-PerRel-Dot:  42%|████▏     | 42/100 [00:03<00:03, 14.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  40 | Loss: 0.6734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining R-LightGCN-Scalar-PerRel-Dot:  42%|████▏     | 42/100 [00:03<00:04, 12.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping triggered at epoch 43\n",
            "Validation metrics for R-LightGCN-Scalar-PerRel-Dot: {'auc': 0.8791772127151489, 'hits@1': 0.32827324478178366, 'hits@5': 0.6280834914611005, 'hits@10': 0.7106261859582542}\n",
            "Running final test evaluation for R-LightGCN-Scalar-PerRel-Dot...\n",
            "Test metrics for R-LightGCN-Scalar-PerRel-Dot: {'auc': 0.8664345741271973, 'hits@1': 0.3368121442125237, 'hits@5': 0.6043643263757116, 'hits@10': 0.7039848197343453}\n",
            "Final model saved: /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Scalar-PerRel-Dot.pt\n",
            "Per-model metrics saved to /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Scalar-PerRel-Dot_metrics.txt\n",
            "Test results saved to /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Scalar-PerRel-Dot_test_results.txt\n",
            "\n",
            "=== Training R-LightGCN-Diagonal-PerRel-Dot ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-PerRel-Dot:   2%|▏         | 2/100 [00:00<00:06, 14.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1 | Loss: 0.8605\n",
            "Epoch   2 | Loss: 0.8056\n",
            "Epoch   3 | Loss: 0.7737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining R-LightGCN-Diagonal-PerRel-Dot:   4%|▍         | 4/100 [00:00<00:06, 14.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   5 | Loss: 0.7368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-PerRel-Dot:  12%|█▏        | 12/100 [00:00<00:06, 12.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  10 | Loss: 0.7015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-PerRel-Dot:  16%|█▌        | 16/100 [00:01<00:06, 13.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  15 | Loss: 0.6889\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-PerRel-Dot:  22%|██▏       | 22/100 [00:01<00:05, 14.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  20 | Loss: 0.6813\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining R-LightGCN-Diagonal-PerRel-Dot:  24%|██▍       | 24/100 [00:01<00:05, 14.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  25 | Loss: 0.6764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-PerRel-Dot:  31%|███       | 31/100 [00:02<00:05, 12.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  30 | Loss: 0.6784\n",
            "Early stopping triggered at epoch 32\n",
            "Validation metrics for R-LightGCN-Diagonal-PerRel-Dot: {'auc': 0.8770204186439514, 'hits@1': 0.34819734345351044, 'hits@5': 0.6157495256166983, 'hits@10': 0.715370018975332}\n",
            "Running final test evaluation for R-LightGCN-Diagonal-PerRel-Dot...\n",
            "Test metrics for R-LightGCN-Diagonal-PerRel-Dot: {'auc': 0.863928496837616, 'hits@1': 0.3377609108159393, 'hits@5': 0.6081593927893738, 'hits@10': 0.7011385199240987}\n",
            "Final model saved: /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Diagonal-PerRel-Dot.pt\n",
            "Per-model metrics saved to /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Diagonal-PerRel-Dot_metrics.txt\n",
            "Test results saved to /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Diagonal-PerRel-Dot_test_results.txt\n",
            "\n",
            "=== Training R-LightGCN-Scalar-PerRel-DistMult ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Scalar-PerRel-DistMult:   2%|▏         | 2/100 [00:00<00:06, 14.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1 | Loss: 1.3651\n",
            "Epoch   2 | Loss: 1.3356\n",
            "Epoch   3 | Loss: 1.3111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Scalar-PerRel-DistMult:   6%|▌         | 6/100 [00:00<00:06, 14.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   5 | Loss: 1.2752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Scalar-PerRel-DistMult:  12%|█▏        | 12/100 [00:00<00:06, 14.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  10 | Loss: 1.2060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Scalar-PerRel-DistMult:  16%|█▌        | 16/100 [00:01<00:05, 14.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  15 | Loss: 1.1451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Scalar-PerRel-DistMult:  20%|██        | 20/100 [00:01<00:06, 12.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  20 | Loss: 1.0902\n",
            "Early stopping triggered at epoch 21\n",
            "Validation metrics for R-LightGCN-Scalar-PerRel-DistMult: {'auc': 0.783104658126831, 'hits@1': 0.2144212523719165, 'hits@5': 0.4459203036053131, 'hits@10': 0.5455407969639469}\n",
            "Running final test evaluation for R-LightGCN-Scalar-PerRel-DistMult...\n",
            "Test metrics for R-LightGCN-Scalar-PerRel-DistMult: {'auc': 0.7552236318588257, 'hits@1': 0.2125237191650854, 'hits@5': 0.4478178368121442, 'hits@10': 0.5398481973434535}\n",
            "Final model saved: /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Scalar-PerRel-DistMult.pt\n",
            "Per-model metrics saved to /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Scalar-PerRel-DistMult_metrics.txt\n",
            "Test results saved to /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Scalar-PerRel-DistMult_test_results.txt\n",
            "\n",
            "=== Training R-LightGCN-Diagonal-PerRel-DistMult ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining R-LightGCN-Diagonal-PerRel-DistMult:   0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1 | Loss: 1.3803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining R-LightGCN-Diagonal-PerRel-DistMult:   2%|▏         | 2/100 [00:00<00:06, 14.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   2 | Loss: 1.3439\n",
            "Epoch   3 | Loss: 1.3136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining R-LightGCN-Diagonal-PerRel-DistMult:   4%|▍         | 4/100 [00:00<00:06, 14.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   5 | Loss: 1.2657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-PerRel-DistMult:  12%|█▏        | 12/100 [00:00<00:05, 14.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  10 | Loss: 1.1909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining R-LightGCN-Diagonal-PerRel-DistMult:  14%|█▍        | 14/100 [00:00<00:05, 14.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  15 | Loss: 1.1345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training R-LightGCN-Diagonal-PerRel-DistMult:  21%|██        | 21/100 [00:01<00:06, 12.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  20 | Loss: 1.0888\n",
            "Early stopping triggered at epoch 22\n",
            "Validation metrics for R-LightGCN-Diagonal-PerRel-DistMult: {'auc': 0.8152947425842285, 'hits@1': 0.22296015180265655, 'hits@5': 0.444022770398482, 'hits@10': 0.5540796963946869}\n",
            "Running final test evaluation for R-LightGCN-Diagonal-PerRel-DistMult...\n",
            "Test metrics for R-LightGCN-Diagonal-PerRel-DistMult: {'auc': 0.7963829040527344, 'hits@1': 0.21631878557874762, 'hits@5': 0.4573055028462998, 'hits@10': 0.5721062618595826}\n",
            "Final model saved: /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Diagonal-PerRel-DistMult.pt\n",
            "Per-model metrics saved to /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Diagonal-PerRel-DistMult_metrics.txt\n",
            "Test results saved to /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/R-LightGCN-Diagonal-PerRel-DistMult_test_results.txt\n",
            "Combined metrics table saved to /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/all_models_metrics.txt\n",
            "Test summary saved to /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA/all_models_test_summary.txt\n",
            "\n",
            "=== FINAL TEST RESULTS SUMMARY ===\n",
            "R-LightGCN-Basic-Global-Dot:\n",
            "  Test AUC: 0.8610\n",
            "  Test Hits@1: 0.3416\n",
            "  Test Hits@5: 0.6233\n",
            "  Test Hits@10: 0.7135\n",
            "R-LightGCN-Diagonal-Global-Dot:\n",
            "  Test AUC: 0.8699\n",
            "  Test Hits@1: 0.3435\n",
            "  Test Hits@5: 0.6110\n",
            "  Test Hits@10: 0.6983\n",
            "R-LightGCN-Scalar-Global-DistMult:\n",
            "  Test AUC: 0.8038\n",
            "  Test Hits@1: 0.2419\n",
            "  Test Hits@5: 0.4658\n",
            "  Test Hits@10: 0.5674\n",
            "R-LightGCN-Diagonal-Global-DistMult:\n",
            "  Test AUC: 0.7848\n",
            "  Test Hits@1: 0.2097\n",
            "  Test Hits@5: 0.4668\n",
            "  Test Hits@10: 0.5474\n",
            "R-LightGCN-Scalar-PerRel-Dot:\n",
            "  Test AUC: 0.8664\n",
            "  Test Hits@1: 0.3368\n",
            "  Test Hits@5: 0.6044\n",
            "  Test Hits@10: 0.7040\n",
            "R-LightGCN-Diagonal-PerRel-Dot:\n",
            "  Test AUC: 0.8639\n",
            "  Test Hits@1: 0.3378\n",
            "  Test Hits@5: 0.6082\n",
            "  Test Hits@10: 0.7011\n",
            "R-LightGCN-Scalar-PerRel-DistMult:\n",
            "  Test AUC: 0.7552\n",
            "  Test Hits@1: 0.2125\n",
            "  Test Hits@5: 0.4478\n",
            "  Test Hits@10: 0.5398\n",
            "R-LightGCN-Diagonal-PerRel-DistMult:\n",
            "  Test AUC: 0.7964\n",
            "  Test Hits@1: 0.2163\n",
            "  Test Hits@5: 0.4573\n",
            "  Test Hits@10: 0.5721\n",
            "\n",
            "All ablation experiments completed. Metrics saved in /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# Device\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -----------------------------\n",
        "#  Define number of nodes and relations\n",
        "# -----------------------------\n",
        "num_nodes = num_entities\n",
        "# num_relations = num_relations   # original number of relations\n",
        "# Use the number of relations including inverses from dm_typed for consistency\n",
        "num_relations_typed = num_relations_with_inv\n",
        "\n",
        "# -----------------------------\n",
        "# Self-loops\n",
        "# -----------------------------\n",
        "# Create self-loop edges\n",
        "self_loop_edges = torch.arange(num_nodes, device=device)\n",
        "self_loop_edges = torch.stack([self_loop_edges, self_loop_edges], dim=0)  # [2, num_nodes]\n",
        "\n",
        "# Assign a new relation ID for self-loops, outside the range of existing typed relations\n",
        "self_loop_rel_id = num_relations_typed  # the next available relation id\n",
        "self_loop_types = torch.full((num_nodes,), self_loop_rel_id, dtype=torch.long, device=device)\n",
        "\n",
        "# -----------------------------\n",
        "# Combine with training edges\n",
        "# -----------------------------\n",
        "# Use triples from dm_typed which already have relation IDs from 0 to num_relations_typed - 1\n",
        "rel_edge_index = dm_typed._train_triples[:, [0, 2]].t().contiguous().to(device)  # [2, N]\n",
        "edge_type = dm_typed._train_triples[:, 1].to(device)  # [N]\n",
        "\n",
        "rel_edge_index = torch.cat([rel_edge_index, self_loop_edges], dim=1)\n",
        "edge_type = torch.cat([edge_type, self_loop_types], dim=0)\n",
        "\n",
        "num_relations_final = num_relations_typed + 1  # include original, inverse, and self-loop relation\n",
        "\n",
        "# -----------------------------\n",
        "# Validation and Test triples\n",
        "# -----------------------------\n",
        "val_triples = torch.as_tensor(dm_typed._valid_triples, device=device)\n",
        "test_triples = torch.as_tensor(dm_typed._test_triples, device=device)  # Added test triples\n",
        "\n",
        "print(f\"Validation triples: {len(val_triples):,}\")\n",
        "print(f\"Test triples: {len(test_triples):,}\")\n",
        "\n",
        "# -----------------------------\n",
        "# DataLoader for training triples\n",
        "# -----------------------------\n",
        "# Wrap training triples in a TensorDataset and DataLoader\n",
        "train_triples_tensor = torch.as_tensor(dm_typed._train_triples, device=device)\n",
        "train_dataset = TensorDataset(train_triples_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Ablation configs\n",
        "# -----------------------------\n",
        "ABLATION_CONFIGS = [\n",
        "    {'name':'R-LightGCN-Basic-Global-Dot','relation_transform':'scalar','norm_type':'global','decoder':'dot'},\n",
        "    {'name':'R-LightGCN-Diagonal-Global-Dot','relation_transform':'diagonal','norm_type':'global','decoder':'dot'},\n",
        "    {'name':'R-LightGCN-Scalar-Global-DistMult','relation_transform':'scalar','norm_type':'global','decoder':'distmult'},\n",
        "    {'name':'R-LightGCN-Diagonal-Global-DistMult','relation_transform':'diagonal','norm_type':'global','decoder':'distmult'},\n",
        "    {'name':'R-LightGCN-Scalar-PerRel-Dot','relation_transform':'scalar','norm_type':'per_relation','decoder':'dot'},\n",
        "    {'name':'R-LightGCN-Diagonal-PerRel-Dot','relation_transform':'diagonal','norm_type':'per_relation','decoder':'dot'},\n",
        "    {'name':'R-LightGCN-Scalar-PerRel-DistMult','relation_transform':'scalar','norm_type':'per_relation','decoder':'distmult'},\n",
        "    {'name':'R-LightGCN-Diagonal-PerRel-DistMult','relation_transform':'diagonal','norm_type':'per_relation','decoder':'distmult'}\n",
        "]\n",
        "\n",
        "# -----------------------------\n",
        "# Run ablation\n",
        "# -----------------------------\n",
        "all_metrics = run_r_lightgcn_ablation(\n",
        "    num_nodes=num_nodes,\n",
        "    num_relations=num_relations_final, # Pass the final number of relations including self-loops\n",
        "    train_loader=train_loader,\n",
        "    val_triples=val_triples,\n",
        "    test_triples=test_triples,  # Added test_triples parameter\n",
        "    edge_index=rel_edge_index,\n",
        "    edge_type=edge_type,\n",
        "    emb_dim=64,\n",
        "    num_layers=3,\n",
        "    lr=0.001,\n",
        "    epochs=100,\n",
        "    batch_size=2048,\n",
        "    patience=10,\n",
        "    save_dir=\"/content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/CORA\",\n",
        "    ABLATION_CONFIGS=ABLATION_CONFIGS,\n",
        "    device=device\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
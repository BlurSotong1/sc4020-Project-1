{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "p_FimQyynYgw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4401,
     "status": "ok",
     "timestamp": 1760629846403,
     "user": {
      "displayName": "Sih Jia Qi",
      "userId": "08353846451037845175"
     },
     "user_tz": -480
    },
    "id": "p_FimQyynYgw",
    "outputId": "55b0a72d-3504-4156-eb02-9eddf6db09bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (1.8.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu126)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (0.15.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.10.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9dd09d",
   "metadata": {
    "id": "9f9dd09d"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7eaca170",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1760629846408,
     "user": {
      "displayName": "Sih Jia Qi",
      "userId": "08353846451037845175"
     },
     "user_tz": -480
    },
    "id": "7eaca170"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Standard library\n",
    "# -----------------------------\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "# -----------------------------\n",
    "# Third-party libraries\n",
    "# -----------------------------\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from google.colab import drive\n",
    "\n",
    "# -----------------------------\n",
    "# PyTorch and PyG libraries\n",
    "# -----------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch_geometric.utils import degree, add_self_loops\n",
    "from torch_geometric.nn import MessagePassing\n",
    "# from torch_scatter import scatter\n",
    "\n",
    "# import sys\n",
    "# from pathlib import Path\n",
    "# current_dir = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n",
    "# parent_dir = current_dir.parent\n",
    "# sys.path.append(str(parent_dir))\n",
    "# from utility.dataset_loader import KGDataModuleCollapsed, KGDataModuleTyped\n",
    "\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ukSMz-srql5c",
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1760629846463,
     "user": {
      "displayName": "Sih Jia Qi",
      "userId": "08353846451037845175"
     },
     "user_tz": -480
    },
    "id": "ukSMz-srql5c"
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def _read_triples(path: Path) -> List[Tuple[str, str, str]]:\n",
    "    triples = []\n",
    "    with open(path, \"r\", newline=\"\") as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            h, r, t = row\n",
    "            triples.append((h, r, t))\n",
    "    return triples\n",
    "\n",
    "\n",
    "def _build_id_maps(\n",
    "        train_p: Path,\n",
    "        valid_p: Optional[Path] = None,\n",
    "        test_p: Optional[Path] = None,\n",
    ") -> Tuple[Dict[str, int], Dict[str, int]]:\n",
    "    \"\"\"Build ent2id and rel2id from all splits available.\"\"\"\n",
    "    ents, rels = set(), set()\n",
    "    for p in [train_p, valid_p, test_p]:\n",
    "        if p is None:\n",
    "            continue\n",
    "        for h, r, t in _read_triples(p):\n",
    "            ents.add(h); ents.add(t); rels.add(r)\n",
    "    ent2id = {e: i for i, e in enumerate(sorted(ents))}\n",
    "    rel2id = {r: i for i, r in enumerate(sorted(rels))}\n",
    "    return ent2id, rel2id\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Datasets\n",
    "# -----------------------\n",
    "class _PairsDataset(Dataset):\n",
    "    \"\"\"Untyped pairs (h, t), label=1 for each positive edge.\"\"\"\n",
    "    def __init__(self, pairs: torch.LongTensor):\n",
    "        # pairs: [N,2]\n",
    "        self.pairs = pairs\n",
    "        self.labels = torch.ones(pairs.size(0), dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.pairs.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "class _TriplesDataset(Dataset):\n",
    "    \"\"\"Typed triples (h, r, t), label=1 for each positive triple.\"\"\"\n",
    "    def __init__(self, triples: torch.LongTensor):\n",
    "        # triples: [N,3]\n",
    "        self.triples = triples\n",
    "        self.labels = torch.ones(triples.size(0), dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.triples.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.triples[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Collapsed edge types (untyped) datamodule\n",
    "# ============================================================\n",
    "class KGDataModuleCollapsed:\n",
    "    \"\"\"\n",
    "    Produces untyped (h, t) pairs from KG triples.\n",
    "\n",
    "    Public methods:\n",
    "      - train_loader()\n",
    "      - val_loader()\n",
    "      - test_loader()\n",
    "\n",
    "    Args:\n",
    "        train_path, valid_path, test_path: Path to split files (FB15K format).\n",
    "        batch_size: int\n",
    "        shuffle: bool (applied to train loader only)\n",
    "        num_workers: int (DataLoader)\n",
    "        add_reverse: if True, also add (t, h) for every (h, t)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            train_path: Path,\n",
    "            valid_path: Optional[Path] = None,\n",
    "            test_path: Optional[Path] = None,\n",
    "            batch_size: int = 2048,\n",
    "            shuffle: bool = True,\n",
    "            num_workers: int = 0,\n",
    "            add_reverse: bool = True,\n",
    "    ):\n",
    "        self.train_path = Path(train_path)\n",
    "        self.valid_path = Path(valid_path) if valid_path else None\n",
    "        self.test_path  = Path(test_path)  if test_path  else None\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        self.add_reverse = add_reverse\n",
    "\n",
    "        # Build ids\n",
    "        self.ent2id, self.rel2id = _build_id_maps(self.train_path, self.valid_path, self.test_path)\n",
    "\n",
    "        # Build tensors per split\n",
    "        self._train_pairs = self._make_pairs(self.train_path)\n",
    "        self._valid_pairs = self._make_pairs(self.valid_path) if self.valid_path else None\n",
    "        self._test_pairs  = self._make_pairs(self.test_path)  if self.test_path  else None\n",
    "\n",
    "        # Datasets\n",
    "        self._train_ds = _PairsDataset(self._train_pairs)\n",
    "        self._valid_ds = _PairsDataset(self._valid_pairs) if self._valid_pairs is not None else None\n",
    "        self._test_ds  = _PairsDataset(self._test_pairs)  if self._test_pairs  is not None else None\n",
    "\n",
    "    def _make_pairs(self, path: Path) -> torch.LongTensor:\n",
    "        pairs = []\n",
    "        for h, _, t in _read_triples(path):\n",
    "            h_id, t_id = self.ent2id[h], self.ent2id[t]\n",
    "            pairs.append((h_id, t_id))\n",
    "            if self.add_reverse:\n",
    "                pairs.append((t_id, h_id))\n",
    "        if not pairs:\n",
    "            return torch.empty(0, 2, dtype=torch.long)\n",
    "        return torch.tensor(pairs, dtype=torch.long)\n",
    "\n",
    "    # -------- public loaders --------\n",
    "    def train_loader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self._train_ds, batch_size=self.batch_size, shuffle=self.shuffle,\n",
    "            num_workers=self.num_workers, pin_memory=False\n",
    "        )\n",
    "\n",
    "    def val_loader(self) -> Optional[DataLoader]:\n",
    "        if self._valid_ds is None:\n",
    "            return None\n",
    "        return DataLoader(\n",
    "            self._valid_ds, batch_size=self.batch_size, shuffle=False,\n",
    "            num_workers=self.num_workers, pin_memory=False\n",
    "        )\n",
    "\n",
    "    def test_loader(self) -> Optional[DataLoader]:\n",
    "        if self._test_ds is None:\n",
    "            return None\n",
    "        return DataLoader(\n",
    "            self._test_ds, batch_size=self.batch_size, shuffle=False,\n",
    "            num_workers=self.num_workers, pin_memory=False\n",
    "        )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Typed (with edge types) datamodule\n",
    "# ============================================================\n",
    "class KGDataModuleTyped:\n",
    "    \"\"\"\n",
    "    Produces typed (h, r, t) triples from KG files.\n",
    "\n",
    "    Public methods:\n",
    "      - train_loader()\n",
    "      - val_loader()\n",
    "      - test_loader()\n",
    "\n",
    "    Args:\n",
    "        train_path, valid_path, test_path: Path to split files.\n",
    "        batch_size, shuffle, num_workers: DataLoader args.\n",
    "        add_reverse: If True, add reverse links.\n",
    "        reverse_relation_strategy:\n",
    "            'duplicate_rel' -> create an inverse relation id per r (e.g., r#inv)\n",
    "            'same_rel'      -> reuse the same r id for reverse triple\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            train_path: Path,\n",
    "            valid_path: Optional[Path] = None,\n",
    "            test_path: Optional[Path] = None,\n",
    "            batch_size: int = 2048,\n",
    "            shuffle: bool = True,\n",
    "            num_workers: int = 0,\n",
    "            add_reverse: bool = True,\n",
    "            reverse_relation_strategy: str = \"duplicate_rel\",\n",
    "    ):\n",
    "        assert reverse_relation_strategy in (\"duplicate_rel\", \"same_rel\")\n",
    "        self.train_path = Path(train_path)\n",
    "        self.valid_path = Path(valid_path) if valid_path else None\n",
    "        self.test_path  = Path(test_path)  if test_path  else None\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        self.add_reverse = add_reverse\n",
    "        self.reverse_relation_strategy = reverse_relation_strategy\n",
    "\n",
    "        # Base id maps from original relations\n",
    "        self.ent2id, base_rel2id = _build_id_maps(self.train_path, self.valid_path, self.test_path)\n",
    "\n",
    "        # Possibly extend rel2id with inverse relations\n",
    "        if add_reverse and reverse_relation_strategy == \"duplicate_rel\":\n",
    "            # Make space for inverse ids\n",
    "            self.rel2id = dict(base_rel2id)\n",
    "            self.inv_of = {}  # map original rel -> inverse rel id\n",
    "            next_id = max(self.rel2id.values(), default=-1) + 1\n",
    "            # Ensure deterministic order\n",
    "            for r in sorted(base_rel2id.keys()):\n",
    "                inv_name = r + \"_rev\"\n",
    "                if inv_name not in self.rel2id:\n",
    "                    self.rel2id[inv_name] = next_id\n",
    "                    self.inv_of[r] = next_id\n",
    "                    next_id += 1\n",
    "        else:\n",
    "            self.rel2id = base_rel2id\n",
    "            self.inv_of = None  # not used\n",
    "\n",
    "        # Build tensors per split\n",
    "        self._train_triples = self._make_triples(self.train_path)\n",
    "        self._valid_triples = self._make_triples(self.valid_path) if self.valid_path else None\n",
    "        self._test_triples  = self._make_triples(self.test_path)  if self.test_path  else None\n",
    "\n",
    "        # Datasets\n",
    "        self._train_ds = _TriplesDataset(self._train_triples)\n",
    "        self._valid_ds = _TriplesDataset(self._valid_triples) if self._valid_triples is not None else None\n",
    "        self._test_ds  = _TriplesDataset(self._test_triples)  if self._test_triples  is not None else None\n",
    "\n",
    "    def _make_triples(self, path: Path) -> torch.LongTensor:\n",
    "        if path is None:\n",
    "            return torch.empty(0, 3, dtype=torch.long)\n",
    "\n",
    "        rows = []\n",
    "        for h, r, t in _read_triples(path):\n",
    "            h_id, t_id = self.ent2id[h], self.ent2id[t]\n",
    "            r_id = self.rel2id[r]  # original direction\n",
    "            rows.append((h_id, r_id, t_id))\n",
    "\n",
    "            if self.add_reverse:\n",
    "                if self.reverse_relation_strategy == \"duplicate_rel\":\n",
    "                    r_inv_id = self.inv_of[r]  # created in __init__\n",
    "                    rows.append((t_id, r_inv_id, h_id))\n",
    "                else:  # same_rel\n",
    "                    rows.append((t_id, r_id, h_id))\n",
    "\n",
    "        if not rows:\n",
    "            return torch.empty(0, 3, dtype=torch.long)\n",
    "        return torch.tensor(rows, dtype=torch.long)\n",
    "\n",
    "    # -------- public loaders --------\n",
    "    def train_loader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self._train_ds, batch_size=self.batch_size, shuffle=self.shuffle,\n",
    "            num_workers=self.num_workers, pin_memory=False\n",
    "        )\n",
    "\n",
    "    def val_loader(self) -> Optional[DataLoader]:\n",
    "        if self._valid_ds is None:\n",
    "            return None\n",
    "        return DataLoader(\n",
    "            self._valid_ds, batch_size=self.batch_size, shuffle=False,\n",
    "            num_workers=self.num_workers, pin_memory=False\n",
    "        )\n",
    "\n",
    "    def test_loader(self) -> Optional[DataLoader]:\n",
    "        if self._test_ds is None:\n",
    "            return None\n",
    "        return DataLoader(\n",
    "            self._test_ds, batch_size=self.batch_size, shuffle=False,\n",
    "            num_workers=self.num_workers, pin_memory=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1229a7677039018",
   "metadata": {
    "id": "1229a7677039018"
   },
   "source": [
    "# Dataset Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9927759",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3930,
     "status": "ok",
     "timestamp": 1760629850395,
     "user": {
      "displayName": "Sih Jia Qi",
      "userId": "08353846451037845175"
     },
     "user_tz": -480
    },
    "id": "a9927759",
    "outputId": "29a96e07-962b-4483-e9cb-32ea548bf663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Dataset: FB15K\n",
      "Entities: 14,541\n",
      "Original Relations: 237\n",
      "Relations (with inverse): 474\n",
      "Training pairs: 544,230\n",
      "Training triples (typed): 544,230\n",
      "Graph edges (with self-loops): 558,771\n",
      "Graph edge_index shape: (2, 558771)\n",
      "Data loaders created:\n",
      "Train batches (collapsed): 133\n",
      "Val batches (collapsed): 9\n",
      "Test batches (collapsed): 10\n"
     ]
    }
   ],
   "source": [
    "# Dataset loading using dataset_loader.py\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define the path to your dataset\n",
    "base_path = Path(\"/content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/FB15K-237.2\")\n",
    "\n",
    "# Dataset paths\n",
    "train_path = base_path / \"train.txt\"\n",
    "valid_path = base_path / \"valid.txt\"\n",
    "test_path = base_path / \"test.txt\"\n",
    "\n",
    "# Initialize data modules\n",
    "# Collapsed mode for LightGCN (untyped pairs)\n",
    "dm_collapsed = KGDataModuleCollapsed(\n",
    "    train_path=train_path,\n",
    "    valid_path=valid_path,\n",
    "    test_path=test_path,\n",
    "    batch_size=4096,\n",
    "    shuffle=True,\n",
    "    add_reverse=True\n",
    ")\n",
    "\n",
    "# Typed mode for R-LightGCN (typed triples)\n",
    "dm_typed = KGDataModuleTyped(\n",
    "    train_path=train_path,\n",
    "    valid_path=valid_path,\n",
    "    test_path=test_path,\n",
    "    batch_size=4096,\n",
    "    shuffle=True,\n",
    "    add_reverse=True,\n",
    "    reverse_relation_strategy=\"duplicate_rel\"\n",
    ")\n",
    "\n",
    "num_entities = len(dm_collapsed.ent2id)\n",
    "num_relations = len(dm_collapsed.rel2id)  # Original relations\n",
    "num_relations_with_inv = len(dm_typed.rel2id)  # With inverse relations\n",
    "\n",
    "print(f\"Dataset: FB15K\")\n",
    "print(f\"Entities: {num_entities:,}\")\n",
    "print(f\"Original Relations: {num_relations:,}\")\n",
    "print(f\"Relations (with inverse): {num_relations_with_inv:,}\")\n",
    "print(f\"Training pairs: {len(dm_collapsed._train_pairs):,}\")\n",
    "print(f\"Training triples (typed): {len(dm_typed._train_triples):,}\")\n",
    "\n",
    "# Build edge_index for LightGCN (collapsed pairs)\n",
    "edge_index = dm_collapsed._train_pairs.t().contiguous().to(device)\n",
    "edge_index, _ = add_self_loops(edge_index, num_nodes=num_entities)\n",
    "\n",
    "print(f\"Graph edges (with self-loops): {edge_index.shape[1]:,}\")\n",
    "print(f\"Graph edge_index shape: {tuple(edge_index.shape)}\")\n",
    "\n",
    "train_loader_collapsed = dm_collapsed.train_loader()\n",
    "val_loader_collapsed = dm_collapsed.val_loader()\n",
    "test_loader_collapsed = dm_collapsed.test_loader()\n",
    "\n",
    "train_loader_typed = dm_typed.train_loader()\n",
    "val_loader_typed = dm_typed.val_loader()\n",
    "test_loader_typed = dm_typed.test_loader()\n",
    "\n",
    "print(f\"Data loaders created:\")\n",
    "print(f\"Train batches (collapsed): {len(train_loader_collapsed)}\")\n",
    "print(f\"Val batches (collapsed): {len(val_loader_collapsed) if val_loader_collapsed else 0}\")\n",
    "print(f\"Test batches (collapsed): {len(test_loader_collapsed) if test_loader_collapsed else 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beecc39",
   "metadata": {
    "id": "0beecc39"
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a54bd072",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1760629850418,
     "user": {
      "displayName": "Sih Jia Qi",
      "userId": "08353846451037845175"
     },
     "user_tz": -480
    },
    "id": "a54bd072"
   },
   "outputs": [],
   "source": [
    "# Model Saving Utility Functions\n",
    "def save_model_checkpoint(model, optimizer, hyperparameters, final_test_metrics,\n",
    "                         training_history, model_name, filename):\n",
    "    \"\"\"\n",
    "    Save model checkpoint with comprehensive information.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model\n",
    "        optimizer: The optimizer used\n",
    "        hyperparameters: Dict with training hyperparameters\n",
    "        final_test_metrics: Final test evaluation results\n",
    "        training_history: Training metrics history\n",
    "        model_name: Name of the model for display\n",
    "        filename: Output filename for the checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'hyperparameters': hyperparameters,\n",
    "        'final_test_metrics': final_test_metrics,\n",
    "        'training_history': training_history,\n",
    "        'model_name': model_name\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"{model_name} model checkpoint saved to {filename}\")\n",
    "\n",
    "def load_model_checkpoint(filename, model_class, device, **model_kwargs):\n",
    "    \"\"\"\n",
    "    Load model checkpoint and restore model state.\n",
    "\n",
    "    Args:\n",
    "        filename: Path to checkpoint file\n",
    "        model_class: Model class to instantiate\n",
    "        device: Device to load model on\n",
    "        **model_kwargs: Arguments for model instantiation\n",
    "\n",
    "    Returns:\n",
    "        model: Loaded model\n",
    "        checkpoint: Full checkpoint data\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filename, map_location=device)\n",
    "\n",
    "    # Create model instance\n",
    "    model = model_class(**model_kwargs).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    print(f\"Model loaded from {filename}\")\n",
    "    print(f\"Model: {checkpoint.get('model_name', 'Unknown')}\")\n",
    "    print(f\"Hyperparameters: {checkpoint.get('hyperparameters', {})}\")\n",
    "\n",
    "    if 'final_test_metrics' in checkpoint:\n",
    "        metrics = checkpoint['final_test_metrics']\n",
    "        if 'head' in metrics and 'tail' in metrics:\n",
    "            auc_avg = (metrics['head']['auc'] + metrics['tail']['auc']) / 2\n",
    "            hits10_avg = (metrics['head']['hits@10'] + metrics['tail']['hits@10']) / 2\n",
    "            print(f\"Test AUC (avg): {auc_avg:.4f}\")\n",
    "            print(f\"Test Hits@10 (avg): {hits10_avg:.4f}\")\n",
    "\n",
    "    return model, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ca1b6bb",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1760629850425,
     "user": {
      "displayName": "Sih Jia Qi",
      "userId": "08353846451037845175"
     },
     "user_tz": -480
    },
    "id": "8ca1b6bb"
   },
   "outputs": [],
   "source": [
    "class MetricsTracker:\n",
    "    \"\"\"Simplified metrics tracker without plotting\"\"\"\n",
    "    def __init__(self):\n",
    "        self.metrics = defaultdict(list)\n",
    "\n",
    "    def add(self, epoch, **kwargs):\n",
    "        self.metrics['epoch'].append(epoch)\n",
    "        for key, value in kwargs.items():\n",
    "            self.metrics[key].append(value)\n",
    "\n",
    "    def get_best_epoch(self, metric='val_auc_head'):\n",
    "        \"\"\"Get the epoch with the best performance for a given metric\"\"\"\n",
    "        if metric not in self.metrics or not self.metrics[metric]:\n",
    "            return 0\n",
    "\n",
    "        # For AUC and Hits@K, higher is better\n",
    "        best_idx = np.argmax(self.metrics[metric])\n",
    "        return self.metrics['epoch'][best_idx]\n",
    "\n",
    "    def save_to_file(self, filepath):\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(\"Epoch\\tLoss\\tAUC\\tHits@1\\tHits@5\\tHits@10\\n\")\n",
    "            for i in range(len(self.metrics['epoch'])):\n",
    "                epoch = self.metrics['epoch'][i]\n",
    "                loss = self.metrics['loss'][i]\n",
    "                auc = self.metrics['val_auc'][i]\n",
    "                h1 = self.metrics['val_hits1'][i]\n",
    "                h5 = self.metrics['val_hits5'][i]\n",
    "                h10 = self.metrics['val_hits10'][i]\n",
    "\n",
    "                f.write(f\"{epoch}\\t{loss:.6f}\\t{auc:.6f}\\t{h1:.6f}\\t{h5:.6f}\\t{h10:.6f}\\n\")\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, metric='val_auc_head', mode='max', min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.metric = metric\n",
    "        self.mode = mode  # 'max' for AUC, Hits@K; 'min' for loss\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.should_stop = False\n",
    "\n",
    "    def __call__(self, metrics_tracker):\n",
    "        current_score = metrics_tracker.metrics[self.metric][-1] if self.metric in metrics_tracker.metrics else 0\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "        else:\n",
    "            if self.mode == 'max':\n",
    "                improved = current_score > self.best_score + self.min_delta\n",
    "            else:  # mode == 'min'\n",
    "                improved = current_score < self.best_score - self.min_delta\n",
    "\n",
    "            if improved:\n",
    "                self.best_score = current_score\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "\n",
    "        self.should_stop = self.counter >= self.patience\n",
    "        return self.should_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "315071c9",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1760629850432,
     "user": {
      "displayName": "Sih Jia Qi",
      "userId": "08353846451037845175"
     },
     "user_tz": -480
    },
    "id": "315071c9"
   },
   "outputs": [],
   "source": [
    "# Training functions with data loaders\n",
    "def train_one_epoch_lightgcn(model, data_loader, optimizer, edge_index, device, num_entities, max_grad_norm=5.0):\n",
    "    \"\"\"\n",
    "    Stable LightGCN training loop (BPR loss).\n",
    "    Handles NaNs, exploding gradients, and large score differences gracefully.\n",
    "\n",
    "    Args:\n",
    "        model: LightGCN model\n",
    "        data_loader: DataLoader yielding positive pairs (batch_size, 2)\n",
    "        optimizer: Optimizer instance\n",
    "        edge_index: Graph edges tensor on the correct device\n",
    "        device: torch.device to run on\n",
    "        num_entities: Total number of entities (int)\n",
    "        max_grad_norm: Optional gradient clipping value\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # ------------------------------\n",
    "        # Handle batch format\n",
    "        # ------------------------------\n",
    "        if isinstance(batch, (list, tuple)):\n",
    "            pairs_tensor = batch[0]\n",
    "        else:\n",
    "            pairs_tensor = batch\n",
    "\n",
    "        pairs_tensor = pairs_tensor.to(device)\n",
    "        if pairs_tensor.dim() == 1:\n",
    "            pairs_tensor = pairs_tensor.unsqueeze(0)\n",
    "\n",
    "        pos_edges = pairs_tensor  # [batch_size, 2]\n",
    "        num_pos = pos_edges.size(0)\n",
    "\n",
    "        # ------------------------------\n",
    "        # Create random negatives\n",
    "        # ------------------------------\n",
    "        neg_tail = torch.randint(0, num_entities, (num_pos,), device=device)\n",
    "        neg_head = torch.randint(0, num_entities, (num_pos,), device=device)\n",
    "\n",
    "        # ------------------------------\n",
    "        # Encode embeddings (per-batch to avoid retain_graph issues)\n",
    "        # ------------------------------\n",
    "        embeddings = model.encode(edge_index)\n",
    "\n",
    "        # Normalize to prevent exploding scores\n",
    "        emb = embeddings / (embeddings.norm(dim=1, keepdim=True) + 1e-9)\n",
    "\n",
    "        # ------------------------------\n",
    "        # Compute scores\n",
    "        # ------------------------------\n",
    "        pos_scores = (emb[pos_edges[:, 0]] * emb[pos_edges[:, 1]]).sum(dim=1)\n",
    "        neg_scores_tail = (emb[pos_edges[:, 0]] * emb[neg_tail]).sum(dim=1)\n",
    "        neg_scores_head = (emb[neg_head] * emb[pos_edges[:, 1]]).sum(dim=1)\n",
    "\n",
    "        # ------------------------------\n",
    "        # Stable BPR loss\n",
    "        # ------------------------------\n",
    "        loss_tail = F.softplus(-(pos_scores - neg_scores_tail)).mean()\n",
    "        loss_head = F.softplus(-(pos_scores - neg_scores_head)).mean()\n",
    "        loss = loss_tail + loss_head\n",
    "\n",
    "        # ------------------------------\n",
    "        # NaN / inf guard\n",
    "        # ------------------------------\n",
    "        if not torch.isfinite(loss):\n",
    "            print(f\"[Batch {batch_idx}] NaN or inf detected — skipping this batch.\")\n",
    "            continue\n",
    "\n",
    "        # ------------------------------\n",
    "        # Backprop and update\n",
    "        # ------------------------------\n",
    "        loss.backward()\n",
    "\n",
    "        # Optional gradient clipping (prevents explosions)\n",
    "        if max_grad_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / max(1, num_batches)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8399de0a",
   "metadata": {
    "id": "8399de0a"
   },
   "source": [
    "### Negative Sampling\n",
    "- A training technique used in knowledge graph link prediction to create \"negative examples\", triples that are likely to be false.\n",
    "- Since knowledge graphs only contain positive facts (true triples), we need to artificially create negative examples for the model to learn what relationships are incorrect.\n",
    "- Both head and tail corruptions are used to train the model to understand connections flowing in both directions. Model learns \"What subjects fit this relation-object\" and \"What object fits this subject-relation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef898ab7",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1760629850437,
     "user": {
      "displayName": "Sih Jia Qi",
      "userId": "08353846451037845175"
     },
     "user_tz": -480
    },
    "id": "ef898ab7"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pairs_from_triples(triples: torch.LongTensor) -> torch.LongTensor:\n",
    "    \"\"\"\n",
    "    Convert (h, r, t) -> pairs [2, N] = (h, t) for decoding on collapsed graph.\n",
    "    \"\"\"\n",
    "    return triples[:, [0, 2]].t().contiguous()  # [2, N]\n",
    "\n",
    "@torch.no_grad()\n",
    "def negative_sample_heads(triples: torch.LongTensor, num_nodes: int) -> torch.LongTensor:\n",
    "    \"\"\"\n",
    "    Corrupt heads: (h, r, t) -> (h', t)\n",
    "    Returns pairs [2, N].\n",
    "    \"\"\"\n",
    "    N = triples.size(0)\n",
    "    neg_h = torch.randint(0, num_nodes, (N,), device=triples.device)\n",
    "    t = triples[:, 2]\n",
    "    return torch.stack([neg_h, t], dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def negative_sample_tails(triples: torch.LongTensor, num_nodes: int) -> torch.LongTensor:\n",
    "    \"\"\"\n",
    "    Corrupt tails: (h, r, t) -> (h, t')\n",
    "    Returns pairs [2, N].\n",
    "    \"\"\"\n",
    "    N = triples.size(0)\n",
    "    h = triples[:, 0]\n",
    "    neg_t = torch.randint(0, num_nodes, (N,), device=triples.device)\n",
    "    return torch.stack([h, neg_t], dim=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4c4130",
   "metadata": {
    "id": "ff4c4130"
   },
   "source": [
    "# Model 1 LightGCN\n",
    "- We also add new inverse type of relation on top of the 11 that already exists.\n",
    "- This allows for information to be passed around which originally did not.\n",
    "- A -> B is one way, and there should be an inverse relationship (or some information) which is missed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f5105ec",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1760629850444,
     "user": {
      "displayName": "Sih Jia Qi",
      "userId": "08353846451037845175"
     },
     "user_tz": -480
    },
    "id": "8f5105ec"
   },
   "outputs": [],
   "source": [
    "# -------- LightGCN layer --------\n",
    "class LightGCNConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr='add')\n",
    "\n",
    "    # Compute symmetric normalization term D^-0.5*A*D^-0.5 to propagate messages through normalized adjacency\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.clamp(min=1).pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    # Scales the neighbor embeddings\n",
    "    def message(self, x_j: torch.Tensor, norm: torch.Tensor) -> torch.Tensor:\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "# -------- LightGCN encoder + dot-product decoder --------\n",
    "class LightGCN(nn.Module):\n",
    "    # Initialize trainable node embeddings\n",
    "    def __init__(self, num_nodes: int, emb_dim: int = 64, num_layers: int = 3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_nodes, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        self.convs = nn.ModuleList([LightGCNConv() for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def encode(self, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        x0 = self.embedding.weight\n",
    "        out = x0\n",
    "        x = x0\n",
    "        # Each layer's output is accumulated and averaged\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            out = out + x\n",
    "        return out / (self.num_layers + 1)\n",
    "\n",
    "    # Compute dot product between node embeddings for each edge (positive or negative pair)\n",
    "    @staticmethod\n",
    "    def decode(z: torch.Tensor, pairs: torch.LongTensor) -> torch.Tensor:\n",
    "        # pairs: [2, B] with [src; dst]\n",
    "        return (z[pairs[0]] * z[pairs[1]]).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34dbf35c",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1760629850450,
     "user": {
      "displayName": "Sih Jia Qi",
      "userId": "08353846451037845175"
     },
     "user_tz": -480
    },
    "id": "34dbf35c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def evaluate_auc_hits(model, triples, num_entities, edge_index=None, batch_size=4096, device=None):\n",
    "    \"\"\"\n",
    "    Evaluate AUC and Hits@K (K=1,5,10) for LightGCN or R-LightGCN.\n",
    "    Unfiltered version — each positive triple is compared to 99 random negatives.\n",
    "\n",
    "    Args:\n",
    "        model: LightGCN or R-LightGCN with .encode()\n",
    "        triples: torch.Tensor [N, 3] or [N, 2] validation/test triples\n",
    "        num_entities: total number of entities\n",
    "        edge_index: graph structure for encoding\n",
    "        batch_size: number of triples per batch\n",
    "        device: torch.device to use\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "    # -------------------------\n",
    "    # Helper functions\n",
    "    # -------------------------\n",
    "    def batch_iter(tensor, size):\n",
    "        for i in range(0, len(tensor), size):\n",
    "            yield tensor[i:i + size]\n",
    "\n",
    "    def sample_negatives(pos_batch, num_entities):\n",
    "        \"\"\"Corrupt tail entities randomly.\"\"\"\n",
    "        neg_batch = pos_batch.clone()\n",
    "        neg_batch[:, -1] = torch.randint(0, num_entities, (pos_batch.size(0),), device=pos_batch.device)\n",
    "        return neg_batch\n",
    "\n",
    "    # -------------------------\n",
    "    # 1️⃣ AUC computation\n",
    "    # -------------------------\n",
    "    scores_all, labels_all = [], []\n",
    "\n",
    "    for pos in batch_iter(triples, batch_size):\n",
    "        pos = pos.to(device)\n",
    "        neg = sample_negatives(pos, num_entities)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if hasattr(model, 'encode'):\n",
    "                emb = model.encode(edge_index)\n",
    "                s_pos = (emb[pos[:, 0]] * emb[pos[:, -1]]).sum(dim=1)\n",
    "                s_neg = (emb[neg[:, 0]] * emb[neg[:, -1]]).sum(dim=1)\n",
    "            else:\n",
    "                s_pos = model(pos)\n",
    "                s_neg = model(neg)\n",
    "\n",
    "            scores_all.append(torch.cat([s_pos, s_neg], dim=0))\n",
    "            labels_all.append(torch.cat([torch.ones_like(s_pos), torch.zeros_like(s_neg)], dim=0))\n",
    "\n",
    "    # If no scores were collected, return defaults\n",
    "    if len(scores_all) == 0:\n",
    "        return {\"auc\": 0.5, \"hits@1\": 0.0, \"hits@5\": 0.0, \"hits@10\": 0.0}\n",
    "\n",
    "    scores_all = torch.cat(scores_all, dim=0)\n",
    "    labels_all = torch.cat(labels_all, dim=0)\n",
    "    # Compute ROC-AUC using torchmetrics (pure torch, avoids numpy)\n",
    "    try:\n",
    "        from torchmetrics.classification import BinaryAUROC\n",
    "        auroc_metric = BinaryAUROC().to(scores_all.device)\n",
    "        auc = auroc_metric(scores_all, labels_all.int()).item()\n",
    "    except Exception:\n",
    "        auc = 0.5\n",
    "\n",
    "    # -------------------------\n",
    "    # 2️⃣ Hits@K computation (1,5,10)\n",
    "    # -------------------------\n",
    "    hits_at = {1: 0, 5: 0, 10: 0}\n",
    "    n_trials = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ent = model.encode(edge_index) if hasattr(model, 'encode') else model.encoder(edge_index)\n",
    "\n",
    "        for pos in batch_iter(triples, batch_size):\n",
    "            pos = pos.to(device)\n",
    "            B = pos.size(0)\n",
    "            true_t = pos[:, -1]\n",
    "            rand_t = torch.randint(0, num_entities, (B, 99), device=device)\n",
    "            tails = torch.cat([true_t.unsqueeze(1), rand_t], dim=1)  # [B,100]\n",
    "\n",
    "            e_h = ent[pos[:, 0]]                                    # [B,d]\n",
    "            e_candidates = ent[tails]                               # [B,100,d]\n",
    "            s = (e_h.unsqueeze(1) * e_candidates).sum(dim=2)        # [B,100]\n",
    "\n",
    "            # rank position of true tail\n",
    "            ranks = (s.argsort(dim=1, descending=True) == 0).nonzero()[:, 1] + 1  # 1-based\n",
    "\n",
    "            for k in hits_at.keys():\n",
    "                hits_at[k] += (ranks <= k).sum().item()\n",
    "\n",
    "            n_trials += B\n",
    "\n",
    "    # Normalize\n",
    "    hits_at = {f\"hits@{k}\": v / n_trials for k, v in hits_at.items()}\n",
    "\n",
    "    return {\"auc\": float(auc), **hits_at}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd155a53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 497203,
     "status": "ok",
     "timestamp": 1760632174253,
     "user": {
      "displayName": "Sih Jia Qi",
      "userId": "08353846451037845175"
     },
     "user_tz": -480
    },
    "id": "fd155a53",
    "outputId": "d61c5270-7a17-4802-b465-fadc1b7f999d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LightGCN training...\n",
      "Max epochs: 100, Early stopping patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraining LightGCN:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1: Loss=0.8880\n",
      "\n",
      "Evaluating epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraining LightGCN:   1%|          | 1/100 [00:08<13:24,  8.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.8880, AUC=0.8733, Hits@1=0.2725, Hits@5=0.5445, Hits@10=0.6833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraining LightGCN:   2%|▏         | 2/100 [00:15<12:38,  7.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2: Loss=0.8166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraining LightGCN:   3%|▎         | 3/100 [00:23<12:18,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3: Loss=0.8101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraining LightGCN:   4%|▍         | 4/100 [00:30<12:12,  7.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4: Loss=0.8086\n",
      "Epoch   5: Loss=0.8075\n",
      "\n",
      "Evaluating epoch 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraining LightGCN:   5%|▌         | 5/100 [00:38<12:20,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss=0.8075, AUC=0.8817, Hits@1=0.2779, Hits@5=0.5592, Hits@10=0.7063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:   9%|▉         | 9/100 [01:08<11:26,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10: Loss=0.8054\n",
      "\n",
      "Evaluating epoch 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraining LightGCN:  10%|█         | 10/100 [01:16<11:34,  7.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss=0.8054, AUC=0.8834, Hits@1=0.2774, Hits@5=0.5606, Hits@10=0.7061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  14%|█▍        | 14/100 [01:46<10:50,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15: Loss=0.8043\n",
      "\n",
      "Evaluating epoch 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraining LightGCN:  15%|█▌        | 15/100 [01:55<10:56,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Loss=0.8043, AUC=0.8862, Hits@1=0.2783, Hits@5=0.5620, Hits@10=0.7057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  19%|█▉        | 19/100 [02:25<10:13,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20: Loss=0.8032\n",
      "\n",
      "Evaluating epoch 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraining LightGCN:  20%|██        | 20/100 [02:33<10:18,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Loss=0.8032, AUC=0.8836, Hits@1=0.2801, Hits@5=0.5622, Hits@10=0.7061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  24%|██▍       | 24/100 [03:03<09:31,  7.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  25: Loss=0.8036\n",
      "\n",
      "Evaluating epoch 25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraining LightGCN:  25%|██▌       | 25/100 [03:11<09:35,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Loss=0.8036, AUC=0.8840, Hits@1=0.2780, Hits@5=0.5611, Hits@10=0.7041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  29%|██▉       | 29/100 [03:41<08:57,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30: Loss=0.8031\n",
      "\n",
      "Evaluating epoch 30...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraining LightGCN:  30%|███       | 30/100 [03:49<09:00,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Loss=0.8031, AUC=0.8831, Hits@1=0.2766, Hits@5=0.5604, Hits@10=0.7052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  34%|███▍      | 34/100 [04:19<08:21,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  35: Loss=0.8028\n",
      "\n",
      "Evaluating epoch 35...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraining LightGCN:  35%|███▌      | 35/100 [04:27<08:23,  7.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: Loss=0.8028, AUC=0.8838, Hits@1=0.2768, Hits@5=0.5614, Hits@10=0.7054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  39%|███▉      | 39/100 [04:57<07:40,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  40: Loss=0.8031\n",
      "\n",
      "Evaluating epoch 40...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraining LightGCN:  40%|████      | 40/100 [05:05<07:42,  7.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: Loss=0.8031, AUC=0.8854, Hits@1=0.2743, Hits@5=0.5604, Hits@10=0.7041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  44%|████▍     | 44/100 [05:35<07:03,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  45: Loss=0.8026\n",
      "\n",
      "Evaluating epoch 45...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraining LightGCN:  45%|████▌     | 45/100 [05:43<07:05,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: Loss=0.8026, AUC=0.8845, Hits@1=0.2762, Hits@5=0.5594, Hits@10=0.7046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  49%|████▉     | 49/100 [06:14<06:26,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  50: Loss=0.8024\n",
      "\n",
      "Evaluating epoch 50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraining LightGCN:  50%|█████     | 50/100 [06:22<06:26,  7.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Loss=0.8024, AUC=0.8827, Hits@1=0.2740, Hits@5=0.5562, Hits@10=0.7040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  54%|█████▍    | 54/100 [06:52<05:46,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  55: Loss=0.8025\n",
      "\n",
      "Evaluating epoch 55...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraining LightGCN:  55%|█████▌    | 55/100 [07:00<05:46,  7.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: Loss=0.8025, AUC=0.8828, Hits@1=0.2732, Hits@5=0.5596, Hits@10=0.7030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  59%|█████▉    | 59/100 [07:30<05:10,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  60: Loss=0.8017\n",
      "\n",
      "Evaluating epoch 60...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraining LightGCN:  60%|██████    | 60/100 [07:38<05:09,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: Loss=0.8017, AUC=0.8843, Hits@1=0.2740, Hits@5=0.5579, Hits@10=0.7045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  64%|██████▍   | 64/100 [08:08<04:33,  7.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  65: Loss=0.8020\n",
      "\n",
      "Evaluating epoch 65...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraining LightGCN:  64%|██████▍   | 64/100 [08:16<04:39,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65: Loss=0.8020, AUC=0.8843, Hits@1=0.2723, Hits@5=0.5557, Hits@10=0.7034\n",
      "Early stopping at epoch 65 (Best AUC: 0.8862)\n",
      "\n",
      "LightGCN training completed!\n",
      "\n",
      "Running final test evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGCN FB15K TEST RESULTS]\n",
      "AUC: 0.8821\n",
      "Hits@1: 0.2720\n",
      "Hits@5: 0.5579\n",
      "Hits@10: 0.7034\n",
      "Final model saved: /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/FB15K/fb15k_lightgcn_model.pt\n",
      "Saving 14 consistent metric entries...\n",
      "Metrics saved to /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/FB15K/fb15k_lightgcn_metrics.txt\n",
      "✅ Full model + training history bundle saved: /content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/FB15K/fb15k_lightgcn_full_bundle.pt\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Hyperparameters\n",
    "# ===============================\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "emb_dim = 64\n",
    "num_layers = 3\n",
    "eval_every = 5\n",
    "batch_size = 2048\n",
    "patience = 10\n",
    "\n",
    "# ===============================\n",
    "# Setup\n",
    "# ===============================\n",
    "lightgcn_metrics_tracker = MetricsTracker()\n",
    "lightgcn_early_stopping = EarlyStopping(patience=patience, metric='val_auc')\n",
    "\n",
    "# Create model + optimizer\n",
    "lightgcn_model = LightGCN(num_nodes=num_entities, emb_dim=emb_dim, num_layers=num_layers).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(lightgcn_model.parameters(), lr=lr)\n",
    "\n",
    "print(\"Starting LightGCN training...\")\n",
    "print(f\"Max epochs: {epochs}, Early stopping patience: {patience}\")\n",
    "\n",
    "# ===============================\n",
    "# Training Loop\n",
    "# ===============================\n",
    "for epoch in tqdm(range(1, epochs + 1), desc=\"Training LightGCN\"):\n",
    "    avg_loss = train_one_epoch_lightgcn(\n",
    "        lightgcn_model, train_loader_collapsed, optimizer,\n",
    "        edge_index, device, num_entities\n",
    "    )\n",
    "\n",
    "    if epoch <= 5 or epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: Loss={avg_loss:.4f}\")\n",
    "\n",
    "    # ===============================\n",
    "    # Evaluation\n",
    "    # ===============================\n",
    "    if epoch % eval_every == 0 or epoch == 1:\n",
    "        print(f\"\\nEvaluating epoch {epoch}...\")\n",
    "        # Convert val_loader to flat tensor of pairs/triples\n",
    "        all_val_data = []\n",
    "        for batch in val_loader_collapsed:\n",
    "            data_tensor = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "            if data_tensor.dim() == 1:\n",
    "                data_tensor = data_tensor.unsqueeze(0)\n",
    "            all_val_data.append(data_tensor)\n",
    "        val_triples = torch.cat(all_val_data, dim=0)\n",
    "\n",
    "        # Run simplified evaluation (AUC + Hits@K)\n",
    "        val_metrics = evaluate_auc_hits(\n",
    "            lightgcn_model,\n",
    "            val_triples,\n",
    "            num_entities=num_entities,\n",
    "            edge_index=edge_index,\n",
    "            batch_size=2048,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Print summary\n",
    "        print(f\"Epoch {epoch}: Loss={avg_loss:.4f}, \"\n",
    "              f\"AUC={val_metrics['auc']:.4f}, \"\n",
    "              f\"Hits@1={val_metrics['hits@1']:.4f}, \"\n",
    "              f\"Hits@5={val_metrics['hits@5']:.4f}, \"\n",
    "              f\"Hits@10={val_metrics['hits@10']:.4f}\")\n",
    "\n",
    "        # Log to tracker (simple flattening)\n",
    "        lightgcn_metrics_tracker.add(\n",
    "            epoch=epoch,\n",
    "            loss=avg_loss,\n",
    "            val_auc=val_metrics['auc'],\n",
    "            val_hits1=val_metrics['hits@1'],\n",
    "            val_hits5=val_metrics['hits@5'],\n",
    "            val_hits10=val_metrics['hits@10']\n",
    "        )\n",
    "\n",
    "        # Early stopping\n",
    "        if lightgcn_early_stopping(lightgcn_metrics_tracker):\n",
    "            print(f\"Early stopping at epoch {epoch} \"\n",
    "                  f\"(Best AUC: {lightgcn_early_stopping.best_score:.4f})\")\n",
    "            break\n",
    "\n",
    "print(\"\\nLightGCN training completed!\")\n",
    "\n",
    "# ===============================\n",
    "# Final Test Evaluation\n",
    "# ===============================\n",
    "print(\"\\nRunning final test evaluation...\")\n",
    "all_test_data = []\n",
    "for batch in test_loader_collapsed:\n",
    "    data_tensor = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "    if data_tensor.dim() == 1:\n",
    "        data_tensor = data_tensor.unsqueeze(0)\n",
    "    all_test_data.append(data_tensor)\n",
    "test_triples = torch.cat(all_test_data, dim=0)\n",
    "\n",
    "lightgcn_final_test_metrics = evaluate_auc_hits(\n",
    "    lightgcn_model,\n",
    "    test_triples,\n",
    "    num_entities=num_entities,\n",
    "    edge_index=edge_index,\n",
    "    batch_size=2048,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\n[LightGCN FB15K TEST RESULTS]\")\n",
    "print(f\"AUC: {lightgcn_final_test_metrics['auc']:.4f}\")\n",
    "print(f\"Hits@1: {lightgcn_final_test_metrics['hits@1']:.4f}\")\n",
    "print(f\"Hits@5: {lightgcn_final_test_metrics['hits@5']:.4f}\")\n",
    "print(f\"Hits@10: {lightgcn_final_test_metrics['hits@10']:.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# Save model + metrics\n",
    "# ===============================\n",
    "save_dir = \"/content/drive/My Drive/NTU/Y3S1/SC4020 Data Analytics and Mining/checkpoints/FB15K\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# --- Save final model ---\n",
    "model_file = os.path.join(save_dir, \"fb15k_lightgcn_model.pt\")\n",
    "torch.save(lightgcn_model.state_dict(), model_file)\n",
    "print(f\"Final model saved: {model_file}\")\n",
    "\n",
    "# --- Prepare and align metrics ---\n",
    "metrics = lightgcn_metrics_tracker.metrics\n",
    "min_len = min(len(v) for v in metrics.values())\n",
    "print(f\"Saving {min_len} consistent metric entries...\")\n",
    "\n",
    "# --- Save metrics safely ---\n",
    "metrics_file = os.path.join(save_dir, \"fb15k_lightgcn_metrics.txt\")\n",
    "with open(metrics_file, \"w\") as f:\n",
    "    f.write(\"Epoch\\tLoss\\tVal_AUC\\tHits@1\\tHits@5\\tHits@10\\n\")\n",
    "    for i in range(min_len):\n",
    "        f.write(\n",
    "            f\"{metrics['epoch'][i]}\\t\"\n",
    "            f\"{metrics['loss'][i]:.6f}\\t\"\n",
    "            f\"{metrics['val_auc'][i]:.6f}\\t\"\n",
    "            f\"{metrics['val_hits1'][i]:.6f}\\t\"\n",
    "            f\"{metrics['val_hits5'][i]:.6f}\\t\"\n",
    "            f\"{metrics['val_hits10'][i]:.6f}\\n\"\n",
    "        )\n",
    "print(f\"Metrics saved to {metrics_file}\")\n",
    "\n",
    "# --- Optional: save complete training bundle ---\n",
    "bundle_file = os.path.join(save_dir, \"fb15k_lightgcn_full_bundle.pt\")\n",
    "torch.save({\n",
    "    \"model_state_dict\": lightgcn_model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"hyperparameters\": {\n",
    "        \"emb_dim\": emb_dim,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"lr\": lr,\n",
    "        \"num_entities\": num_entities\n",
    "    },\n",
    "    \"final_test_metrics\": lightgcn_final_test_metrics,\n",
    "    \"training_history\": metrics\n",
    "}, bundle_file)\n",
    "print(f\"Full model + training history bundle saved: {bundle_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

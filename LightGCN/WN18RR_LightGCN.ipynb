{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f9dd09d",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaca170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import degree, add_self_loops\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import sys\n",
    "current_dir = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n",
    "parent_dir = current_dir.parent\n",
    "sys.path.append(str(parent_dir))\n",
    "from utility.dataset_loader import KGDataModuleCollapsed, KGDataModuleTyped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1229a7677039018",
   "metadata": {},
   "source": [
    "# Dataset Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a9927759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Dataset: WN18RR\n",
      "Entities: 40,943\n",
      "Original Relations: 11\n",
      "Relations (with inverse): 22\n",
      "Training pairs: 173,670\n",
      "Training triples (typed): 173,670\n",
      "Graph edges (with self-loops): 214,613\n",
      "Graph edge_index shape: (2, 214613)\n",
      "Data loaders created:\n",
      "Train batches (collapsed): 43\n",
      "Val batches (collapsed): 2\n",
      "Test batches (collapsed): 2\n",
      "Dataset: WN18RR\n",
      "Entities: 40,943\n",
      "Original Relations: 11\n",
      "Relations (with inverse): 22\n",
      "Training pairs: 173,670\n",
      "Training triples (typed): 173,670\n",
      "Graph edges (with self-loops): 214,613\n",
      "Graph edge_index shape: (2, 214613)\n",
      "Data loaders created:\n",
      "Train batches (collapsed): 43\n",
      "Val batches (collapsed): 2\n",
      "Test batches (collapsed): 2\n"
     ]
    }
   ],
   "source": [
    "# Dataset loading using dataset_loader.py\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset paths\n",
    "train_path = Path(\"../WN18RR/train.txt\")\n",
    "valid_path = Path(\"../WN18RR/valid.txt\")  \n",
    "test_path = Path(\"../WN18RR/test.txt\")\n",
    "\n",
    "# Initialize data modules\n",
    "# Collapsed mode for LightGCN (untyped pairs)\n",
    "dm_collapsed = KGDataModuleCollapsed(\n",
    "    train_path=train_path,\n",
    "    valid_path=valid_path,\n",
    "    test_path=test_path,\n",
    "    batch_size=4096,\n",
    "    shuffle=True,\n",
    "    add_reverse=True\n",
    ")\n",
    "\n",
    "# Typed mode for R-LightGCN (typed triples)  \n",
    "dm_typed = KGDataModuleTyped(\n",
    "    train_path=train_path,\n",
    "    valid_path=valid_path,\n",
    "    test_path=test_path,\n",
    "    batch_size=4096,\n",
    "    shuffle=True,\n",
    "    add_reverse=True,\n",
    "    reverse_relation_strategy=\"duplicate_rel\"\n",
    ")\n",
    "\n",
    "num_entities = len(dm_collapsed.ent2id)\n",
    "num_relations = len(dm_collapsed.rel2id)  # Original relations\n",
    "num_relations_with_inv = len(dm_typed.rel2id)  # With inverse relations\n",
    "\n",
    "print(f\"Dataset: WN18RR\")\n",
    "print(f\"Entities: {num_entities:,}\")\n",
    "print(f\"Original Relations: {num_relations:,}\")\n",
    "print(f\"Relations (with inverse): {num_relations_with_inv:,}\")\n",
    "print(f\"Training pairs: {len(dm_collapsed._train_pairs):,}\")\n",
    "print(f\"Training triples (typed): {len(dm_typed._train_triples):,}\")\n",
    "\n",
    "# Build edge_index for LightGCN (collapsed pairs)\n",
    "edge_index = dm_collapsed._train_pairs.t().contiguous().to(device)\n",
    "edge_index, _ = add_self_loops(edge_index, num_nodes=num_entities)\n",
    "\n",
    "print(f\"Graph edges (with self-loops): {edge_index.shape[1]:,}\")\n",
    "print(f\"Graph edge_index shape: {tuple(edge_index.shape)}\")\n",
    "\n",
    "train_loader_collapsed = dm_collapsed.train_loader()\n",
    "val_loader_collapsed = dm_collapsed.val_loader()\n",
    "test_loader_collapsed = dm_collapsed.test_loader()\n",
    "\n",
    "train_loader_typed = dm_typed.train_loader()\n",
    "val_loader_typed = dm_typed.val_loader()\n",
    "test_loader_typed = dm_typed.test_loader()\n",
    "\n",
    "print(f\"Data loaders created:\")\n",
    "print(f\"Train batches (collapsed): {len(train_loader_collapsed)}\")\n",
    "print(f\"Val batches (collapsed): {len(val_loader_collapsed) if val_loader_collapsed else 0}\")\n",
    "print(f\"Test batches (collapsed): {len(test_loader_collapsed) if test_loader_collapsed else 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beecc39",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a54bd072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Saving Utility Functions\n",
    "def save_model_checkpoint(model, optimizer, hyperparameters, final_test_metrics, \n",
    "                         training_history, model_name, filename):\n",
    "    \"\"\"\n",
    "    Save model checkpoint with comprehensive information.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        optimizer: The optimizer used\n",
    "        hyperparameters: Dict with training hyperparameters\n",
    "        final_test_metrics: Final test evaluation results\n",
    "        training_history: Training metrics history\n",
    "        model_name: Name of the model for display\n",
    "        filename: Output filename for the checkpoint\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'hyperparameters': hyperparameters,\n",
    "        'final_test_metrics': final_test_metrics,\n",
    "        'training_history': training_history,\n",
    "        'model_name': model_name\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"{model_name} model checkpoint saved to {filename}\")\n",
    "\n",
    "def load_model_checkpoint(filename, model_class, device, **model_kwargs):\n",
    "    \"\"\"\n",
    "    Load model checkpoint and restore model state.\n",
    "    \n",
    "    Args:\n",
    "        filename: Path to checkpoint file\n",
    "        model_class: Model class to instantiate\n",
    "        device: Device to load model on\n",
    "        **model_kwargs: Arguments for model instantiation\n",
    "    \n",
    "    Returns:\n",
    "        model: Loaded model\n",
    "        checkpoint: Full checkpoint data\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(filename, map_location=device)\n",
    "    \n",
    "    # Create model instance\n",
    "    model = model_class(**model_kwargs).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"Model loaded from {filename}\")\n",
    "    print(f\"Model: {checkpoint.get('model_name', 'Unknown')}\")\n",
    "    print(f\"Hyperparameters: {checkpoint.get('hyperparameters', {})}\")\n",
    "    \n",
    "    if 'final_test_metrics' in checkpoint:\n",
    "        metrics = checkpoint['final_test_metrics']\n",
    "        if 'head' in metrics and 'tail' in metrics:\n",
    "            auc_avg = (metrics['head']['auc'] + metrics['tail']['auc']) / 2\n",
    "            hits10_avg = (metrics['head']['hits@10'] + metrics['tail']['hits@10']) / 2\n",
    "            print(f\"Test AUC (avg): {auc_avg:.4f}\")\n",
    "            print(f\"Test Hits@10 (avg): {hits10_avg:.4f}\")\n",
    "    \n",
    "    return model, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8ca1b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsTracker:\n",
    "    \"\"\"Simplified metrics tracker without plotting\"\"\"\n",
    "    def __init__(self):\n",
    "        self.metrics = defaultdict(list)\n",
    "    \n",
    "    def add(self, epoch, **kwargs):\n",
    "        self.metrics['epoch'].append(epoch)\n",
    "        for key, value in kwargs.items():\n",
    "            self.metrics[key].append(value)\n",
    "    \n",
    "    def get_best_epoch(self, metric='val_auc_head'):\n",
    "        \"\"\"Get the epoch with the best performance for a given metric\"\"\"\n",
    "        if metric not in self.metrics or not self.metrics[metric]:\n",
    "            return 0\n",
    "        \n",
    "        # For AUC and Hits@K, higher is better\n",
    "        best_idx = np.argmax(self.metrics[metric])\n",
    "        return self.metrics['epoch'][best_idx]\n",
    "    \n",
    "    def save_to_file(self, filepath):\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(\"Epoch\\tLoss\\tAUC\\tHits@1\\tHits@5\\tHits@10\\n\")\n",
    "            for i in range(len(self.metrics['epoch'])):\n",
    "                epoch = self.metrics['epoch'][i]\n",
    "                loss = self.metrics['loss'][i]\n",
    "                auc = self.metrics['val_auc'][i]\n",
    "                h1 = self.metrics['val_hits1'][i]\n",
    "                h5 = self.metrics['val_hits5'][i]\n",
    "                h10 = self.metrics['val_hits10'][i]\n",
    "                \n",
    "                f.write(f\"{epoch}\\t{loss:.6f}\\t{auc:.6f}\\t{h1:.6f}\\t{h5:.6f}\\t{h10:.6f}\\n\")\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, metric='val_auc_head', mode='max', min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.metric = metric\n",
    "        self.mode = mode  # 'max' for AUC, Hits@K; 'min' for loss\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.should_stop = False\n",
    "        \n",
    "    def __call__(self, metrics_tracker):\n",
    "        current_score = metrics_tracker.metrics[self.metric][-1] if self.metric in metrics_tracker.metrics else 0\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "        else:\n",
    "            if self.mode == 'max':\n",
    "                improved = current_score > self.best_score + self.min_delta\n",
    "            else:  # mode == 'min'\n",
    "                improved = current_score < self.best_score - self.min_delta\n",
    "                \n",
    "            if improved:\n",
    "                self.best_score = current_score\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                \n",
    "        self.should_stop = self.counter >= self.patience\n",
    "        return self.should_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "315071c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions with data loaders\n",
    "def train_one_epoch_lightgcn(model, data_loader, optimizer, edge_index, device, num_entities, max_grad_norm=5.0):\n",
    "    \"\"\"\n",
    "    Stable LightGCN training loop (BPR loss).\n",
    "    Handles NaNs, exploding gradients, and large score differences gracefully.\n",
    "\n",
    "    Args:\n",
    "        model: LightGCN model\n",
    "        data_loader: DataLoader yielding positive pairs (batch_size, 2)\n",
    "        optimizer: Optimizer instance\n",
    "        edge_index: Graph edges tensor on the correct device\n",
    "        device: torch.device to run on\n",
    "        num_entities: Total number of entities (int)\n",
    "        max_grad_norm: Optional gradient clipping value\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # ------------------------------\n",
    "        # Handle batch format\n",
    "        # ------------------------------\n",
    "        if isinstance(batch, (list, tuple)):\n",
    "            pairs_tensor = batch[0]\n",
    "        else:\n",
    "            pairs_tensor = batch\n",
    "\n",
    "        pairs_tensor = pairs_tensor.to(device)\n",
    "        if pairs_tensor.dim() == 1:\n",
    "            pairs_tensor = pairs_tensor.unsqueeze(0)\n",
    "\n",
    "        pos_edges = pairs_tensor  # [batch_size, 2]\n",
    "        num_pos = pos_edges.size(0)\n",
    "\n",
    "        # ------------------------------\n",
    "        # Create random negatives\n",
    "        # ------------------------------\n",
    "        neg_tail = torch.randint(0, num_entities, (num_pos,), device=device)\n",
    "        neg_head = torch.randint(0, num_entities, (num_pos,), device=device)\n",
    "\n",
    "        # ------------------------------\n",
    "        # Encode embeddings (per-batch to avoid retain_graph issues)\n",
    "        # ------------------------------\n",
    "        embeddings = model.encode(edge_index)\n",
    "\n",
    "        # Normalize to prevent exploding scores\n",
    "        emb = embeddings / (embeddings.norm(dim=1, keepdim=True) + 1e-9)\n",
    "\n",
    "        # ------------------------------\n",
    "        # Compute scores\n",
    "        # ------------------------------\n",
    "        pos_scores = (emb[pos_edges[:, 0]] * emb[pos_edges[:, 1]]).sum(dim=1)\n",
    "        neg_scores_tail = (emb[pos_edges[:, 0]] * emb[neg_tail]).sum(dim=1)\n",
    "        neg_scores_head = (emb[neg_head] * emb[pos_edges[:, 1]]).sum(dim=1)\n",
    "\n",
    "        # ------------------------------\n",
    "        # Stable BPR loss\n",
    "        # ------------------------------\n",
    "        loss_tail = F.softplus(-(pos_scores - neg_scores_tail)).mean()\n",
    "        loss_head = F.softplus(-(pos_scores - neg_scores_head)).mean()\n",
    "        loss = loss_tail + loss_head\n",
    "\n",
    "        # ------------------------------\n",
    "        # NaN / inf guard\n",
    "        # ------------------------------\n",
    "        if not torch.isfinite(loss):\n",
    "            print(f\"[Batch {batch_idx}] NaN or inf detected — skipping this batch.\")\n",
    "            continue\n",
    "\n",
    "        # ------------------------------\n",
    "        # Backprop and update\n",
    "        # ------------------------------\n",
    "        loss.backward()\n",
    "\n",
    "        # Optional gradient clipping (prevents explosions)\n",
    "        if max_grad_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / max(1, num_batches)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8399de0a",
   "metadata": {},
   "source": [
    "### Negative Sampling\n",
    "- A training technique used in knowledge graph link prediction to create \"negative examples\", triples that are likely to be false. \n",
    "- Since knowledge graphs only contain positive facts (true triples), we need to artificially create negative examples for the model to learn what relationships are incorrect.\n",
    "- Both head and tail corruptions are used to train the model to understand connections flowing in both directions. Model learns \"What subjects fit this relation-object\" and \"What object fits this subject-relation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef898ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def pairs_from_triples(triples: torch.LongTensor) -> torch.LongTensor:\n",
    "    \"\"\"\n",
    "    Convert (h, r, t) -> pairs [2, N] = (h, t) for decoding on collapsed graph.\n",
    "    \"\"\"\n",
    "    return triples[:, [0, 2]].t().contiguous()  # [2, N]\n",
    "\n",
    "@torch.no_grad()\n",
    "def negative_sample_heads(triples: torch.LongTensor, num_nodes: int) -> torch.LongTensor:\n",
    "    \"\"\"\n",
    "    Corrupt heads: (h, r, t) -> (h', t)\n",
    "    Returns pairs [2, N].\n",
    "    \"\"\"\n",
    "    N = triples.size(0)\n",
    "    neg_h = torch.randint(0, num_nodes, (N,), device=triples.device)\n",
    "    t = triples[:, 2]\n",
    "    return torch.stack([neg_h, t], dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def negative_sample_tails(triples: torch.LongTensor, num_nodes: int) -> torch.LongTensor:\n",
    "    \"\"\"\n",
    "    Corrupt tails: (h, r, t) -> (h, t')\n",
    "    Returns pairs [2, N].\n",
    "    \"\"\"\n",
    "    N = triples.size(0)\n",
    "    h = triples[:, 0]\n",
    "    neg_t = torch.randint(0, num_nodes, (N,), device=triples.device)\n",
    "    return torch.stack([h, neg_t], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f5105ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- LightGCN layer --------\n",
    "class LightGCNConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr='add')\n",
    "\n",
    "    # Compute symmetric normalization term D^-0.5*A*D^-0.5 to propagate messages through normalized adjacency\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.clamp(min=1).pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    # Scales the neighbor embeddings\n",
    "    def message(self, x_j: torch.Tensor, norm: torch.Tensor) -> torch.Tensor:\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "# -------- LightGCN encoder + dot-product decoder --------\n",
    "class LightGCN(nn.Module):\n",
    "    # Initialize trainable node embeddings\n",
    "    def __init__(self, num_nodes: int, emb_dim: int = 64, num_layers: int = 3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_nodes, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        self.convs = nn.ModuleList([LightGCNConv() for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def encode(self, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        x0 = self.embedding.weight\n",
    "        out = x0\n",
    "        x = x0\n",
    "        # Each layer's output is accumulated and averaged\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "            out = out + x\n",
    "        return out / (self.num_layers + 1)\n",
    "\n",
    "    # Compute dot product between node embeddings for each edge (positive or negative pair)\n",
    "    @staticmethod\n",
    "    def decode(z: torch.Tensor, pairs: torch.LongTensor) -> torch.Tensor:\n",
    "        # pairs: [2, B] with [src; dst]\n",
    "        return (z[pairs[0]] * z[pairs[1]]).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dbf35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_auc_hits(model, triples, num_entities, edge_index=None, batch_size=4096, device=None):\n",
    "    \"\"\"\n",
    "    Evaluate AUC and Hits@K (K=1,5,10) for LightGCN or R-LightGCN.\n",
    "    Unfiltered version — each positive triple is compared to 99 random negatives.\n",
    "\n",
    "    Args:\n",
    "        model: LightGCN or R-LightGCN with .encode()\n",
    "        triples: torch.Tensor [N, 3] or [N, 2] validation/test triples\n",
    "        num_entities: total number of entities\n",
    "        edge_index: graph structure for encoding\n",
    "        batch_size: number of triples per batch\n",
    "        device: torch.device to use\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "    # -------------------------\n",
    "    # Helper functions\n",
    "    # -------------------------\n",
    "    def batch_iter(tensor, size):\n",
    "        for i in range(0, len(tensor), size):\n",
    "            yield tensor[i:i + size]\n",
    "\n",
    "    def sample_negatives(pos_batch, num_entities):\n",
    "        \"\"\"Corrupt tail entities randomly.\"\"\"\n",
    "        neg_batch = pos_batch.clone()\n",
    "        neg_batch[:, -1] = torch.randint(0, num_entities, (pos_batch.size(0),), device=pos_batch.device)\n",
    "        return neg_batch\n",
    "\n",
    "    # -------------------------\n",
    "    # AUC computation\n",
    "    # -------------------------\n",
    "    scores_all, labels_all = [], []\n",
    "\n",
    "    for pos in batch_iter(triples, batch_size):\n",
    "        pos = pos.to(device)\n",
    "        neg = sample_negatives(pos, num_entities)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if hasattr(model, 'encode'):\n",
    "                emb = model.encode(edge_index)\n",
    "                s_pos = (emb[pos[:, 0]] * emb[pos[:, -1]]).sum(dim=1)\n",
    "                s_neg = (emb[neg[:, 0]] * emb[neg[:, -1]]).sum(dim=1)\n",
    "            else:\n",
    "                s_pos = model(pos)\n",
    "                s_neg = model(neg)\n",
    "\n",
    "        scores_all.append(torch.cat([s_pos, s_neg], 0).cpu().numpy())\n",
    "        labels_all.append(np.concatenate([np.ones(len(s_pos)), np.zeros(len(s_neg))], 0))\n",
    "\n",
    "    scores_all = np.concatenate(scores_all, 0)\n",
    "    labels_all = np.concatenate(labels_all, 0)\n",
    "    auc = roc_auc_score(labels_all, scores_all)\n",
    "\n",
    "    # -------------------------\n",
    "    # Hits@K computation (1,5,10)\n",
    "    # -------------------------\n",
    "    hits_at = {1: 0, 5: 0, 10: 0}\n",
    "    n_trials = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ent = model.encode(edge_index) if hasattr(model, 'encode') else model.encoder(edge_index)\n",
    "\n",
    "        for pos in batch_iter(triples, batch_size):\n",
    "            pos = pos.to(device)\n",
    "            B = pos.size(0)\n",
    "            true_t = pos[:, -1]\n",
    "            rand_t = torch.randint(0, num_entities, (B, 99), device=device)\n",
    "            tails = torch.cat([true_t.unsqueeze(1), rand_t], dim=1)  # [B,100]\n",
    "\n",
    "            e_h = ent[pos[:, 0]]                                    # [B,d]\n",
    "            e_candidates = ent[tails]                               # [B,100,d]\n",
    "            s = (e_h.unsqueeze(1) * e_candidates).sum(dim=2)        # [B,100]\n",
    "\n",
    "            # rank position of true tail\n",
    "            ranks = (s.argsort(dim=1, descending=True) == 0).nonzero()[:, 1] + 1  # 1-based\n",
    "\n",
    "            for k in hits_at.keys():\n",
    "                hits_at[k] += (ranks <= k).sum().item()\n",
    "\n",
    "            n_trials += B\n",
    "\n",
    "    # Normalize\n",
    "    hits_at = {f\"hits@{k}\": v / n_trials for k, v in hits_at.items()}\n",
    "\n",
    "    return {\"auc\": float(auc), **hits_at}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd155a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LightGCN training...\n",
      "Max epochs: 100, Early stopping patience: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1: Loss=0.8249\n",
      "\n",
      "Evaluating epoch 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:   1%|          | 1/100 [00:07<12:30,  7.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.8249, AUC=0.8821, Hits@1=0.6091, Hits@5=0.7426, Hits@10=0.7810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:   2%|▏         | 2/100 [00:15<12:40,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2: Loss=0.7204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:   3%|▎         | 3/100 [00:22<12:12,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3: Loss=0.7016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:   4%|▍         | 4/100 [00:32<13:22,  8.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4: Loss=0.6920\n",
      "Epoch   5: Loss=0.6867\n",
      "\n",
      "Evaluating epoch 5...\n",
      "Epoch   5: Loss=0.6867\n",
      "\n",
      "Evaluating epoch 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:   5%|▌         | 5/100 [00:40<13:01,  8.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss=0.6867, AUC=0.9153, Hits@1=0.6386, Hits@5=0.8090, Hits@10=0.8497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:   9%|▉         | 9/100 [01:15<12:42,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10: Loss=0.6751\n",
      "\n",
      "Evaluating epoch 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  10%|█         | 10/100 [01:23<12:33,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss=0.6751, AUC=0.9249, Hits@1=0.6177, Hits@5=0.8230, Hits@10=0.8645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  14%|█▍        | 14/100 [01:59<12:31,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15: Loss=0.6723\n",
      "\n",
      "Evaluating epoch 15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  15%|█▌        | 15/100 [02:08<12:37,  8.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Loss=0.6723, AUC=0.9280, Hits@1=0.6119, Hits@5=0.8240, Hits@10=0.8724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  19%|█▉        | 19/100 [02:43<11:39,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20: Loss=0.6703\n",
      "\n",
      "Evaluating epoch 20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  20%|██        | 20/100 [02:52<11:54,  8.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Loss=0.6703, AUC=0.9309, Hits@1=0.6056, Hits@5=0.8284, Hits@10=0.8724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  24%|██▍       | 24/100 [03:27<11:02,  8.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  25: Loss=0.6694\n",
      "\n",
      "Evaluating epoch 25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  25%|██▌       | 25/100 [03:36<11:04,  8.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Loss=0.6694, AUC=0.9299, Hits@1=0.6088, Hits@5=0.8279, Hits@10=0.8726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  29%|██▉       | 29/100 [04:12<10:37,  8.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30: Loss=0.6683\n",
      "\n",
      "Evaluating epoch 30...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  30%|███       | 30/100 [04:20<10:17,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Loss=0.6683, AUC=0.9291, Hits@1=0.5992, Hits@5=0.8293, Hits@10=0.8743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  34%|███▍      | 34/100 [04:51<08:38,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  35: Loss=0.6680\n",
      "\n",
      "Evaluating epoch 35...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  35%|███▌      | 35/100 [04:59<08:38,  7.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: Loss=0.6680, AUC=0.9308, Hits@1=0.6046, Hits@5=0.8268, Hits@10=0.8733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  39%|███▉      | 39/100 [05:29<07:49,  7.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  40: Loss=0.6672\n",
      "\n",
      "Evaluating epoch 40...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  40%|████      | 40/100 [05:37<07:48,  7.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: Loss=0.6672, AUC=0.9309, Hits@1=0.5948, Hits@5=0.8296, Hits@10=0.8738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  44%|████▍     | 44/100 [06:09<07:28,  8.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  45: Loss=0.6666\n",
      "\n",
      "Evaluating epoch 45...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  45%|████▌     | 45/100 [06:17<07:26,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: Loss=0.6666, AUC=0.9294, Hits@1=0.6032, Hits@5=0.8304, Hits@10=0.8716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  49%|████▉     | 49/100 [06:47<06:33,  7.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  50: Loss=0.6666\n",
      "\n",
      "Evaluating epoch 50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  50%|█████     | 50/100 [06:57<06:48,  8.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Loss=0.6666, AUC=0.9301, Hits@1=0.5999, Hits@5=0.8319, Hits@10=0.8736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  54%|█████▍    | 54/100 [07:28<06:02,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  55: Loss=0.6665\n",
      "\n",
      "Evaluating epoch 55...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  55%|█████▌    | 55/100 [07:36<05:53,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: Loss=0.6665, AUC=0.9328, Hits@1=0.5951, Hits@5=0.8289, Hits@10=0.8752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  59%|█████▉    | 59/100 [08:07<05:22,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  60: Loss=0.6659\n",
      "\n",
      "Evaluating epoch 60...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  60%|██████    | 60/100 [08:16<05:32,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: Loss=0.6659, AUC=0.9322, Hits@1=0.5974, Hits@5=0.8298, Hits@10=0.8771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  64%|██████▍   | 64/100 [08:47<04:42,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  65: Loss=0.6658\n",
      "\n",
      "Evaluating epoch 65...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  65%|██████▌   | 65/100 [08:56<04:42,  8.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65: Loss=0.6658, AUC=0.9295, Hits@1=0.6018, Hits@5=0.8238, Hits@10=0.8744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  69%|██████▉   | 69/100 [09:27<04:08,  8.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  70: Loss=0.6662\n",
      "\n",
      "Evaluating epoch 70...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  70%|███████   | 70/100 [09:36<04:07,  8.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70: Loss=0.6662, AUC=0.9329, Hits@1=0.5944, Hits@5=0.8261, Hits@10=0.8785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  74%|███████▍  | 74/100 [10:08<03:28,  8.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  75: Loss=0.6657\n",
      "\n",
      "Evaluating epoch 75...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  75%|███████▌  | 75/100 [10:16<03:19,  7.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75: Loss=0.6657, AUC=0.9331, Hits@1=0.5892, Hits@5=0.8296, Hits@10=0.8780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  79%|███████▉  | 79/100 [10:45<02:38,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  80: Loss=0.6654\n",
      "\n",
      "Evaluating epoch 80...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  80%|████████  | 80/100 [10:54<02:37,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80: Loss=0.6654, AUC=0.9324, Hits@1=0.5948, Hits@5=0.8314, Hits@10=0.8779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  84%|████████▍ | 84/100 [11:28<02:16,  8.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  85: Loss=0.6656\n",
      "\n",
      "Evaluating epoch 85...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  85%|████████▌ | 85/100 [11:36<02:06,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85: Loss=0.6656, AUC=0.9306, Hits@1=0.5984, Hits@5=0.8281, Hits@10=0.8800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  89%|████████▉ | 89/100 [12:09<01:28,  8.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  90: Loss=0.6651\n",
      "\n",
      "Evaluating epoch 90...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  90%|█████████ | 90/100 [12:17<01:20,  8.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90: Loss=0.6651, AUC=0.9365, Hits@1=0.5972, Hits@5=0.8339, Hits@10=0.8823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  94%|█████████▍| 94/100 [12:47<00:46,  7.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  95: Loss=0.6651\n",
      "\n",
      "Evaluating epoch 95...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  95%|█████████▌| 95/100 [12:56<00:39,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95: Loss=0.6651, AUC=0.9367, Hits@1=0.5943, Hits@5=0.8316, Hits@10=0.8809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN:  99%|█████████▉| 99/100 [13:26<00:07,  7.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Loss=0.6649\n",
      "\n",
      "Evaluating epoch 100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LightGCN: 100%|██████████| 100/100 [13:34<00:00,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: Loss=0.6649, AUC=0.9349, Hits@1=0.5944, Hits@5=0.8317, Hits@10=0.8802\n",
      "\n",
      "LightGCN training completed!\n",
      "\n",
      "Running final test evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGCN WN18RR TEST RESULTS]\n",
      "AUC: 0.9319\n",
      "Hits@1: 0.5994\n",
      "Hits@5: 0.8336\n",
      "Hits@10: 0.8805\n",
      "Model saved to wn18rr_lightgcn_model.pt\n",
      "Metrics saved to wn18rr_lightgcn_metrics.txt\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Hyperparameters\n",
    "# ===============================\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "emb_dim = 64\n",
    "num_layers = 3\n",
    "eval_every = 5\n",
    "batch_size = 2048\n",
    "patience = 10\n",
    "\n",
    "# ===============================\n",
    "# Setup\n",
    "# ===============================\n",
    "lightgcn_metrics_tracker = MetricsTracker()\n",
    "lightgcn_early_stopping = EarlyStopping(patience=patience, metric='val_auc')\n",
    "\n",
    "# Create model + optimizer\n",
    "lightgcn_model = LightGCN(num_nodes=num_entities, emb_dim=emb_dim, num_layers=num_layers).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(lightgcn_model.parameters(), lr=lr)\n",
    "\n",
    "print(\"Starting LightGCN training...\")\n",
    "print(f\"Max epochs: {epochs}, Early stopping patience: {patience}\")\n",
    "\n",
    "# ===============================\n",
    "# Training Loop\n",
    "# ===============================\n",
    "for epoch in tqdm(range(1, epochs + 1), desc=\"Training LightGCN\"):\n",
    "    avg_loss = train_one_epoch_lightgcn(\n",
    "        lightgcn_model, train_loader_collapsed, optimizer, \n",
    "        edge_index, device, num_entities\n",
    "    )\n",
    "\n",
    "    if epoch <= 5 or epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: Loss={avg_loss:.4f}\")\n",
    "\n",
    "    # ===============================\n",
    "    # Evaluation\n",
    "    # ===============================\n",
    "    if epoch % eval_every == 0 or epoch == 1:\n",
    "        print(f\"\\nEvaluating epoch {epoch}...\")\n",
    "        # Convert val_loader to flat tensor of pairs/triples\n",
    "        all_val_data = []\n",
    "        for batch in val_loader_collapsed:\n",
    "            data_tensor = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "            if data_tensor.dim() == 1:\n",
    "                data_tensor = data_tensor.unsqueeze(0)\n",
    "            all_val_data.append(data_tensor)\n",
    "        val_triples = torch.cat(all_val_data, dim=0)\n",
    "\n",
    "        # Run simplified evaluation (AUC + Hits@K)\n",
    "        val_metrics = evaluate_auc_hits(\n",
    "            lightgcn_model,\n",
    "            val_triples,\n",
    "            num_entities=num_entities,\n",
    "            edge_index=edge_index,\n",
    "            batch_size=2048,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Print summary\n",
    "        print(f\"Epoch {epoch}: Loss={avg_loss:.4f}, \"\n",
    "              f\"AUC={val_metrics['auc']:.4f}, \"\n",
    "              f\"Hits@1={val_metrics['hits@1']:.4f}, \"\n",
    "              f\"Hits@5={val_metrics['hits@5']:.4f}, \"\n",
    "              f\"Hits@10={val_metrics['hits@10']:.4f}\")\n",
    "\n",
    "        # Log to tracker (simple flattening)\n",
    "        lightgcn_metrics_tracker.add(\n",
    "            epoch=epoch,\n",
    "            loss=avg_loss,\n",
    "            val_auc=val_metrics['auc'],\n",
    "            val_hits1=val_metrics['hits@1'],\n",
    "            val_hits5=val_metrics['hits@5'],\n",
    "            val_hits10=val_metrics['hits@10']\n",
    "        )\n",
    "\n",
    "        # Early stopping\n",
    "        if lightgcn_early_stopping(lightgcn_metrics_tracker):\n",
    "            print(f\"Early stopping at epoch {epoch} \"\n",
    "                  f\"(Best AUC: {lightgcn_early_stopping.best_score:.4f})\")\n",
    "            break\n",
    "\n",
    "print(\"\\nLightGCN training completed!\")\n",
    "\n",
    "# ===============================\n",
    "# Final Test Evaluation\n",
    "# ===============================\n",
    "print(\"\\nRunning final test evaluation...\")\n",
    "all_test_data = []\n",
    "for batch in test_loader_collapsed:\n",
    "    data_tensor = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "    if data_tensor.dim() == 1:\n",
    "        data_tensor = data_tensor.unsqueeze(0)\n",
    "    all_test_data.append(data_tensor)\n",
    "test_triples = torch.cat(all_test_data, dim=0)\n",
    "\n",
    "lightgcn_final_test_metrics = evaluate_auc_hits(\n",
    "    lightgcn_model,\n",
    "    test_triples,\n",
    "    num_entities=num_entities,\n",
    "    edge_index=edge_index,\n",
    "    batch_size=2048,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\n[LightGCN WN18RR TEST RESULTS]\")\n",
    "print(f\"AUC: {lightgcn_final_test_metrics['auc']:.4f}\")\n",
    "print(f\"Hits@1: {lightgcn_final_test_metrics['hits@1']:.4f}\")\n",
    "print(f\"Hits@5: {lightgcn_final_test_metrics['hits@5']:.4f}\")\n",
    "print(f\"Hits@10: {lightgcn_final_test_metrics['hits@10']:.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# Save model + metrics\n",
    "# ===============================\n",
    "torch.save({\n",
    "    'model_state_dict': lightgcn_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'hyperparameters': {\n",
    "        'emb_dim': emb_dim,\n",
    "        'num_layers': num_layers,\n",
    "        'lr': lr,\n",
    "        'num_entities': num_entities\n",
    "    },\n",
    "    'final_test_metrics': lightgcn_final_test_metrics,\n",
    "    'training_history': lightgcn_metrics_tracker.metrics\n",
    "}, \"wn18rr_lightgcn_model.pt\")\n",
    "\n",
    "print(\"Model saved to wn18rr_lightgcn_model.pt\")\n",
    "\n",
    "lightgcn_metrics_tracker.save_to_file(\"wn18rr_lightgcn_metrics.txt\")\n",
    "print(\"Metrics saved to wn18rr_lightgcn_metrics.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
